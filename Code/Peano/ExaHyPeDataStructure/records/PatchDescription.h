#ifndef _EXAHYPE_RECORDS_PATCHDESCRIPTION_H
#define _EXAHYPE_RECORDS_PATCHDESCRIPTION_H

#include "peano/utils/Globals.h"
#include "tarch/compiler/CompilerSpecificSettings.h"
#include "peano/utils/PeanoOptimisations.h"
#ifdef Parallel
	#include "tarch/parallel/Node.h"
#endif
#ifdef Parallel
	#include <mpi.h>
#endif
#include "tarch/logging/Log.h"
#include "tarch/la/Vector.h"
#include <bitset>
#include <complex>
#include <string>
#include <iostream>

namespace exahype {
   namespace records {
      class PatchDescription;
      class PatchDescriptionPacked;
   }
}

/**
 * @author This class is generated by DaStGen
 * 		   DataStructureGenerator (DaStGen)
 * 		   2007-2009 Wolfgang Eckhardt
 * 		   2012      Tobias Weinzierl
 *
 * 		   build date: 09-02-2014 14:40
 *
 * @date   11/11/2015 17:13
 */
class exahype::records::PatchDescription { 
   
   public:
      
      typedef exahype::records::PatchDescriptionPacked Packed;
      
      struct PersistentRecords {
         int _spaceTimePredictorDof;
         int _spaceTimeVolumeFluxesDof;
         int _dof;
         int _updateDof;
         int _predictorDof;
         int _volumeFluxesDof;
         int _extrapolatedPredictorDof;
         int _normalFluxesDof;
         int _fluctuationsDof;
         int _level;
         #ifdef UseManualAlignment
         tarch::la::Vector<DIMENSIONS,double> _offset __attribute__((aligned(VectorisationAlignment)));
         #else
         tarch::la::Vector<DIMENSIONS,double> _offset;
         #endif
         #ifdef UseManualAlignment
         tarch::la::Vector<DIMENSIONS,double> _size __attribute__((aligned(VectorisationAlignment)));
         #else
         tarch::la::Vector<DIMENSIONS,double> _size;
         #endif
         /**
          * Generated
          */
         PersistentRecords();
         
         /**
          * Generated
          */
         PersistentRecords(const int& spaceTimePredictorDof, const int& spaceTimeVolumeFluxesDof, const int& dof, const int& updateDof, const int& predictorDof, const int& volumeFluxesDof, const int& extrapolatedPredictorDof, const int& normalFluxesDof, const int& fluctuationsDof, const int& level, const tarch::la::Vector<DIMENSIONS,double>& offset, const tarch::la::Vector<DIMENSIONS,double>& size);
         
         
         inline int getSpaceTimePredictorDof() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _spaceTimePredictorDof;
         }
         
         
         
         inline void setSpaceTimePredictorDof(const int& spaceTimePredictorDof) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _spaceTimePredictorDof = spaceTimePredictorDof;
         }
         
         
         
         inline int getSpaceTimeVolumeFluxesDof() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _spaceTimeVolumeFluxesDof;
         }
         
         
         
         inline void setSpaceTimeVolumeFluxesDof(const int& spaceTimeVolumeFluxesDof) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _spaceTimeVolumeFluxesDof = spaceTimeVolumeFluxesDof;
         }
         
         
         
         inline int getDof() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _dof;
         }
         
         
         
         inline void setDof(const int& dof) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _dof = dof;
         }
         
         
         
         inline int getUpdateDof() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _updateDof;
         }
         
         
         
         inline void setUpdateDof(const int& updateDof) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _updateDof = updateDof;
         }
         
         
         
         inline int getPredictorDof() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _predictorDof;
         }
         
         
         
         inline void setPredictorDof(const int& predictorDof) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _predictorDof = predictorDof;
         }
         
         
         
         inline int getVolumeFluxesDof() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _volumeFluxesDof;
         }
         
         
         
         inline void setVolumeFluxesDof(const int& volumeFluxesDof) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _volumeFluxesDof = volumeFluxesDof;
         }
         
         
         
         inline int getExtrapolatedPredictorDof() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _extrapolatedPredictorDof;
         }
         
         
         
         inline void setExtrapolatedPredictorDof(const int& extrapolatedPredictorDof) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _extrapolatedPredictorDof = extrapolatedPredictorDof;
         }
         
         
         
         inline int getNormalFluxesDof() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _normalFluxesDof;
         }
         
         
         
         inline void setNormalFluxesDof(const int& normalFluxesDof) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _normalFluxesDof = normalFluxesDof;
         }
         
         
         
         inline int getFluctuationsDof() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _fluctuationsDof;
         }
         
         
         
         inline void setFluctuationsDof(const int& fluctuationsDof) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _fluctuationsDof = fluctuationsDof;
         }
         
         
         
         inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _level;
         }
         
         
         
         inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _level = level;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<DIMENSIONS,double> getOffset() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _offset;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setOffset(const tarch::la::Vector<DIMENSIONS,double>& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _offset = (offset);
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<DIMENSIONS,double> getSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _size;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setSize(const tarch::la::Vector<DIMENSIONS,double>& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _size = (size);
         }
         
         
         
      };
      
   private: 
      PersistentRecords _persistentRecords;
      
   public:
      /**
       * Generated
       */
      PatchDescription();
      
      /**
       * Generated
       */
      PatchDescription(const PersistentRecords& persistentRecords);
      
      /**
       * Generated
       */
      PatchDescription(const int& spaceTimePredictorDof, const int& spaceTimeVolumeFluxesDof, const int& dof, const int& updateDof, const int& predictorDof, const int& volumeFluxesDof, const int& extrapolatedPredictorDof, const int& normalFluxesDof, const int& fluctuationsDof, const int& level, const tarch::la::Vector<DIMENSIONS,double>& offset, const tarch::la::Vector<DIMENSIONS,double>& size);
      
      /**
       * Generated
       */
      ~PatchDescription();
      
      
      inline int getSpaceTimePredictorDof() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         return _persistentRecords._spaceTimePredictorDof;
      }
      
      
      
      inline void setSpaceTimePredictorDof(const int& spaceTimePredictorDof) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         _persistentRecords._spaceTimePredictorDof = spaceTimePredictorDof;
      }
      
      
      
      inline int getSpaceTimeVolumeFluxesDof() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         return _persistentRecords._spaceTimeVolumeFluxesDof;
      }
      
      
      
      inline void setSpaceTimeVolumeFluxesDof(const int& spaceTimeVolumeFluxesDof) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         _persistentRecords._spaceTimeVolumeFluxesDof = spaceTimeVolumeFluxesDof;
      }
      
      
      
      inline int getDof() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         return _persistentRecords._dof;
      }
      
      
      
      inline void setDof(const int& dof) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         _persistentRecords._dof = dof;
      }
      
      
      
      inline int getUpdateDof() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         return _persistentRecords._updateDof;
      }
      
      
      
      inline void setUpdateDof(const int& updateDof) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         _persistentRecords._updateDof = updateDof;
      }
      
      
      
      inline int getPredictorDof() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         return _persistentRecords._predictorDof;
      }
      
      
      
      inline void setPredictorDof(const int& predictorDof) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         _persistentRecords._predictorDof = predictorDof;
      }
      
      
      
      inline int getVolumeFluxesDof() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         return _persistentRecords._volumeFluxesDof;
      }
      
      
      
      inline void setVolumeFluxesDof(const int& volumeFluxesDof) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         _persistentRecords._volumeFluxesDof = volumeFluxesDof;
      }
      
      
      
      inline int getExtrapolatedPredictorDof() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         return _persistentRecords._extrapolatedPredictorDof;
      }
      
      
      
      inline void setExtrapolatedPredictorDof(const int& extrapolatedPredictorDof) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         _persistentRecords._extrapolatedPredictorDof = extrapolatedPredictorDof;
      }
      
      
      
      inline int getNormalFluxesDof() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         return _persistentRecords._normalFluxesDof;
      }
      
      
      
      inline void setNormalFluxesDof(const int& normalFluxesDof) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         _persistentRecords._normalFluxesDof = normalFluxesDof;
      }
      
      
      
      inline int getFluctuationsDof() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         return _persistentRecords._fluctuationsDof;
      }
      
      
      
      inline void setFluctuationsDof(const int& fluctuationsDof) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         _persistentRecords._fluctuationsDof = fluctuationsDof;
      }
      
      
      
      inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         return _persistentRecords._level;
      }
      
      
      
      inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         _persistentRecords._level = level;
      }
      
      
      
      /**
       * Generated and optimized
       * 
       * If you realise a for loop using exclusively arrays (vectors) and compile 
       * with -DUseManualAlignment you may add 
       * \code
       #pragma vector aligned
       #pragma simd
       \endcode to this for loop to enforce your compiler to use SSE/AVX.
       * 
       * The alignment is tied to the unpacked records, i.e. for packed class
       * variants the machine's natural alignment is switched off to recude the  
       * memory footprint. Do not use any SSE/AVX operations or 
       * vectorisation on the result for the packed variants, as the data is misaligned. 
       * If you rely on vectorisation, convert the underlying record 
       * into the unpacked version first. 
       * 
       * @see convert()
       */
      inline tarch::la::Vector<DIMENSIONS,double> getOffset() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         return _persistentRecords._offset;
      }
      
      
      
      /**
       * Generated and optimized
       * 
       * If you realise a for loop using exclusively arrays (vectors) and compile 
       * with -DUseManualAlignment you may add 
       * \code
       #pragma vector aligned
       #pragma simd
       \endcode to this for loop to enforce your compiler to use SSE/AVX.
       * 
       * The alignment is tied to the unpacked records, i.e. for packed class
       * variants the machine's natural alignment is switched off to recude the  
       * memory footprint. Do not use any SSE/AVX operations or 
       * vectorisation on the result for the packed variants, as the data is misaligned. 
       * If you rely on vectorisation, convert the underlying record 
       * into the unpacked version first. 
       * 
       * @see convert()
       */
      inline void setOffset(const tarch::la::Vector<DIMENSIONS,double>& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         _persistentRecords._offset = (offset);
      }
      
      
      
      inline double getOffset(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         assertion(elementIndex>=0);
         assertion(elementIndex<DIMENSIONS);
         return _persistentRecords._offset[elementIndex];
         
      }
      
      
      
      inline void setOffset(int elementIndex, const double& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         assertion(elementIndex>=0);
         assertion(elementIndex<DIMENSIONS);
         _persistentRecords._offset[elementIndex]= offset;
         
      }
      
      
      
      /**
       * Generated and optimized
       * 
       * If you realise a for loop using exclusively arrays (vectors) and compile 
       * with -DUseManualAlignment you may add 
       * \code
       #pragma vector aligned
       #pragma simd
       \endcode to this for loop to enforce your compiler to use SSE/AVX.
       * 
       * The alignment is tied to the unpacked records, i.e. for packed class
       * variants the machine's natural alignment is switched off to recude the  
       * memory footprint. Do not use any SSE/AVX operations or 
       * vectorisation on the result for the packed variants, as the data is misaligned. 
       * If you rely on vectorisation, convert the underlying record 
       * into the unpacked version first. 
       * 
       * @see convert()
       */
      inline tarch::la::Vector<DIMENSIONS,double> getSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         return _persistentRecords._size;
      }
      
      
      
      /**
       * Generated and optimized
       * 
       * If you realise a for loop using exclusively arrays (vectors) and compile 
       * with -DUseManualAlignment you may add 
       * \code
       #pragma vector aligned
       #pragma simd
       \endcode to this for loop to enforce your compiler to use SSE/AVX.
       * 
       * The alignment is tied to the unpacked records, i.e. for packed class
       * variants the machine's natural alignment is switched off to recude the  
       * memory footprint. Do not use any SSE/AVX operations or 
       * vectorisation on the result for the packed variants, as the data is misaligned. 
       * If you rely on vectorisation, convert the underlying record 
       * into the unpacked version first. 
       * 
       * @see convert()
       */
      inline void setSize(const tarch::la::Vector<DIMENSIONS,double>& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         _persistentRecords._size = (size);
      }
      
      
      
      inline double getSize(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         assertion(elementIndex>=0);
         assertion(elementIndex<DIMENSIONS);
         return _persistentRecords._size[elementIndex];
         
      }
      
      
      
      inline void setSize(int elementIndex, const double& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         assertion(elementIndex>=0);
         assertion(elementIndex<DIMENSIONS);
         _persistentRecords._size[elementIndex]= size;
         
      }
      
      
      /**
       * Generated
       */
      std::string toString() const;
      
      /**
       * Generated
       */
      void toString(std::ostream& out) const;
      
      
      PersistentRecords getPersistentRecords() const;
      /**
       * Generated
       */
      PatchDescriptionPacked convert() const;
      
      
   #ifdef Parallel
      protected:
         static tarch::logging::Log _log;
         
      public:
         
         /**
          * Global that represents the mpi datatype.
          * There are two variants: Datatype identifies only those attributes marked with
          * parallelise. FullDatatype instead identifies the whole record with all fields.
          */
         static MPI_Datatype Datatype;
         static MPI_Datatype FullDatatype;
         
         /**
          * Initializes the data type for the mpi operations. Has to be called
          * before the very first send or receive operation is called.
          */
         static void initDatatype();
         
         static void shutdownDatatype();
         
         /**
          * @param communicateSleep -1 Data exchange through blocking mpi
          * @param communicateSleep  0 Data exchange through non-blocking mpi, i.e. pending messages are received via polling until MPI_Test succeeds
          * @param communicateSleep >0 Same as 0 but in addition, each unsuccessful MPI_Test is follows by an usleep
          */
         void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
         
         void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
         
         static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
         
         #endif
            
         };
         
         #ifndef DaStGenPackedPadding
           #define DaStGenPackedPadding 1      // 32 bit version
           // #define DaStGenPackedPadding 2   // 64 bit version
         #endif
         
         
         #ifdef PackedRecords
            #pragma pack (push, DaStGenPackedPadding)
         #endif
         
         /**
          * @author This class is generated by DaStGen
          * 		   DataStructureGenerator (DaStGen)
          * 		   2007-2009 Wolfgang Eckhardt
          * 		   2012      Tobias Weinzierl
          *
          * 		   build date: 09-02-2014 14:40
          *
          * @date   11/11/2015 17:13
          */
         class exahype::records::PatchDescriptionPacked { 
            
            public:
               
               struct PersistentRecords {
                  int _spaceTimePredictorDof;
                  int _spaceTimeVolumeFluxesDof;
                  int _dof;
                  int _updateDof;
                  int _predictorDof;
                  int _volumeFluxesDof;
                  int _extrapolatedPredictorDof;
                  int _normalFluxesDof;
                  int _fluctuationsDof;
                  int _level;
                  tarch::la::Vector<DIMENSIONS,double> _offset;
                  tarch::la::Vector<DIMENSIONS,double> _size;
                  /**
                   * Generated
                   */
                  PersistentRecords();
                  
                  /**
                   * Generated
                   */
                  PersistentRecords(const int& spaceTimePredictorDof, const int& spaceTimeVolumeFluxesDof, const int& dof, const int& updateDof, const int& predictorDof, const int& volumeFluxesDof, const int& extrapolatedPredictorDof, const int& normalFluxesDof, const int& fluctuationsDof, const int& level, const tarch::la::Vector<DIMENSIONS,double>& offset, const tarch::la::Vector<DIMENSIONS,double>& size);
                  
                  
                  inline int getSpaceTimePredictorDof() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _spaceTimePredictorDof;
                  }
                  
                  
                  
                  inline void setSpaceTimePredictorDof(const int& spaceTimePredictorDof) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _spaceTimePredictorDof = spaceTimePredictorDof;
                  }
                  
                  
                  
                  inline int getSpaceTimeVolumeFluxesDof() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _spaceTimeVolumeFluxesDof;
                  }
                  
                  
                  
                  inline void setSpaceTimeVolumeFluxesDof(const int& spaceTimeVolumeFluxesDof) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _spaceTimeVolumeFluxesDof = spaceTimeVolumeFluxesDof;
                  }
                  
                  
                  
                  inline int getDof() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _dof;
                  }
                  
                  
                  
                  inline void setDof(const int& dof) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _dof = dof;
                  }
                  
                  
                  
                  inline int getUpdateDof() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _updateDof;
                  }
                  
                  
                  
                  inline void setUpdateDof(const int& updateDof) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _updateDof = updateDof;
                  }
                  
                  
                  
                  inline int getPredictorDof() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _predictorDof;
                  }
                  
                  
                  
                  inline void setPredictorDof(const int& predictorDof) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _predictorDof = predictorDof;
                  }
                  
                  
                  
                  inline int getVolumeFluxesDof() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _volumeFluxesDof;
                  }
                  
                  
                  
                  inline void setVolumeFluxesDof(const int& volumeFluxesDof) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _volumeFluxesDof = volumeFluxesDof;
                  }
                  
                  
                  
                  inline int getExtrapolatedPredictorDof() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _extrapolatedPredictorDof;
                  }
                  
                  
                  
                  inline void setExtrapolatedPredictorDof(const int& extrapolatedPredictorDof) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _extrapolatedPredictorDof = extrapolatedPredictorDof;
                  }
                  
                  
                  
                  inline int getNormalFluxesDof() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _normalFluxesDof;
                  }
                  
                  
                  
                  inline void setNormalFluxesDof(const int& normalFluxesDof) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _normalFluxesDof = normalFluxesDof;
                  }
                  
                  
                  
                  inline int getFluctuationsDof() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _fluctuationsDof;
                  }
                  
                  
                  
                  inline void setFluctuationsDof(const int& fluctuationsDof) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _fluctuationsDof = fluctuationsDof;
                  }
                  
                  
                  
                  inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _level;
                  }
                  
                  
                  
                  inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _level = level;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline tarch::la::Vector<DIMENSIONS,double> getOffset() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _offset;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline void setOffset(const tarch::la::Vector<DIMENSIONS,double>& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _offset = (offset);
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline tarch::la::Vector<DIMENSIONS,double> getSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _size;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline void setSize(const tarch::la::Vector<DIMENSIONS,double>& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _size = (size);
                  }
                  
                  
                  
               };
               
            private: 
               PersistentRecords _persistentRecords;
               
            public:
               /**
                * Generated
                */
               PatchDescriptionPacked();
               
               /**
                * Generated
                */
               PatchDescriptionPacked(const PersistentRecords& persistentRecords);
               
               /**
                * Generated
                */
               PatchDescriptionPacked(const int& spaceTimePredictorDof, const int& spaceTimeVolumeFluxesDof, const int& dof, const int& updateDof, const int& predictorDof, const int& volumeFluxesDof, const int& extrapolatedPredictorDof, const int& normalFluxesDof, const int& fluctuationsDof, const int& level, const tarch::la::Vector<DIMENSIONS,double>& offset, const tarch::la::Vector<DIMENSIONS,double>& size);
               
               /**
                * Generated
                */
               ~PatchDescriptionPacked();
               
               
               inline int getSpaceTimePredictorDof() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._spaceTimePredictorDof;
               }
               
               
               
               inline void setSpaceTimePredictorDof(const int& spaceTimePredictorDof) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._spaceTimePredictorDof = spaceTimePredictorDof;
               }
               
               
               
               inline int getSpaceTimeVolumeFluxesDof() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._spaceTimeVolumeFluxesDof;
               }
               
               
               
               inline void setSpaceTimeVolumeFluxesDof(const int& spaceTimeVolumeFluxesDof) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._spaceTimeVolumeFluxesDof = spaceTimeVolumeFluxesDof;
               }
               
               
               
               inline int getDof() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._dof;
               }
               
               
               
               inline void setDof(const int& dof) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._dof = dof;
               }
               
               
               
               inline int getUpdateDof() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._updateDof;
               }
               
               
               
               inline void setUpdateDof(const int& updateDof) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._updateDof = updateDof;
               }
               
               
               
               inline int getPredictorDof() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._predictorDof;
               }
               
               
               
               inline void setPredictorDof(const int& predictorDof) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._predictorDof = predictorDof;
               }
               
               
               
               inline int getVolumeFluxesDof() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._volumeFluxesDof;
               }
               
               
               
               inline void setVolumeFluxesDof(const int& volumeFluxesDof) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._volumeFluxesDof = volumeFluxesDof;
               }
               
               
               
               inline int getExtrapolatedPredictorDof() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._extrapolatedPredictorDof;
               }
               
               
               
               inline void setExtrapolatedPredictorDof(const int& extrapolatedPredictorDof) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._extrapolatedPredictorDof = extrapolatedPredictorDof;
               }
               
               
               
               inline int getNormalFluxesDof() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._normalFluxesDof;
               }
               
               
               
               inline void setNormalFluxesDof(const int& normalFluxesDof) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._normalFluxesDof = normalFluxesDof;
               }
               
               
               
               inline int getFluctuationsDof() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._fluctuationsDof;
               }
               
               
               
               inline void setFluctuationsDof(const int& fluctuationsDof) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._fluctuationsDof = fluctuationsDof;
               }
               
               
               
               inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._level;
               }
               
               
               
               inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._level = level;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS,double> getOffset() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._offset;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setOffset(const tarch::la::Vector<DIMENSIONS,double>& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._offset = (offset);
               }
               
               
               
               inline double getOffset(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS);
                  return _persistentRecords._offset[elementIndex];
                  
               }
               
               
               
               inline void setOffset(int elementIndex, const double& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS);
                  _persistentRecords._offset[elementIndex]= offset;
                  
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS,double> getSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._size;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setSize(const tarch::la::Vector<DIMENSIONS,double>& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._size = (size);
               }
               
               
               
               inline double getSize(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS);
                  return _persistentRecords._size[elementIndex];
                  
               }
               
               
               
               inline void setSize(int elementIndex, const double& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS);
                  _persistentRecords._size[elementIndex]= size;
                  
               }
               
               
               /**
                * Generated
                */
               std::string toString() const;
               
               /**
                * Generated
                */
               void toString(std::ostream& out) const;
               
               
               PersistentRecords getPersistentRecords() const;
               /**
                * Generated
                */
               PatchDescription convert() const;
               
               
            #ifdef Parallel
               protected:
                  static tarch::logging::Log _log;
                  
               public:
                  
                  /**
                   * Global that represents the mpi datatype.
                   * There are two variants: Datatype identifies only those attributes marked with
                   * parallelise. FullDatatype instead identifies the whole record with all fields.
                   */
                  static MPI_Datatype Datatype;
                  static MPI_Datatype FullDatatype;
                  
                  /**
                   * Initializes the data type for the mpi operations. Has to be called
                   * before the very first send or receive operation is called.
                   */
                  static void initDatatype();
                  
                  static void shutdownDatatype();
                  
                  /**
                   * @param communicateSleep -1 Data exchange through blocking mpi
                   * @param communicateSleep  0 Data exchange through non-blocking mpi, i.e. pending messages are received via polling until MPI_Test succeeds
                   * @param communicateSleep >0 Same as 0 but in addition, each unsuccessful MPI_Test is follows by an usleep
                   */
                  void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
                  
                  void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
                  
                  static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                  
                  #endif
                     
                  };
                  
                  #ifdef PackedRecords
                  #pragma pack (pop)
                  #endif
                  
                  
                  #endif
                  
