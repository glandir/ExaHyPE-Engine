Packed-Type: short int;

class exahype::dastgen::State {    
  #ifdef Parallel
  enum GridConstructionState {
    Default, Veto, Aggressive
  };
  
  parallelise persistent GridConstructionState  gridConstructionState;
  #endif
  
  /**
   * This enum is used to specify
   * which data we want to merge in the
   * Merging mapping.
   * 
   * Note that we define sending and merging here 
   * as operation between individual cells not
   * just between subdomains belong to different threads
   * or MPI processes.
   * 
   * For example, an ADER-DG solver might send
   * data to its face data arrays or a neighbour rank's heap in one
   * iteration.
   * In the next iteration, the data is picked up and merrged by its 
   * direct neighbour which might be a local cell
   * or a cell belonging to another rank.
   */
  enum MergeMode {
    /**
     * Do not merge anything.
     */
    MergeNothing,

    /**
     * This is one of the modes
     * for the standard time stepping
     * algorithm.
     *
     * This mode should be set before
     * performing the correction phase.
     *
     * Time step data exchange is
     * a reduction-broadcast operation.
     */
    BroadcastAndMergeTimeStepData,
    /**
     * This is one of the modes
     * for the standard time stepping
     * algorithm.
     *
     * This mode should be set before
     * performing the prediction phase.
     *
     * In principal, face data exchange is
     * direct neighbour communication.
     * However if we consider adaptive meshes,
     * face data exchange also includes
     * as master-worker and worker-master
     * communication.
     */
    MergeFaceData,
    /**
     * This is the default mode if you use
     * fused time stepping.
     *
     * In this case, we exchange
     * time step as well as
     * face data.
     */
    BroadcastAndMergeTimeStepDataAndMergeFaceData
  };

  parallelise persistent MergeMode mergeMode;

  /**
   * This enum is used to specify
   * which data we want to send in the
   * Sending mapping.
   * 
   * Note that we define sending and merging here 
   * as operations between individual cells not
   * just between subdomains belong to different threads
   * or MPI processes. 
   * 
   * For example, an ADER-DG solver might send
   * data to its face data arrays or a neighbour rank's heap in
   * one iteration.
   * In the next iteration, the data is picked up and merrged by its 
   * direct neighbour which might be a local cell
   * or a cell belonging to another rank.
   */
  enum SendMode {
    /**
     * Do not send anything.
     */
    SendNothing,

    /**
     * This is one of the modes
     * for the standard time stepping
     * algorithm.
     *
     * This mode should be set before
     * performing the correction phase.
     *
     * Time step data exchange is
     * a reduction-broadcast operation.
     */
    ReduceAndMergeTimeStepData,
    /**
     * This is one of the modes
     * for the standard time stepping
     * algorithm.
     *
     * This mode should be set before
     * performing the prediction phase.
     *
     * In principal, face data exchange is
     * direct neighbour communication.
     * However if we consider adaptive meshes,
     * face data exchange also includes
     * as master-worker and worker-master
     * communication.
     */
    SendFaceData,
    /**
     * Choose this mode if you use
     * fused time stepping.
     *
     * In this case, we exchange
     * time step as well as
     * face data. We thus overlap the
     * reduction-broadcast with
     * the master-worker and
     * worker-master communication.
     */
    ReduceAndMergeTimeStepDataAndSendFaceData
  };
  parallelise persistent SendMode sendMode;
  
  /**
   * This flag indicates if at least one solver
   * has performed a time step with an 
   * instable (too large) time step size and
   * thus needs to be recomputed.
   * This can happen when we run the fused ADER-DG
   * variant. 
   * 
   * Needs to be reset after the affected
   * solvers have been rerun with a
   * stable time step size.
   * 
   * <h2>MPI</h2>
   * This flag does not need to be parallelised since
   * we use it only for the global master rank.
   */
  persistent bool stabilityConditionOfOneSolverWasViolated;
  
  /**
   * Flag indicating if we run the fused ADER-DG time
   * stepping variant.
   * 
   * <h2>MPI</h2>
   * This flag does not need to be parallelised since
   * we use it only for the global master rank.
   */
  persistent bool fuseADERDGPhases;
  
  /**
   * A factor in the range (0,1] that
   * is used for reseting the time step sizes
   * of an ADER-DG solver.
   * 
   * The reset corrector and predictor time step sizes
   * are reset to a stable time step size times this factor. 
   */
  persistent double timeStepSizeWeightForPredictionRerun;
};
