#ifndef _EXAHYPE_RECORDS_ADERDGCELLDESCRIPTION_H
#define _EXAHYPE_RECORDS_ADERDGCELLDESCRIPTION_H

#include "peano/utils/Globals.h"
#include "tarch/compiler/CompilerSpecificSettings.h"
#include "peano/utils/PeanoOptimisations.h"
#ifdef Parallel
	#include "tarch/parallel/Node.h"
#endif
#ifdef Parallel
	#include <mpi.h>
#endif
#include "tarch/logging/Log.h"
#include "tarch/la/Vector.h"
#include <bitset>
#include <complex>
#include <string>
#include <iostream>

namespace exahype {
   namespace records {
      class ADERDGCellDescription;
      class ADERDGCellDescriptionPacked;
   }
}

#if defined(Parallel)
   /**
    * @author This class is generated by DaStGen
    * 		   DataStructureGenerator (DaStGen)
    * 		   2007-2009 Wolfgang Eckhardt
    * 		   2012      Tobias Weinzierl
    *
    * 		   build date: 09-02-2014 14:40
    *
    * @date   07/10/2016 15:03
    */
   class exahype::records::ADERDGCellDescription { 
      
      public:
         
         typedef exahype::records::ADERDGCellDescriptionPacked Packed;
         
         enum LimiterStatus {
            Ok = 0, Troubled = 1, NeighbourIsTroubledCell = 2, NeighbourIsNeighbourOfTroubledCell = 3
         };
         
         enum RefinementEvent {
            None = 0, ErasingChildrenRequested = 1, ErasingChildren = 2, ChangeChildrenToDescendantsRequested = 3, ChangeChildrenToDescendants = 4, RefiningRequested = 5, Refining = 6, DeaugmentingChildrenRequestedTriggered = 7, DeaugmentingChildrenRequested = 8, DeaugmentingChildren = 9, AugmentingRequested = 10, Augmenting = 11
         };
         
         enum Type {
            Erased = 0, Ancestor = 1, EmptyAncestor = 2, Cell = 3, Descendant = 4, EmptyDescendant = 5
         };
         
         struct PersistentRecords {
            int _solverNumber;
            #ifdef UseManualAlignment
            std::bitset<DIMENSIONS_TIMES_TWO> _riemannSolvePerformed __attribute__((aligned(VectorisationAlignment)));
            #else
            std::bitset<DIMENSIONS_TIMES_TWO> _riemannSolvePerformed;
            #endif
            #ifdef UseManualAlignment
            std::bitset<DIMENSIONS_TIMES_TWO> _isInside __attribute__((aligned(VectorisationAlignment)));
            #else
            std::bitset<DIMENSIONS_TIMES_TWO> _isInside;
            #endif
            bool _hasToHoldDataForNeighbourCommunication;
            bool _hasToHoldDataForMasterWorkerCommunication;
            #ifdef UseManualAlignment
            tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> _faceDataExchangeCounter __attribute__((aligned(VectorisationAlignment)));
            #else
            tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> _faceDataExchangeCounter;
            #endif
            int _parentIndex;
            Type _type;
            RefinementEvent _refinementEvent;
            int _level;
            #ifdef UseManualAlignment
            tarch::la::Vector<DIMENSIONS,double> _offset __attribute__((aligned(VectorisationAlignment)));
            #else
            tarch::la::Vector<DIMENSIONS,double> _offset;
            #endif
            #ifdef UseManualAlignment
            tarch::la::Vector<DIMENSIONS,double> _size __attribute__((aligned(VectorisationAlignment)));
            #else
            tarch::la::Vector<DIMENSIONS,double> _size;
            #endif
            double _correctorTimeStepSize;
            double _correctorTimeStamp;
            double _predictorTimeStepSize;
            double _predictorTimeStamp;
            double _nextPredictorTimeStepSize;
            int _solution;
            int _solutionAverages;
            int _update;
            int _updateAverages;
            int _extrapolatedPredictor;
            int _extrapolatedPredictorAverages;
            int _fluctuation;
            int _fluctuationAverages;
            int _solutionMin;
            int _solutionMax;
            LimiterStatus _limiterStatus;
            /**
             * Generated
             */
            PersistentRecords();
            
            /**
             * Generated
             */
            PersistentRecords(const int& solverNumber, const std::bitset<DIMENSIONS_TIMES_TWO>& riemannSolvePerformed, const std::bitset<DIMENSIONS_TIMES_TWO>& isInside, const bool& hasToHoldDataForNeighbourCommunication, const bool& hasToHoldDataForMasterWorkerCommunication, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& faceDataExchangeCounter, const int& parentIndex, const Type& type, const RefinementEvent& refinementEvent, const int& level, const tarch::la::Vector<DIMENSIONS,double>& offset, const tarch::la::Vector<DIMENSIONS,double>& size, const double& correctorTimeStepSize, const double& correctorTimeStamp, const double& predictorTimeStepSize, const double& predictorTimeStamp, const double& nextPredictorTimeStepSize, const int& solution, const int& solutionAverages, const int& update, const int& updateAverages, const int& extrapolatedPredictor, const int& extrapolatedPredictorAverages, const int& fluctuation, const int& fluctuationAverages, const int& solutionMin, const int& solutionMax, const LimiterStatus& limiterStatus);
            
            
            inline int getSolverNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _solverNumber;
            }
            
            
            
            inline void setSolverNumber(const int& solverNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _solverNumber = solverNumber;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline std::bitset<DIMENSIONS_TIMES_TWO> getRiemannSolvePerformed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _riemannSolvePerformed;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setRiemannSolvePerformed(const std::bitset<DIMENSIONS_TIMES_TWO>& riemannSolvePerformed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _riemannSolvePerformed = (riemannSolvePerformed);
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline std::bitset<DIMENSIONS_TIMES_TWO> getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _isInside;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setIsInside(const std::bitset<DIMENSIONS_TIMES_TWO>& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _isInside = (isInside);
            }
            
            
            
            inline bool getHasToHoldDataForNeighbourCommunication() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _hasToHoldDataForNeighbourCommunication;
            }
            
            
            
            inline void setHasToHoldDataForNeighbourCommunication(const bool& hasToHoldDataForNeighbourCommunication) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _hasToHoldDataForNeighbourCommunication = hasToHoldDataForNeighbourCommunication;
            }
            
            
            
            inline bool getHasToHoldDataForMasterWorkerCommunication() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _hasToHoldDataForMasterWorkerCommunication;
            }
            
            
            
            inline void setHasToHoldDataForMasterWorkerCommunication(const bool& hasToHoldDataForMasterWorkerCommunication) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _hasToHoldDataForMasterWorkerCommunication = hasToHoldDataForMasterWorkerCommunication;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> getFaceDataExchangeCounter() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _faceDataExchangeCounter;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setFaceDataExchangeCounter(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& faceDataExchangeCounter) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _faceDataExchangeCounter = (faceDataExchangeCounter);
            }
            
            
            
            inline int getParentIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _parentIndex;
            }
            
            
            
            inline void setParentIndex(const int& parentIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _parentIndex = parentIndex;
            }
            
            
            
            inline Type getType() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _type;
            }
            
            
            
            inline void setType(const Type& type) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _type = type;
            }
            
            
            
            inline RefinementEvent getRefinementEvent() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _refinementEvent;
            }
            
            
            
            inline void setRefinementEvent(const RefinementEvent& refinementEvent) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _refinementEvent = refinementEvent;
            }
            
            
            
            inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _level;
            }
            
            
            
            inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _level = level;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS,double> getOffset() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _offset;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setOffset(const tarch::la::Vector<DIMENSIONS,double>& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _offset = (offset);
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS,double> getSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _size;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setSize(const tarch::la::Vector<DIMENSIONS,double>& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _size = (size);
            }
            
            
            
            inline double getCorrectorTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _correctorTimeStepSize;
            }
            
            
            
            inline void setCorrectorTimeStepSize(const double& correctorTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _correctorTimeStepSize = correctorTimeStepSize;
            }
            
            
            
            inline double getCorrectorTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _correctorTimeStamp;
            }
            
            
            
            inline void setCorrectorTimeStamp(const double& correctorTimeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _correctorTimeStamp = correctorTimeStamp;
            }
            
            
            
            inline double getPredictorTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _predictorTimeStepSize;
            }
            
            
            
            inline void setPredictorTimeStepSize(const double& predictorTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _predictorTimeStepSize = predictorTimeStepSize;
            }
            
            
            
            inline double getPredictorTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _predictorTimeStamp;
            }
            
            
            
            inline void setPredictorTimeStamp(const double& predictorTimeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _predictorTimeStamp = predictorTimeStamp;
            }
            
            
            
            inline double getNextPredictorTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _nextPredictorTimeStepSize;
            }
            
            
            
            inline void setNextPredictorTimeStepSize(const double& nextPredictorTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _nextPredictorTimeStepSize = nextPredictorTimeStepSize;
            }
            
            
            
            inline int getSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _solution;
            }
            
            
            
            inline void setSolution(const int& solution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _solution = solution;
            }
            
            
            
            inline int getSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _solutionAverages;
            }
            
            
            
            inline void setSolutionAverages(const int& solutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _solutionAverages = solutionAverages;
            }
            
            
            
            inline int getUpdate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _update;
            }
            
            
            
            inline void setUpdate(const int& update) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _update = update;
            }
            
            
            
            inline int getUpdateAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _updateAverages;
            }
            
            
            
            inline void setUpdateAverages(const int& updateAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _updateAverages = updateAverages;
            }
            
            
            
            inline int getExtrapolatedPredictor() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _extrapolatedPredictor;
            }
            
            
            
            inline void setExtrapolatedPredictor(const int& extrapolatedPredictor) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _extrapolatedPredictor = extrapolatedPredictor;
            }
            
            
            
            inline int getExtrapolatedPredictorAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _extrapolatedPredictorAverages;
            }
            
            
            
            inline void setExtrapolatedPredictorAverages(const int& extrapolatedPredictorAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _extrapolatedPredictorAverages = extrapolatedPredictorAverages;
            }
            
            
            
            inline int getFluctuation() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _fluctuation;
            }
            
            
            
            inline void setFluctuation(const int& fluctuation) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _fluctuation = fluctuation;
            }
            
            
            
            inline int getFluctuationAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _fluctuationAverages;
            }
            
            
            
            inline void setFluctuationAverages(const int& fluctuationAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _fluctuationAverages = fluctuationAverages;
            }
            
            
            
            inline int getSolutionMin() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _solutionMin;
            }
            
            
            
            inline void setSolutionMin(const int& solutionMin) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _solutionMin = solutionMin;
            }
            
            
            
            inline int getSolutionMax() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _solutionMax;
            }
            
            
            
            inline void setSolutionMax(const int& solutionMax) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _solutionMax = solutionMax;
            }
            
            
            
            inline LimiterStatus getLimiterStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _limiterStatus;
            }
            
            
            
            inline void setLimiterStatus(const LimiterStatus& limiterStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _limiterStatus = limiterStatus;
            }
            
            
            
         };
         
      private: 
         PersistentRecords _persistentRecords;
         bool _skipSolutionUpdate;
         
      public:
         /**
          * Generated
          */
         ADERDGCellDescription();
         
         /**
          * Generated
          */
         ADERDGCellDescription(const PersistentRecords& persistentRecords);
         
         /**
          * Generated
          */
         ADERDGCellDescription(const int& solverNumber, const std::bitset<DIMENSIONS_TIMES_TWO>& riemannSolvePerformed, const std::bitset<DIMENSIONS_TIMES_TWO>& isInside, const bool& hasToHoldDataForNeighbourCommunication, const bool& hasToHoldDataForMasterWorkerCommunication, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& faceDataExchangeCounter, const int& parentIndex, const Type& type, const RefinementEvent& refinementEvent, const int& level, const tarch::la::Vector<DIMENSIONS,double>& offset, const tarch::la::Vector<DIMENSIONS,double>& size, const double& correctorTimeStepSize, const double& correctorTimeStamp, const double& predictorTimeStepSize, const double& predictorTimeStamp, const double& nextPredictorTimeStepSize, const int& solution, const int& solutionAverages, const int& update, const int& updateAverages, const int& extrapolatedPredictor, const int& extrapolatedPredictorAverages, const int& fluctuation, const int& fluctuationAverages, const int& solutionMin, const int& solutionMax, const LimiterStatus& limiterStatus);
         
         /**
          * Generated
          */
         ADERDGCellDescription(const int& solverNumber, const std::bitset<DIMENSIONS_TIMES_TWO>& riemannSolvePerformed, const std::bitset<DIMENSIONS_TIMES_TWO>& isInside, const bool& hasToHoldDataForNeighbourCommunication, const bool& hasToHoldDataForMasterWorkerCommunication, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& faceDataExchangeCounter, const int& parentIndex, const Type& type, const RefinementEvent& refinementEvent, const int& level, const tarch::la::Vector<DIMENSIONS,double>& offset, const tarch::la::Vector<DIMENSIONS,double>& size, const double& correctorTimeStepSize, const double& correctorTimeStamp, const double& predictorTimeStepSize, const double& predictorTimeStamp, const double& nextPredictorTimeStepSize, const int& solution, const int& solutionAverages, const int& update, const int& updateAverages, const int& extrapolatedPredictor, const int& extrapolatedPredictorAverages, const int& fluctuation, const int& fluctuationAverages, const int& solutionMin, const int& solutionMax, const bool& skipSolutionUpdate, const LimiterStatus& limiterStatus);
         
         /**
          * Generated
          */
         ~ADERDGCellDescription();
         
         
         inline int getSolverNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._solverNumber;
         }
         
         
         
         inline void setSolverNumber(const int& solverNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._solverNumber = solverNumber;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline std::bitset<DIMENSIONS_TIMES_TWO> getRiemannSolvePerformed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._riemannSolvePerformed;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setRiemannSolvePerformed(const std::bitset<DIMENSIONS_TIMES_TWO>& riemannSolvePerformed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._riemannSolvePerformed = (riemannSolvePerformed);
         }
         
         
         
         inline bool getRiemannSolvePerformed(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS_TIMES_TWO);
            return _persistentRecords._riemannSolvePerformed[elementIndex];
            
         }
         
         
         
         inline void setRiemannSolvePerformed(int elementIndex, const bool& riemannSolvePerformed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS_TIMES_TWO);
            _persistentRecords._riemannSolvePerformed[elementIndex]= riemannSolvePerformed;
            
         }
         
         
         
         inline void flipRiemannSolvePerformed(int elementIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS_TIMES_TWO);
            _persistentRecords._riemannSolvePerformed.flip(elementIndex);
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline std::bitset<DIMENSIONS_TIMES_TWO> getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._isInside;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setIsInside(const std::bitset<DIMENSIONS_TIMES_TWO>& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._isInside = (isInside);
         }
         
         
         
         inline bool getIsInside(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS_TIMES_TWO);
            return _persistentRecords._isInside[elementIndex];
            
         }
         
         
         
         inline void setIsInside(int elementIndex, const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS_TIMES_TWO);
            _persistentRecords._isInside[elementIndex]= isInside;
            
         }
         
         
         
         inline void flipIsInside(int elementIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS_TIMES_TWO);
            _persistentRecords._isInside.flip(elementIndex);
         }
         
         
         
         inline bool getHasToHoldDataForNeighbourCommunication() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._hasToHoldDataForNeighbourCommunication;
         }
         
         
         
         inline void setHasToHoldDataForNeighbourCommunication(const bool& hasToHoldDataForNeighbourCommunication) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._hasToHoldDataForNeighbourCommunication = hasToHoldDataForNeighbourCommunication;
         }
         
         
         
         inline bool getHasToHoldDataForMasterWorkerCommunication() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._hasToHoldDataForMasterWorkerCommunication;
         }
         
         
         
         inline void setHasToHoldDataForMasterWorkerCommunication(const bool& hasToHoldDataForMasterWorkerCommunication) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._hasToHoldDataForMasterWorkerCommunication = hasToHoldDataForMasterWorkerCommunication;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> getFaceDataExchangeCounter() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._faceDataExchangeCounter;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setFaceDataExchangeCounter(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& faceDataExchangeCounter) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._faceDataExchangeCounter = (faceDataExchangeCounter);
         }
         
         
         
         inline int getFaceDataExchangeCounter(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS_TIMES_TWO);
            return _persistentRecords._faceDataExchangeCounter[elementIndex];
            
         }
         
         
         
         inline void setFaceDataExchangeCounter(int elementIndex, const int& faceDataExchangeCounter) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS_TIMES_TWO);
            _persistentRecords._faceDataExchangeCounter[elementIndex]= faceDataExchangeCounter;
            
         }
         
         
         
         inline int getParentIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._parentIndex;
         }
         
         
         
         inline void setParentIndex(const int& parentIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._parentIndex = parentIndex;
         }
         
         
         
         inline Type getType() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._type;
         }
         
         
         
         inline void setType(const Type& type) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._type = type;
         }
         
         
         
         inline RefinementEvent getRefinementEvent() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._refinementEvent;
         }
         
         
         
         inline void setRefinementEvent(const RefinementEvent& refinementEvent) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._refinementEvent = refinementEvent;
         }
         
         
         
         inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._level;
         }
         
         
         
         inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._level = level;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<DIMENSIONS,double> getOffset() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._offset;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setOffset(const tarch::la::Vector<DIMENSIONS,double>& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._offset = (offset);
         }
         
         
         
         inline double getOffset(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS);
            return _persistentRecords._offset[elementIndex];
            
         }
         
         
         
         inline void setOffset(int elementIndex, const double& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS);
            _persistentRecords._offset[elementIndex]= offset;
            
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<DIMENSIONS,double> getSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._size;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setSize(const tarch::la::Vector<DIMENSIONS,double>& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._size = (size);
         }
         
         
         
         inline double getSize(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS);
            return _persistentRecords._size[elementIndex];
            
         }
         
         
         
         inline void setSize(int elementIndex, const double& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS);
            _persistentRecords._size[elementIndex]= size;
            
         }
         
         
         
         inline double getCorrectorTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._correctorTimeStepSize;
         }
         
         
         
         inline void setCorrectorTimeStepSize(const double& correctorTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._correctorTimeStepSize = correctorTimeStepSize;
         }
         
         
         
         inline double getCorrectorTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._correctorTimeStamp;
         }
         
         
         
         inline void setCorrectorTimeStamp(const double& correctorTimeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._correctorTimeStamp = correctorTimeStamp;
         }
         
         
         
         inline double getPredictorTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._predictorTimeStepSize;
         }
         
         
         
         inline void setPredictorTimeStepSize(const double& predictorTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._predictorTimeStepSize = predictorTimeStepSize;
         }
         
         
         
         inline double getPredictorTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._predictorTimeStamp;
         }
         
         
         
         inline void setPredictorTimeStamp(const double& predictorTimeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._predictorTimeStamp = predictorTimeStamp;
         }
         
         
         
         inline double getNextPredictorTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._nextPredictorTimeStepSize;
         }
         
         
         
         inline void setNextPredictorTimeStepSize(const double& nextPredictorTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._nextPredictorTimeStepSize = nextPredictorTimeStepSize;
         }
         
         
         
         inline int getSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._solution;
         }
         
         
         
         inline void setSolution(const int& solution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._solution = solution;
         }
         
         
         
         inline int getSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._solutionAverages;
         }
         
         
         
         inline void setSolutionAverages(const int& solutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._solutionAverages = solutionAverages;
         }
         
         
         
         inline int getUpdate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._update;
         }
         
         
         
         inline void setUpdate(const int& update) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._update = update;
         }
         
         
         
         inline int getUpdateAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._updateAverages;
         }
         
         
         
         inline void setUpdateAverages(const int& updateAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._updateAverages = updateAverages;
         }
         
         
         
         inline int getExtrapolatedPredictor() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._extrapolatedPredictor;
         }
         
         
         
         inline void setExtrapolatedPredictor(const int& extrapolatedPredictor) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._extrapolatedPredictor = extrapolatedPredictor;
         }
         
         
         
         inline int getExtrapolatedPredictorAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._extrapolatedPredictorAverages;
         }
         
         
         
         inline void setExtrapolatedPredictorAverages(const int& extrapolatedPredictorAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._extrapolatedPredictorAverages = extrapolatedPredictorAverages;
         }
         
         
         
         inline int getFluctuation() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._fluctuation;
         }
         
         
         
         inline void setFluctuation(const int& fluctuation) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._fluctuation = fluctuation;
         }
         
         
         
         inline int getFluctuationAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._fluctuationAverages;
         }
         
         
         
         inline void setFluctuationAverages(const int& fluctuationAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._fluctuationAverages = fluctuationAverages;
         }
         
         
         
         inline int getSolutionMin() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._solutionMin;
         }
         
         
         
         inline void setSolutionMin(const int& solutionMin) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._solutionMin = solutionMin;
         }
         
         
         
         inline int getSolutionMax() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._solutionMax;
         }
         
         
         
         inline void setSolutionMax(const int& solutionMax) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._solutionMax = solutionMax;
         }
         
         
         
         inline bool getSkipSolutionUpdate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _skipSolutionUpdate;
         }
         
         
         
         inline void setSkipSolutionUpdate(const bool& skipSolutionUpdate) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _skipSolutionUpdate = skipSolutionUpdate;
         }
         
         
         
         inline LimiterStatus getLimiterStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._limiterStatus;
         }
         
         
         
         inline void setLimiterStatus(const LimiterStatus& limiterStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._limiterStatus = limiterStatus;
         }
         
         
         /**
          * Generated
          */
         static std::string toString(const LimiterStatus& param);
         
         /**
          * Generated
          */
         static std::string getLimiterStatusMapping();
         
         /**
          * Generated
          */
         static std::string toString(const RefinementEvent& param);
         
         /**
          * Generated
          */
         static std::string getRefinementEventMapping();
         
         /**
          * Generated
          */
         static std::string toString(const Type& param);
         
         /**
          * Generated
          */
         static std::string getTypeMapping();
         
         /**
          * Generated
          */
         std::string toString() const;
         
         /**
          * Generated
          */
         void toString(std::ostream& out) const;
         
         
         PersistentRecords getPersistentRecords() const;
         /**
          * Generated
          */
         ADERDGCellDescriptionPacked convert() const;
         
         
      #ifdef Parallel
         protected:
            static tarch::logging::Log _log;
            
         public:
            
            /**
             * Global that represents the mpi datatype.
             * There are two variants: Datatype identifies only those attributes marked with
             * parallelise. FullDatatype instead identifies the whole record with all fields.
             */
            static MPI_Datatype Datatype;
            static MPI_Datatype FullDatatype;
            
            /**
             * Initializes the data type for the mpi operations. Has to be called
             * before the very first send or receive operation is called.
             */
            static void initDatatype();
            
            static void shutdownDatatype();
            
            /**
             * @param communicateSleep -1 Data exchange through blocking mpi
             * @param communicateSleep  0 Data exchange through non-blocking mpi, i.e. pending messages are received via polling until MPI_Test succeeds
             * @param communicateSleep >0 Same as 0 but in addition, each unsuccessful MPI_Test is follows by an usleep
             */
            void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
            
            void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
            
            static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
            
            #endif
               
            };
            
            #ifndef DaStGenPackedPadding
              #define DaStGenPackedPadding 1      // 32 bit version
              // #define DaStGenPackedPadding 2   // 64 bit version
            #endif
            
            
            #ifdef PackedRecords
               #pragma pack (push, DaStGenPackedPadding)
            #endif
            
            /**
             * @author This class is generated by DaStGen
             * 		   DataStructureGenerator (DaStGen)
             * 		   2007-2009 Wolfgang Eckhardt
             * 		   2012      Tobias Weinzierl
             *
             * 		   build date: 09-02-2014 14:40
             *
             * @date   07/10/2016 15:03
             */
            class exahype::records::ADERDGCellDescriptionPacked { 
               
               public:
                  
                  typedef exahype::records::ADERDGCellDescription::Type Type;
                  
                  typedef exahype::records::ADERDGCellDescription::RefinementEvent RefinementEvent;
                  
                  typedef exahype::records::ADERDGCellDescription::LimiterStatus LimiterStatus;
                  
                  struct PersistentRecords {
                     int _solverNumber;
                     std::bitset<DIMENSIONS_TIMES_TWO> _riemannSolvePerformed;
                     std::bitset<DIMENSIONS_TIMES_TWO> _isInside;
                     bool _hasToHoldDataForNeighbourCommunication;
                     bool _hasToHoldDataForMasterWorkerCommunication;
                     tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> _faceDataExchangeCounter;
                     int _parentIndex;
                     Type _type;
                     RefinementEvent _refinementEvent;
                     int _level;
                     tarch::la::Vector<DIMENSIONS,double> _offset;
                     tarch::la::Vector<DIMENSIONS,double> _size;
                     double _correctorTimeStepSize;
                     double _correctorTimeStamp;
                     double _predictorTimeStepSize;
                     double _predictorTimeStamp;
                     double _nextPredictorTimeStepSize;
                     int _solution;
                     int _solutionAverages;
                     int _update;
                     int _updateAverages;
                     int _extrapolatedPredictor;
                     int _extrapolatedPredictorAverages;
                     int _fluctuation;
                     int _fluctuationAverages;
                     int _solutionMin;
                     int _solutionMax;
                     LimiterStatus _limiterStatus;
                     /**
                      * Generated
                      */
                     PersistentRecords();
                     
                     /**
                      * Generated
                      */
                     PersistentRecords(const int& solverNumber, const std::bitset<DIMENSIONS_TIMES_TWO>& riemannSolvePerformed, const std::bitset<DIMENSIONS_TIMES_TWO>& isInside, const bool& hasToHoldDataForNeighbourCommunication, const bool& hasToHoldDataForMasterWorkerCommunication, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& faceDataExchangeCounter, const int& parentIndex, const Type& type, const RefinementEvent& refinementEvent, const int& level, const tarch::la::Vector<DIMENSIONS,double>& offset, const tarch::la::Vector<DIMENSIONS,double>& size, const double& correctorTimeStepSize, const double& correctorTimeStamp, const double& predictorTimeStepSize, const double& predictorTimeStamp, const double& nextPredictorTimeStepSize, const int& solution, const int& solutionAverages, const int& update, const int& updateAverages, const int& extrapolatedPredictor, const int& extrapolatedPredictorAverages, const int& fluctuation, const int& fluctuationAverages, const int& solutionMin, const int& solutionMax, const LimiterStatus& limiterStatus);
                     
                     
                     inline int getSolverNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _solverNumber;
                     }
                     
                     
                     
                     inline void setSolverNumber(const int& solverNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _solverNumber = solverNumber;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline std::bitset<DIMENSIONS_TIMES_TWO> getRiemannSolvePerformed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _riemannSolvePerformed;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline void setRiemannSolvePerformed(const std::bitset<DIMENSIONS_TIMES_TWO>& riemannSolvePerformed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _riemannSolvePerformed = (riemannSolvePerformed);
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline std::bitset<DIMENSIONS_TIMES_TWO> getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _isInside;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline void setIsInside(const std::bitset<DIMENSIONS_TIMES_TWO>& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _isInside = (isInside);
                     }
                     
                     
                     
                     inline bool getHasToHoldDataForNeighbourCommunication() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _hasToHoldDataForNeighbourCommunication;
                     }
                     
                     
                     
                     inline void setHasToHoldDataForNeighbourCommunication(const bool& hasToHoldDataForNeighbourCommunication) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _hasToHoldDataForNeighbourCommunication = hasToHoldDataForNeighbourCommunication;
                     }
                     
                     
                     
                     inline bool getHasToHoldDataForMasterWorkerCommunication() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _hasToHoldDataForMasterWorkerCommunication;
                     }
                     
                     
                     
                     inline void setHasToHoldDataForMasterWorkerCommunication(const bool& hasToHoldDataForMasterWorkerCommunication) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _hasToHoldDataForMasterWorkerCommunication = hasToHoldDataForMasterWorkerCommunication;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> getFaceDataExchangeCounter() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _faceDataExchangeCounter;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline void setFaceDataExchangeCounter(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& faceDataExchangeCounter) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _faceDataExchangeCounter = (faceDataExchangeCounter);
                     }
                     
                     
                     
                     inline int getParentIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _parentIndex;
                     }
                     
                     
                     
                     inline void setParentIndex(const int& parentIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _parentIndex = parentIndex;
                     }
                     
                     
                     
                     inline Type getType() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _type;
                     }
                     
                     
                     
                     inline void setType(const Type& type) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _type = type;
                     }
                     
                     
                     
                     inline RefinementEvent getRefinementEvent() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _refinementEvent;
                     }
                     
                     
                     
                     inline void setRefinementEvent(const RefinementEvent& refinementEvent) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _refinementEvent = refinementEvent;
                     }
                     
                     
                     
                     inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _level;
                     }
                     
                     
                     
                     inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _level = level;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline tarch::la::Vector<DIMENSIONS,double> getOffset() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _offset;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline void setOffset(const tarch::la::Vector<DIMENSIONS,double>& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _offset = (offset);
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline tarch::la::Vector<DIMENSIONS,double> getSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _size;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline void setSize(const tarch::la::Vector<DIMENSIONS,double>& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _size = (size);
                     }
                     
                     
                     
                     inline double getCorrectorTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _correctorTimeStepSize;
                     }
                     
                     
                     
                     inline void setCorrectorTimeStepSize(const double& correctorTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _correctorTimeStepSize = correctorTimeStepSize;
                     }
                     
                     
                     
                     inline double getCorrectorTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _correctorTimeStamp;
                     }
                     
                     
                     
                     inline void setCorrectorTimeStamp(const double& correctorTimeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _correctorTimeStamp = correctorTimeStamp;
                     }
                     
                     
                     
                     inline double getPredictorTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _predictorTimeStepSize;
                     }
                     
                     
                     
                     inline void setPredictorTimeStepSize(const double& predictorTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _predictorTimeStepSize = predictorTimeStepSize;
                     }
                     
                     
                     
                     inline double getPredictorTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _predictorTimeStamp;
                     }
                     
                     
                     
                     inline void setPredictorTimeStamp(const double& predictorTimeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _predictorTimeStamp = predictorTimeStamp;
                     }
                     
                     
                     
                     inline double getNextPredictorTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _nextPredictorTimeStepSize;
                     }
                     
                     
                     
                     inline void setNextPredictorTimeStepSize(const double& nextPredictorTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _nextPredictorTimeStepSize = nextPredictorTimeStepSize;
                     }
                     
                     
                     
                     inline int getSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _solution;
                     }
                     
                     
                     
                     inline void setSolution(const int& solution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _solution = solution;
                     }
                     
                     
                     
                     inline int getSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _solutionAverages;
                     }
                     
                     
                     
                     inline void setSolutionAverages(const int& solutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _solutionAverages = solutionAverages;
                     }
                     
                     
                     
                     inline int getUpdate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _update;
                     }
                     
                     
                     
                     inline void setUpdate(const int& update) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _update = update;
                     }
                     
                     
                     
                     inline int getUpdateAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _updateAverages;
                     }
                     
                     
                     
                     inline void setUpdateAverages(const int& updateAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _updateAverages = updateAverages;
                     }
                     
                     
                     
                     inline int getExtrapolatedPredictor() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _extrapolatedPredictor;
                     }
                     
                     
                     
                     inline void setExtrapolatedPredictor(const int& extrapolatedPredictor) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _extrapolatedPredictor = extrapolatedPredictor;
                     }
                     
                     
                     
                     inline int getExtrapolatedPredictorAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _extrapolatedPredictorAverages;
                     }
                     
                     
                     
                     inline void setExtrapolatedPredictorAverages(const int& extrapolatedPredictorAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _extrapolatedPredictorAverages = extrapolatedPredictorAverages;
                     }
                     
                     
                     
                     inline int getFluctuation() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _fluctuation;
                     }
                     
                     
                     
                     inline void setFluctuation(const int& fluctuation) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _fluctuation = fluctuation;
                     }
                     
                     
                     
                     inline int getFluctuationAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _fluctuationAverages;
                     }
                     
                     
                     
                     inline void setFluctuationAverages(const int& fluctuationAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _fluctuationAverages = fluctuationAverages;
                     }
                     
                     
                     
                     inline int getSolutionMin() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _solutionMin;
                     }
                     
                     
                     
                     inline void setSolutionMin(const int& solutionMin) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _solutionMin = solutionMin;
                     }
                     
                     
                     
                     inline int getSolutionMax() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _solutionMax;
                     }
                     
                     
                     
                     inline void setSolutionMax(const int& solutionMax) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _solutionMax = solutionMax;
                     }
                     
                     
                     
                     inline LimiterStatus getLimiterStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _limiterStatus;
                     }
                     
                     
                     
                     inline void setLimiterStatus(const LimiterStatus& limiterStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _limiterStatus = limiterStatus;
                     }
                     
                     
                     
                  };
                  
               private: 
                  PersistentRecords _persistentRecords;
                  bool _skipSolutionUpdate;
                  
               public:
                  /**
                   * Generated
                   */
                  ADERDGCellDescriptionPacked();
                  
                  /**
                   * Generated
                   */
                  ADERDGCellDescriptionPacked(const PersistentRecords& persistentRecords);
                  
                  /**
                   * Generated
                   */
                  ADERDGCellDescriptionPacked(const int& solverNumber, const std::bitset<DIMENSIONS_TIMES_TWO>& riemannSolvePerformed, const std::bitset<DIMENSIONS_TIMES_TWO>& isInside, const bool& hasToHoldDataForNeighbourCommunication, const bool& hasToHoldDataForMasterWorkerCommunication, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& faceDataExchangeCounter, const int& parentIndex, const Type& type, const RefinementEvent& refinementEvent, const int& level, const tarch::la::Vector<DIMENSIONS,double>& offset, const tarch::la::Vector<DIMENSIONS,double>& size, const double& correctorTimeStepSize, const double& correctorTimeStamp, const double& predictorTimeStepSize, const double& predictorTimeStamp, const double& nextPredictorTimeStepSize, const int& solution, const int& solutionAverages, const int& update, const int& updateAverages, const int& extrapolatedPredictor, const int& extrapolatedPredictorAverages, const int& fluctuation, const int& fluctuationAverages, const int& solutionMin, const int& solutionMax, const LimiterStatus& limiterStatus);
                  
                  /**
                   * Generated
                   */
                  ADERDGCellDescriptionPacked(const int& solverNumber, const std::bitset<DIMENSIONS_TIMES_TWO>& riemannSolvePerformed, const std::bitset<DIMENSIONS_TIMES_TWO>& isInside, const bool& hasToHoldDataForNeighbourCommunication, const bool& hasToHoldDataForMasterWorkerCommunication, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& faceDataExchangeCounter, const int& parentIndex, const Type& type, const RefinementEvent& refinementEvent, const int& level, const tarch::la::Vector<DIMENSIONS,double>& offset, const tarch::la::Vector<DIMENSIONS,double>& size, const double& correctorTimeStepSize, const double& correctorTimeStamp, const double& predictorTimeStepSize, const double& predictorTimeStamp, const double& nextPredictorTimeStepSize, const int& solution, const int& solutionAverages, const int& update, const int& updateAverages, const int& extrapolatedPredictor, const int& extrapolatedPredictorAverages, const int& fluctuation, const int& fluctuationAverages, const int& solutionMin, const int& solutionMax, const bool& skipSolutionUpdate, const LimiterStatus& limiterStatus);
                  
                  /**
                   * Generated
                   */
                  ~ADERDGCellDescriptionPacked();
                  
                  
                  inline int getSolverNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._solverNumber;
                  }
                  
                  
                  
                  inline void setSolverNumber(const int& solverNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._solverNumber = solverNumber;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline std::bitset<DIMENSIONS_TIMES_TWO> getRiemannSolvePerformed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._riemannSolvePerformed;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline void setRiemannSolvePerformed(const std::bitset<DIMENSIONS_TIMES_TWO>& riemannSolvePerformed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._riemannSolvePerformed = (riemannSolvePerformed);
                  }
                  
                  
                  
                  inline bool getRiemannSolvePerformed(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                     return _persistentRecords._riemannSolvePerformed[elementIndex];
                     
                  }
                  
                  
                  
                  inline void setRiemannSolvePerformed(int elementIndex, const bool& riemannSolvePerformed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                     _persistentRecords._riemannSolvePerformed[elementIndex]= riemannSolvePerformed;
                     
                  }
                  
                  
                  
                  inline void flipRiemannSolvePerformed(int elementIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                     _persistentRecords._riemannSolvePerformed.flip(elementIndex);
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline std::bitset<DIMENSIONS_TIMES_TWO> getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._isInside;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline void setIsInside(const std::bitset<DIMENSIONS_TIMES_TWO>& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._isInside = (isInside);
                  }
                  
                  
                  
                  inline bool getIsInside(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                     return _persistentRecords._isInside[elementIndex];
                     
                  }
                  
                  
                  
                  inline void setIsInside(int elementIndex, const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                     _persistentRecords._isInside[elementIndex]= isInside;
                     
                  }
                  
                  
                  
                  inline void flipIsInside(int elementIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                     _persistentRecords._isInside.flip(elementIndex);
                  }
                  
                  
                  
                  inline bool getHasToHoldDataForNeighbourCommunication() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._hasToHoldDataForNeighbourCommunication;
                  }
                  
                  
                  
                  inline void setHasToHoldDataForNeighbourCommunication(const bool& hasToHoldDataForNeighbourCommunication) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._hasToHoldDataForNeighbourCommunication = hasToHoldDataForNeighbourCommunication;
                  }
                  
                  
                  
                  inline bool getHasToHoldDataForMasterWorkerCommunication() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._hasToHoldDataForMasterWorkerCommunication;
                  }
                  
                  
                  
                  inline void setHasToHoldDataForMasterWorkerCommunication(const bool& hasToHoldDataForMasterWorkerCommunication) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._hasToHoldDataForMasterWorkerCommunication = hasToHoldDataForMasterWorkerCommunication;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,int> getFaceDataExchangeCounter() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._faceDataExchangeCounter;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline void setFaceDataExchangeCounter(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,int>& faceDataExchangeCounter) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._faceDataExchangeCounter = (faceDataExchangeCounter);
                  }
                  
                  
                  
                  inline int getFaceDataExchangeCounter(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                     return _persistentRecords._faceDataExchangeCounter[elementIndex];
                     
                  }
                  
                  
                  
                  inline void setFaceDataExchangeCounter(int elementIndex, const int& faceDataExchangeCounter) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                     _persistentRecords._faceDataExchangeCounter[elementIndex]= faceDataExchangeCounter;
                     
                  }
                  
                  
                  
                  inline int getParentIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._parentIndex;
                  }
                  
                  
                  
                  inline void setParentIndex(const int& parentIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._parentIndex = parentIndex;
                  }
                  
                  
                  
                  inline Type getType() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._type;
                  }
                  
                  
                  
                  inline void setType(const Type& type) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._type = type;
                  }
                  
                  
                  
                  inline RefinementEvent getRefinementEvent() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._refinementEvent;
                  }
                  
                  
                  
                  inline void setRefinementEvent(const RefinementEvent& refinementEvent) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._refinementEvent = refinementEvent;
                  }
                  
                  
                  
                  inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._level;
                  }
                  
                  
                  
                  inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._level = level;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline tarch::la::Vector<DIMENSIONS,double> getOffset() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._offset;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline void setOffset(const tarch::la::Vector<DIMENSIONS,double>& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._offset = (offset);
                  }
                  
                  
                  
                  inline double getOffset(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<DIMENSIONS);
                     return _persistentRecords._offset[elementIndex];
                     
                  }
                  
                  
                  
                  inline void setOffset(int elementIndex, const double& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<DIMENSIONS);
                     _persistentRecords._offset[elementIndex]= offset;
                     
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline tarch::la::Vector<DIMENSIONS,double> getSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._size;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline void setSize(const tarch::la::Vector<DIMENSIONS,double>& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._size = (size);
                  }
                  
                  
                  
                  inline double getSize(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<DIMENSIONS);
                     return _persistentRecords._size[elementIndex];
                     
                  }
                  
                  
                  
                  inline void setSize(int elementIndex, const double& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<DIMENSIONS);
                     _persistentRecords._size[elementIndex]= size;
                     
                  }
                  
                  
                  
                  inline double getCorrectorTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._correctorTimeStepSize;
                  }
                  
                  
                  
                  inline void setCorrectorTimeStepSize(const double& correctorTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._correctorTimeStepSize = correctorTimeStepSize;
                  }
                  
                  
                  
                  inline double getCorrectorTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._correctorTimeStamp;
                  }
                  
                  
                  
                  inline void setCorrectorTimeStamp(const double& correctorTimeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._correctorTimeStamp = correctorTimeStamp;
                  }
                  
                  
                  
                  inline double getPredictorTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._predictorTimeStepSize;
                  }
                  
                  
                  
                  inline void setPredictorTimeStepSize(const double& predictorTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._predictorTimeStepSize = predictorTimeStepSize;
                  }
                  
                  
                  
                  inline double getPredictorTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._predictorTimeStamp;
                  }
                  
                  
                  
                  inline void setPredictorTimeStamp(const double& predictorTimeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._predictorTimeStamp = predictorTimeStamp;
                  }
                  
                  
                  
                  inline double getNextPredictorTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._nextPredictorTimeStepSize;
                  }
                  
                  
                  
                  inline void setNextPredictorTimeStepSize(const double& nextPredictorTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._nextPredictorTimeStepSize = nextPredictorTimeStepSize;
                  }
                  
                  
                  
                  inline int getSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._solution;
                  }
                  
                  
                  
                  inline void setSolution(const int& solution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._solution = solution;
                  }
                  
                  
                  
                  inline int getSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._solutionAverages;
                  }
                  
                  
                  
                  inline void setSolutionAverages(const int& solutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._solutionAverages = solutionAverages;
                  }
                  
                  
                  
                  inline int getUpdate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._update;
                  }
                  
                  
                  
                  inline void setUpdate(const int& update) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._update = update;
                  }
                  
                  
                  
                  inline int getUpdateAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._updateAverages;
                  }
                  
                  
                  
                  inline void setUpdateAverages(const int& updateAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._updateAverages = updateAverages;
                  }
                  
                  
                  
                  inline int getExtrapolatedPredictor() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._extrapolatedPredictor;
                  }
                  
                  
                  
                  inline void setExtrapolatedPredictor(const int& extrapolatedPredictor) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._extrapolatedPredictor = extrapolatedPredictor;
                  }
                  
                  
                  
                  inline int getExtrapolatedPredictorAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._extrapolatedPredictorAverages;
                  }
                  
                  
                  
                  inline void setExtrapolatedPredictorAverages(const int& extrapolatedPredictorAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._extrapolatedPredictorAverages = extrapolatedPredictorAverages;
                  }
                  
                  
                  
                  inline int getFluctuation() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._fluctuation;
                  }
                  
                  
                  
                  inline void setFluctuation(const int& fluctuation) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._fluctuation = fluctuation;
                  }
                  
                  
                  
                  inline int getFluctuationAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._fluctuationAverages;
                  }
                  
                  
                  
                  inline void setFluctuationAverages(const int& fluctuationAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._fluctuationAverages = fluctuationAverages;
                  }
                  
                  
                  
                  inline int getSolutionMin() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._solutionMin;
                  }
                  
                  
                  
                  inline void setSolutionMin(const int& solutionMin) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._solutionMin = solutionMin;
                  }
                  
                  
                  
                  inline int getSolutionMax() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._solutionMax;
                  }
                  
                  
                  
                  inline void setSolutionMax(const int& solutionMax) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._solutionMax = solutionMax;
                  }
                  
                  
                  
                  inline bool getSkipSolutionUpdate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _skipSolutionUpdate;
                  }
                  
                  
                  
                  inline void setSkipSolutionUpdate(const bool& skipSolutionUpdate) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _skipSolutionUpdate = skipSolutionUpdate;
                  }
                  
                  
                  
                  inline LimiterStatus getLimiterStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._limiterStatus;
                  }
                  
                  
                  
                  inline void setLimiterStatus(const LimiterStatus& limiterStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._limiterStatus = limiterStatus;
                  }
                  
                  
                  /**
                   * Generated
                   */
                  static std::string toString(const Type& param);
                  
                  /**
                   * Generated
                   */
                  static std::string getTypeMapping();
                  
                  /**
                   * Generated
                   */
                  static std::string toString(const RefinementEvent& param);
                  
                  /**
                   * Generated
                   */
                  static std::string getRefinementEventMapping();
                  
                  /**
                   * Generated
                   */
                  static std::string toString(const LimiterStatus& param);
                  
                  /**
                   * Generated
                   */
                  static std::string getLimiterStatusMapping();
                  
                  /**
                   * Generated
                   */
                  std::string toString() const;
                  
                  /**
                   * Generated
                   */
                  void toString(std::ostream& out) const;
                  
                  
                  PersistentRecords getPersistentRecords() const;
                  /**
                   * Generated
                   */
                  ADERDGCellDescription convert() const;
                  
                  
               #ifdef Parallel
                  protected:
                     static tarch::logging::Log _log;
                     
                  public:
                     
                     /**
                      * Global that represents the mpi datatype.
                      * There are two variants: Datatype identifies only those attributes marked with
                      * parallelise. FullDatatype instead identifies the whole record with all fields.
                      */
                     static MPI_Datatype Datatype;
                     static MPI_Datatype FullDatatype;
                     
                     /**
                      * Initializes the data type for the mpi operations. Has to be called
                      * before the very first send or receive operation is called.
                      */
                     static void initDatatype();
                     
                     static void shutdownDatatype();
                     
                     /**
                      * @param communicateSleep -1 Data exchange through blocking mpi
                      * @param communicateSleep  0 Data exchange through non-blocking mpi, i.e. pending messages are received via polling until MPI_Test succeeds
                      * @param communicateSleep >0 Same as 0 but in addition, each unsuccessful MPI_Test is follows by an usleep
                      */
                     void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
                     
                     void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
                     
                     static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                     
                     #endif
                        
                     };
                     
                     #ifdef PackedRecords
                     #pragma pack (pop)
                     #endif
                     
                     
                     
                  #elif !defined(Parallel)
                     /**
                      * @author This class is generated by DaStGen
                      * 		   DataStructureGenerator (DaStGen)
                      * 		   2007-2009 Wolfgang Eckhardt
                      * 		   2012      Tobias Weinzierl
                      *
                      * 		   build date: 09-02-2014 14:40
                      *
                      * @date   07/10/2016 15:03
                      */
                     class exahype::records::ADERDGCellDescription { 
                        
                        public:
                           
                           typedef exahype::records::ADERDGCellDescriptionPacked Packed;
                           
                           enum LimiterStatus {
                              Ok = 0, Troubled = 1, NeighbourIsTroubledCell = 2, NeighbourIsNeighbourOfTroubledCell = 3
                           };
                           
                           enum RefinementEvent {
                              None = 0, ErasingChildrenRequested = 1, ErasingChildren = 2, ChangeChildrenToDescendantsRequested = 3, ChangeChildrenToDescendants = 4, RefiningRequested = 5, Refining = 6, DeaugmentingChildrenRequestedTriggered = 7, DeaugmentingChildrenRequested = 8, DeaugmentingChildren = 9, AugmentingRequested = 10, Augmenting = 11
                           };
                           
                           enum Type {
                              Erased = 0, Ancestor = 1, EmptyAncestor = 2, Cell = 3, Descendant = 4, EmptyDescendant = 5
                           };
                           
                           struct PersistentRecords {
                              int _solverNumber;
                              #ifdef UseManualAlignment
                              std::bitset<DIMENSIONS_TIMES_TWO> _riemannSolvePerformed __attribute__((aligned(VectorisationAlignment)));
                              #else
                              std::bitset<DIMENSIONS_TIMES_TWO> _riemannSolvePerformed;
                              #endif
                              #ifdef UseManualAlignment
                              std::bitset<DIMENSIONS_TIMES_TWO> _isInside __attribute__((aligned(VectorisationAlignment)));
                              #else
                              std::bitset<DIMENSIONS_TIMES_TWO> _isInside;
                              #endif
                              int _parentIndex;
                              Type _type;
                              RefinementEvent _refinementEvent;
                              int _level;
                              #ifdef UseManualAlignment
                              tarch::la::Vector<DIMENSIONS,double> _offset __attribute__((aligned(VectorisationAlignment)));
                              #else
                              tarch::la::Vector<DIMENSIONS,double> _offset;
                              #endif
                              #ifdef UseManualAlignment
                              tarch::la::Vector<DIMENSIONS,double> _size __attribute__((aligned(VectorisationAlignment)));
                              #else
                              tarch::la::Vector<DIMENSIONS,double> _size;
                              #endif
                              double _correctorTimeStepSize;
                              double _correctorTimeStamp;
                              double _predictorTimeStepSize;
                              double _predictorTimeStamp;
                              double _nextPredictorTimeStepSize;
                              int _solution;
                              int _solutionAverages;
                              int _update;
                              int _updateAverages;
                              int _extrapolatedPredictor;
                              int _extrapolatedPredictorAverages;
                              int _fluctuation;
                              int _fluctuationAverages;
                              int _solutionMin;
                              int _solutionMax;
                              LimiterStatus _limiterStatus;
                              /**
                               * Generated
                               */
                              PersistentRecords();
                              
                              /**
                               * Generated
                               */
                              PersistentRecords(const int& solverNumber, const std::bitset<DIMENSIONS_TIMES_TWO>& riemannSolvePerformed, const std::bitset<DIMENSIONS_TIMES_TWO>& isInside, const int& parentIndex, const Type& type, const RefinementEvent& refinementEvent, const int& level, const tarch::la::Vector<DIMENSIONS,double>& offset, const tarch::la::Vector<DIMENSIONS,double>& size, const double& correctorTimeStepSize, const double& correctorTimeStamp, const double& predictorTimeStepSize, const double& predictorTimeStamp, const double& nextPredictorTimeStepSize, const int& solution, const int& solutionAverages, const int& update, const int& updateAverages, const int& extrapolatedPredictor, const int& extrapolatedPredictorAverages, const int& fluctuation, const int& fluctuationAverages, const int& solutionMin, const int& solutionMax, const LimiterStatus& limiterStatus);
                              
                              
                              inline int getSolverNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _solverNumber;
                              }
                              
                              
                              
                              inline void setSolverNumber(const int& solverNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _solverNumber = solverNumber;
                              }
                              
                              
                              
                              /**
                               * Generated and optimized
                               * 
                               * If you realise a for loop using exclusively arrays (vectors) and compile 
                               * with -DUseManualAlignment you may add 
                               * \code
                               #pragma vector aligned
                               #pragma simd
                               \endcode to this for loop to enforce your compiler to use SSE/AVX.
                               * 
                               * The alignment is tied to the unpacked records, i.e. for packed class
                               * variants the machine's natural alignment is switched off to recude the  
                               * memory footprint. Do not use any SSE/AVX operations or 
                               * vectorisation on the result for the packed variants, as the data is misaligned. 
                               * If you rely on vectorisation, convert the underlying record 
                               * into the unpacked version first. 
                               * 
                               * @see convert()
                               */
                              inline std::bitset<DIMENSIONS_TIMES_TWO> getRiemannSolvePerformed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _riemannSolvePerformed;
                              }
                              
                              
                              
                              /**
                               * Generated and optimized
                               * 
                               * If you realise a for loop using exclusively arrays (vectors) and compile 
                               * with -DUseManualAlignment you may add 
                               * \code
                               #pragma vector aligned
                               #pragma simd
                               \endcode to this for loop to enforce your compiler to use SSE/AVX.
                               * 
                               * The alignment is tied to the unpacked records, i.e. for packed class
                               * variants the machine's natural alignment is switched off to recude the  
                               * memory footprint. Do not use any SSE/AVX operations or 
                               * vectorisation on the result for the packed variants, as the data is misaligned. 
                               * If you rely on vectorisation, convert the underlying record 
                               * into the unpacked version first. 
                               * 
                               * @see convert()
                               */
                              inline void setRiemannSolvePerformed(const std::bitset<DIMENSIONS_TIMES_TWO>& riemannSolvePerformed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _riemannSolvePerformed = (riemannSolvePerformed);
                              }
                              
                              
                              
                              /**
                               * Generated and optimized
                               * 
                               * If you realise a for loop using exclusively arrays (vectors) and compile 
                               * with -DUseManualAlignment you may add 
                               * \code
                               #pragma vector aligned
                               #pragma simd
                               \endcode to this for loop to enforce your compiler to use SSE/AVX.
                               * 
                               * The alignment is tied to the unpacked records, i.e. for packed class
                               * variants the machine's natural alignment is switched off to recude the  
                               * memory footprint. Do not use any SSE/AVX operations or 
                               * vectorisation on the result for the packed variants, as the data is misaligned. 
                               * If you rely on vectorisation, convert the underlying record 
                               * into the unpacked version first. 
                               * 
                               * @see convert()
                               */
                              inline std::bitset<DIMENSIONS_TIMES_TWO> getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _isInside;
                              }
                              
                              
                              
                              /**
                               * Generated and optimized
                               * 
                               * If you realise a for loop using exclusively arrays (vectors) and compile 
                               * with -DUseManualAlignment you may add 
                               * \code
                               #pragma vector aligned
                               #pragma simd
                               \endcode to this for loop to enforce your compiler to use SSE/AVX.
                               * 
                               * The alignment is tied to the unpacked records, i.e. for packed class
                               * variants the machine's natural alignment is switched off to recude the  
                               * memory footprint. Do not use any SSE/AVX operations or 
                               * vectorisation on the result for the packed variants, as the data is misaligned. 
                               * If you rely on vectorisation, convert the underlying record 
                               * into the unpacked version first. 
                               * 
                               * @see convert()
                               */
                              inline void setIsInside(const std::bitset<DIMENSIONS_TIMES_TWO>& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _isInside = (isInside);
                              }
                              
                              
                              
                              inline int getParentIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _parentIndex;
                              }
                              
                              
                              
                              inline void setParentIndex(const int& parentIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _parentIndex = parentIndex;
                              }
                              
                              
                              
                              inline Type getType() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _type;
                              }
                              
                              
                              
                              inline void setType(const Type& type) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _type = type;
                              }
                              
                              
                              
                              inline RefinementEvent getRefinementEvent() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _refinementEvent;
                              }
                              
                              
                              
                              inline void setRefinementEvent(const RefinementEvent& refinementEvent) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _refinementEvent = refinementEvent;
                              }
                              
                              
                              
                              inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _level;
                              }
                              
                              
                              
                              inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _level = level;
                              }
                              
                              
                              
                              /**
                               * Generated and optimized
                               * 
                               * If you realise a for loop using exclusively arrays (vectors) and compile 
                               * with -DUseManualAlignment you may add 
                               * \code
                               #pragma vector aligned
                               #pragma simd
                               \endcode to this for loop to enforce your compiler to use SSE/AVX.
                               * 
                               * The alignment is tied to the unpacked records, i.e. for packed class
                               * variants the machine's natural alignment is switched off to recude the  
                               * memory footprint. Do not use any SSE/AVX operations or 
                               * vectorisation on the result for the packed variants, as the data is misaligned. 
                               * If you rely on vectorisation, convert the underlying record 
                               * into the unpacked version first. 
                               * 
                               * @see convert()
                               */
                              inline tarch::la::Vector<DIMENSIONS,double> getOffset() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _offset;
                              }
                              
                              
                              
                              /**
                               * Generated and optimized
                               * 
                               * If you realise a for loop using exclusively arrays (vectors) and compile 
                               * with -DUseManualAlignment you may add 
                               * \code
                               #pragma vector aligned
                               #pragma simd
                               \endcode to this for loop to enforce your compiler to use SSE/AVX.
                               * 
                               * The alignment is tied to the unpacked records, i.e. for packed class
                               * variants the machine's natural alignment is switched off to recude the  
                               * memory footprint. Do not use any SSE/AVX operations or 
                               * vectorisation on the result for the packed variants, as the data is misaligned. 
                               * If you rely on vectorisation, convert the underlying record 
                               * into the unpacked version first. 
                               * 
                               * @see convert()
                               */
                              inline void setOffset(const tarch::la::Vector<DIMENSIONS,double>& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _offset = (offset);
                              }
                              
                              
                              
                              /**
                               * Generated and optimized
                               * 
                               * If you realise a for loop using exclusively arrays (vectors) and compile 
                               * with -DUseManualAlignment you may add 
                               * \code
                               #pragma vector aligned
                               #pragma simd
                               \endcode to this for loop to enforce your compiler to use SSE/AVX.
                               * 
                               * The alignment is tied to the unpacked records, i.e. for packed class
                               * variants the machine's natural alignment is switched off to recude the  
                               * memory footprint. Do not use any SSE/AVX operations or 
                               * vectorisation on the result for the packed variants, as the data is misaligned. 
                               * If you rely on vectorisation, convert the underlying record 
                               * into the unpacked version first. 
                               * 
                               * @see convert()
                               */
                              inline tarch::la::Vector<DIMENSIONS,double> getSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _size;
                              }
                              
                              
                              
                              /**
                               * Generated and optimized
                               * 
                               * If you realise a for loop using exclusively arrays (vectors) and compile 
                               * with -DUseManualAlignment you may add 
                               * \code
                               #pragma vector aligned
                               #pragma simd
                               \endcode to this for loop to enforce your compiler to use SSE/AVX.
                               * 
                               * The alignment is tied to the unpacked records, i.e. for packed class
                               * variants the machine's natural alignment is switched off to recude the  
                               * memory footprint. Do not use any SSE/AVX operations or 
                               * vectorisation on the result for the packed variants, as the data is misaligned. 
                               * If you rely on vectorisation, convert the underlying record 
                               * into the unpacked version first. 
                               * 
                               * @see convert()
                               */
                              inline void setSize(const tarch::la::Vector<DIMENSIONS,double>& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _size = (size);
                              }
                              
                              
                              
                              inline double getCorrectorTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _correctorTimeStepSize;
                              }
                              
                              
                              
                              inline void setCorrectorTimeStepSize(const double& correctorTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _correctorTimeStepSize = correctorTimeStepSize;
                              }
                              
                              
                              
                              inline double getCorrectorTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _correctorTimeStamp;
                              }
                              
                              
                              
                              inline void setCorrectorTimeStamp(const double& correctorTimeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _correctorTimeStamp = correctorTimeStamp;
                              }
                              
                              
                              
                              inline double getPredictorTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _predictorTimeStepSize;
                              }
                              
                              
                              
                              inline void setPredictorTimeStepSize(const double& predictorTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _predictorTimeStepSize = predictorTimeStepSize;
                              }
                              
                              
                              
                              inline double getPredictorTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _predictorTimeStamp;
                              }
                              
                              
                              
                              inline void setPredictorTimeStamp(const double& predictorTimeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _predictorTimeStamp = predictorTimeStamp;
                              }
                              
                              
                              
                              inline double getNextPredictorTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _nextPredictorTimeStepSize;
                              }
                              
                              
                              
                              inline void setNextPredictorTimeStepSize(const double& nextPredictorTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _nextPredictorTimeStepSize = nextPredictorTimeStepSize;
                              }
                              
                              
                              
                              inline int getSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _solution;
                              }
                              
                              
                              
                              inline void setSolution(const int& solution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _solution = solution;
                              }
                              
                              
                              
                              inline int getSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _solutionAverages;
                              }
                              
                              
                              
                              inline void setSolutionAverages(const int& solutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _solutionAverages = solutionAverages;
                              }
                              
                              
                              
                              inline int getUpdate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _update;
                              }
                              
                              
                              
                              inline void setUpdate(const int& update) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _update = update;
                              }
                              
                              
                              
                              inline int getUpdateAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _updateAverages;
                              }
                              
                              
                              
                              inline void setUpdateAverages(const int& updateAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _updateAverages = updateAverages;
                              }
                              
                              
                              
                              inline int getExtrapolatedPredictor() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _extrapolatedPredictor;
                              }
                              
                              
                              
                              inline void setExtrapolatedPredictor(const int& extrapolatedPredictor) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _extrapolatedPredictor = extrapolatedPredictor;
                              }
                              
                              
                              
                              inline int getExtrapolatedPredictorAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _extrapolatedPredictorAverages;
                              }
                              
                              
                              
                              inline void setExtrapolatedPredictorAverages(const int& extrapolatedPredictorAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _extrapolatedPredictorAverages = extrapolatedPredictorAverages;
                              }
                              
                              
                              
                              inline int getFluctuation() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _fluctuation;
                              }
                              
                              
                              
                              inline void setFluctuation(const int& fluctuation) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _fluctuation = fluctuation;
                              }
                              
                              
                              
                              inline int getFluctuationAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _fluctuationAverages;
                              }
                              
                              
                              
                              inline void setFluctuationAverages(const int& fluctuationAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _fluctuationAverages = fluctuationAverages;
                              }
                              
                              
                              
                              inline int getSolutionMin() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _solutionMin;
                              }
                              
                              
                              
                              inline void setSolutionMin(const int& solutionMin) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _solutionMin = solutionMin;
                              }
                              
                              
                              
                              inline int getSolutionMax() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _solutionMax;
                              }
                              
                              
                              
                              inline void setSolutionMax(const int& solutionMax) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _solutionMax = solutionMax;
                              }
                              
                              
                              
                              inline LimiterStatus getLimiterStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _limiterStatus;
                              }
                              
                              
                              
                              inline void setLimiterStatus(const LimiterStatus& limiterStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _limiterStatus = limiterStatus;
                              }
                              
                              
                              
                           };
                           
                        private: 
                           PersistentRecords _persistentRecords;
                           bool _skipSolutionUpdate;
                           
                        public:
                           /**
                            * Generated
                            */
                           ADERDGCellDescription();
                           
                           /**
                            * Generated
                            */
                           ADERDGCellDescription(const PersistentRecords& persistentRecords);
                           
                           /**
                            * Generated
                            */
                           ADERDGCellDescription(const int& solverNumber, const std::bitset<DIMENSIONS_TIMES_TWO>& riemannSolvePerformed, const std::bitset<DIMENSIONS_TIMES_TWO>& isInside, const int& parentIndex, const Type& type, const RefinementEvent& refinementEvent, const int& level, const tarch::la::Vector<DIMENSIONS,double>& offset, const tarch::la::Vector<DIMENSIONS,double>& size, const double& correctorTimeStepSize, const double& correctorTimeStamp, const double& predictorTimeStepSize, const double& predictorTimeStamp, const double& nextPredictorTimeStepSize, const int& solution, const int& solutionAverages, const int& update, const int& updateAverages, const int& extrapolatedPredictor, const int& extrapolatedPredictorAverages, const int& fluctuation, const int& fluctuationAverages, const int& solutionMin, const int& solutionMax, const LimiterStatus& limiterStatus);
                           
                           /**
                            * Generated
                            */
                           ADERDGCellDescription(const int& solverNumber, const std::bitset<DIMENSIONS_TIMES_TWO>& riemannSolvePerformed, const std::bitset<DIMENSIONS_TIMES_TWO>& isInside, const int& parentIndex, const Type& type, const RefinementEvent& refinementEvent, const int& level, const tarch::la::Vector<DIMENSIONS,double>& offset, const tarch::la::Vector<DIMENSIONS,double>& size, const double& correctorTimeStepSize, const double& correctorTimeStamp, const double& predictorTimeStepSize, const double& predictorTimeStamp, const double& nextPredictorTimeStepSize, const int& solution, const int& solutionAverages, const int& update, const int& updateAverages, const int& extrapolatedPredictor, const int& extrapolatedPredictorAverages, const int& fluctuation, const int& fluctuationAverages, const int& solutionMin, const int& solutionMax, const bool& skipSolutionUpdate, const LimiterStatus& limiterStatus);
                           
                           /**
                            * Generated
                            */
                           ~ADERDGCellDescription();
                           
                           
                           inline int getSolverNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._solverNumber;
                           }
                           
                           
                           
                           inline void setSolverNumber(const int& solverNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._solverNumber = solverNumber;
                           }
                           
                           
                           
                           /**
                            * Generated and optimized
                            * 
                            * If you realise a for loop using exclusively arrays (vectors) and compile 
                            * with -DUseManualAlignment you may add 
                            * \code
                            #pragma vector aligned
                            #pragma simd
                            \endcode to this for loop to enforce your compiler to use SSE/AVX.
                            * 
                            * The alignment is tied to the unpacked records, i.e. for packed class
                            * variants the machine's natural alignment is switched off to recude the  
                            * memory footprint. Do not use any SSE/AVX operations or 
                            * vectorisation on the result for the packed variants, as the data is misaligned. 
                            * If you rely on vectorisation, convert the underlying record 
                            * into the unpacked version first. 
                            * 
                            * @see convert()
                            */
                           inline std::bitset<DIMENSIONS_TIMES_TWO> getRiemannSolvePerformed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._riemannSolvePerformed;
                           }
                           
                           
                           
                           /**
                            * Generated and optimized
                            * 
                            * If you realise a for loop using exclusively arrays (vectors) and compile 
                            * with -DUseManualAlignment you may add 
                            * \code
                            #pragma vector aligned
                            #pragma simd
                            \endcode to this for loop to enforce your compiler to use SSE/AVX.
                            * 
                            * The alignment is tied to the unpacked records, i.e. for packed class
                            * variants the machine's natural alignment is switched off to recude the  
                            * memory footprint. Do not use any SSE/AVX operations or 
                            * vectorisation on the result for the packed variants, as the data is misaligned. 
                            * If you rely on vectorisation, convert the underlying record 
                            * into the unpacked version first. 
                            * 
                            * @see convert()
                            */
                           inline void setRiemannSolvePerformed(const std::bitset<DIMENSIONS_TIMES_TWO>& riemannSolvePerformed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._riemannSolvePerformed = (riemannSolvePerformed);
                           }
                           
                           
                           
                           inline bool getRiemannSolvePerformed(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              assertion(elementIndex>=0);
                              assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                              return _persistentRecords._riemannSolvePerformed[elementIndex];
                              
                           }
                           
                           
                           
                           inline void setRiemannSolvePerformed(int elementIndex, const bool& riemannSolvePerformed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              assertion(elementIndex>=0);
                              assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                              _persistentRecords._riemannSolvePerformed[elementIndex]= riemannSolvePerformed;
                              
                           }
                           
                           
                           
                           inline void flipRiemannSolvePerformed(int elementIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              assertion(elementIndex>=0);
                              assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                              _persistentRecords._riemannSolvePerformed.flip(elementIndex);
                           }
                           
                           
                           
                           /**
                            * Generated and optimized
                            * 
                            * If you realise a for loop using exclusively arrays (vectors) and compile 
                            * with -DUseManualAlignment you may add 
                            * \code
                            #pragma vector aligned
                            #pragma simd
                            \endcode to this for loop to enforce your compiler to use SSE/AVX.
                            * 
                            * The alignment is tied to the unpacked records, i.e. for packed class
                            * variants the machine's natural alignment is switched off to recude the  
                            * memory footprint. Do not use any SSE/AVX operations or 
                            * vectorisation on the result for the packed variants, as the data is misaligned. 
                            * If you rely on vectorisation, convert the underlying record 
                            * into the unpacked version first. 
                            * 
                            * @see convert()
                            */
                           inline std::bitset<DIMENSIONS_TIMES_TWO> getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._isInside;
                           }
                           
                           
                           
                           /**
                            * Generated and optimized
                            * 
                            * If you realise a for loop using exclusively arrays (vectors) and compile 
                            * with -DUseManualAlignment you may add 
                            * \code
                            #pragma vector aligned
                            #pragma simd
                            \endcode to this for loop to enforce your compiler to use SSE/AVX.
                            * 
                            * The alignment is tied to the unpacked records, i.e. for packed class
                            * variants the machine's natural alignment is switched off to recude the  
                            * memory footprint. Do not use any SSE/AVX operations or 
                            * vectorisation on the result for the packed variants, as the data is misaligned. 
                            * If you rely on vectorisation, convert the underlying record 
                            * into the unpacked version first. 
                            * 
                            * @see convert()
                            */
                           inline void setIsInside(const std::bitset<DIMENSIONS_TIMES_TWO>& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._isInside = (isInside);
                           }
                           
                           
                           
                           inline bool getIsInside(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              assertion(elementIndex>=0);
                              assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                              return _persistentRecords._isInside[elementIndex];
                              
                           }
                           
                           
                           
                           inline void setIsInside(int elementIndex, const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              assertion(elementIndex>=0);
                              assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                              _persistentRecords._isInside[elementIndex]= isInside;
                              
                           }
                           
                           
                           
                           inline void flipIsInside(int elementIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              assertion(elementIndex>=0);
                              assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                              _persistentRecords._isInside.flip(elementIndex);
                           }
                           
                           
                           
                           inline int getParentIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._parentIndex;
                           }
                           
                           
                           
                           inline void setParentIndex(const int& parentIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._parentIndex = parentIndex;
                           }
                           
                           
                           
                           inline Type getType() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._type;
                           }
                           
                           
                           
                           inline void setType(const Type& type) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._type = type;
                           }
                           
                           
                           
                           inline RefinementEvent getRefinementEvent() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._refinementEvent;
                           }
                           
                           
                           
                           inline void setRefinementEvent(const RefinementEvent& refinementEvent) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._refinementEvent = refinementEvent;
                           }
                           
                           
                           
                           inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._level;
                           }
                           
                           
                           
                           inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._level = level;
                           }
                           
                           
                           
                           /**
                            * Generated and optimized
                            * 
                            * If you realise a for loop using exclusively arrays (vectors) and compile 
                            * with -DUseManualAlignment you may add 
                            * \code
                            #pragma vector aligned
                            #pragma simd
                            \endcode to this for loop to enforce your compiler to use SSE/AVX.
                            * 
                            * The alignment is tied to the unpacked records, i.e. for packed class
                            * variants the machine's natural alignment is switched off to recude the  
                            * memory footprint. Do not use any SSE/AVX operations or 
                            * vectorisation on the result for the packed variants, as the data is misaligned. 
                            * If you rely on vectorisation, convert the underlying record 
                            * into the unpacked version first. 
                            * 
                            * @see convert()
                            */
                           inline tarch::la::Vector<DIMENSIONS,double> getOffset() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._offset;
                           }
                           
                           
                           
                           /**
                            * Generated and optimized
                            * 
                            * If you realise a for loop using exclusively arrays (vectors) and compile 
                            * with -DUseManualAlignment you may add 
                            * \code
                            #pragma vector aligned
                            #pragma simd
                            \endcode to this for loop to enforce your compiler to use SSE/AVX.
                            * 
                            * The alignment is tied to the unpacked records, i.e. for packed class
                            * variants the machine's natural alignment is switched off to recude the  
                            * memory footprint. Do not use any SSE/AVX operations or 
                            * vectorisation on the result for the packed variants, as the data is misaligned. 
                            * If you rely on vectorisation, convert the underlying record 
                            * into the unpacked version first. 
                            * 
                            * @see convert()
                            */
                           inline void setOffset(const tarch::la::Vector<DIMENSIONS,double>& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._offset = (offset);
                           }
                           
                           
                           
                           inline double getOffset(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              assertion(elementIndex>=0);
                              assertion(elementIndex<DIMENSIONS);
                              return _persistentRecords._offset[elementIndex];
                              
                           }
                           
                           
                           
                           inline void setOffset(int elementIndex, const double& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              assertion(elementIndex>=0);
                              assertion(elementIndex<DIMENSIONS);
                              _persistentRecords._offset[elementIndex]= offset;
                              
                           }
                           
                           
                           
                           /**
                            * Generated and optimized
                            * 
                            * If you realise a for loop using exclusively arrays (vectors) and compile 
                            * with -DUseManualAlignment you may add 
                            * \code
                            #pragma vector aligned
                            #pragma simd
                            \endcode to this for loop to enforce your compiler to use SSE/AVX.
                            * 
                            * The alignment is tied to the unpacked records, i.e. for packed class
                            * variants the machine's natural alignment is switched off to recude the  
                            * memory footprint. Do not use any SSE/AVX operations or 
                            * vectorisation on the result for the packed variants, as the data is misaligned. 
                            * If you rely on vectorisation, convert the underlying record 
                            * into the unpacked version first. 
                            * 
                            * @see convert()
                            */
                           inline tarch::la::Vector<DIMENSIONS,double> getSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._size;
                           }
                           
                           
                           
                           /**
                            * Generated and optimized
                            * 
                            * If you realise a for loop using exclusively arrays (vectors) and compile 
                            * with -DUseManualAlignment you may add 
                            * \code
                            #pragma vector aligned
                            #pragma simd
                            \endcode to this for loop to enforce your compiler to use SSE/AVX.
                            * 
                            * The alignment is tied to the unpacked records, i.e. for packed class
                            * variants the machine's natural alignment is switched off to recude the  
                            * memory footprint. Do not use any SSE/AVX operations or 
                            * vectorisation on the result for the packed variants, as the data is misaligned. 
                            * If you rely on vectorisation, convert the underlying record 
                            * into the unpacked version first. 
                            * 
                            * @see convert()
                            */
                           inline void setSize(const tarch::la::Vector<DIMENSIONS,double>& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._size = (size);
                           }
                           
                           
                           
                           inline double getSize(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              assertion(elementIndex>=0);
                              assertion(elementIndex<DIMENSIONS);
                              return _persistentRecords._size[elementIndex];
                              
                           }
                           
                           
                           
                           inline void setSize(int elementIndex, const double& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              assertion(elementIndex>=0);
                              assertion(elementIndex<DIMENSIONS);
                              _persistentRecords._size[elementIndex]= size;
                              
                           }
                           
                           
                           
                           inline double getCorrectorTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._correctorTimeStepSize;
                           }
                           
                           
                           
                           inline void setCorrectorTimeStepSize(const double& correctorTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._correctorTimeStepSize = correctorTimeStepSize;
                           }
                           
                           
                           
                           inline double getCorrectorTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._correctorTimeStamp;
                           }
                           
                           
                           
                           inline void setCorrectorTimeStamp(const double& correctorTimeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._correctorTimeStamp = correctorTimeStamp;
                           }
                           
                           
                           
                           inline double getPredictorTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._predictorTimeStepSize;
                           }
                           
                           
                           
                           inline void setPredictorTimeStepSize(const double& predictorTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._predictorTimeStepSize = predictorTimeStepSize;
                           }
                           
                           
                           
                           inline double getPredictorTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._predictorTimeStamp;
                           }
                           
                           
                           
                           inline void setPredictorTimeStamp(const double& predictorTimeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._predictorTimeStamp = predictorTimeStamp;
                           }
                           
                           
                           
                           inline double getNextPredictorTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._nextPredictorTimeStepSize;
                           }
                           
                           
                           
                           inline void setNextPredictorTimeStepSize(const double& nextPredictorTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._nextPredictorTimeStepSize = nextPredictorTimeStepSize;
                           }
                           
                           
                           
                           inline int getSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._solution;
                           }
                           
                           
                           
                           inline void setSolution(const int& solution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._solution = solution;
                           }
                           
                           
                           
                           inline int getSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._solutionAverages;
                           }
                           
                           
                           
                           inline void setSolutionAverages(const int& solutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._solutionAverages = solutionAverages;
                           }
                           
                           
                           
                           inline int getUpdate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._update;
                           }
                           
                           
                           
                           inline void setUpdate(const int& update) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._update = update;
                           }
                           
                           
                           
                           inline int getUpdateAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._updateAverages;
                           }
                           
                           
                           
                           inline void setUpdateAverages(const int& updateAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._updateAverages = updateAverages;
                           }
                           
                           
                           
                           inline int getExtrapolatedPredictor() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._extrapolatedPredictor;
                           }
                           
                           
                           
                           inline void setExtrapolatedPredictor(const int& extrapolatedPredictor) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._extrapolatedPredictor = extrapolatedPredictor;
                           }
                           
                           
                           
                           inline int getExtrapolatedPredictorAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._extrapolatedPredictorAverages;
                           }
                           
                           
                           
                           inline void setExtrapolatedPredictorAverages(const int& extrapolatedPredictorAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._extrapolatedPredictorAverages = extrapolatedPredictorAverages;
                           }
                           
                           
                           
                           inline int getFluctuation() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._fluctuation;
                           }
                           
                           
                           
                           inline void setFluctuation(const int& fluctuation) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._fluctuation = fluctuation;
                           }
                           
                           
                           
                           inline int getFluctuationAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._fluctuationAverages;
                           }
                           
                           
                           
                           inline void setFluctuationAverages(const int& fluctuationAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._fluctuationAverages = fluctuationAverages;
                           }
                           
                           
                           
                           inline int getSolutionMin() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._solutionMin;
                           }
                           
                           
                           
                           inline void setSolutionMin(const int& solutionMin) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._solutionMin = solutionMin;
                           }
                           
                           
                           
                           inline int getSolutionMax() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._solutionMax;
                           }
                           
                           
                           
                           inline void setSolutionMax(const int& solutionMax) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._solutionMax = solutionMax;
                           }
                           
                           
                           
                           inline bool getSkipSolutionUpdate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _skipSolutionUpdate;
                           }
                           
                           
                           
                           inline void setSkipSolutionUpdate(const bool& skipSolutionUpdate) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _skipSolutionUpdate = skipSolutionUpdate;
                           }
                           
                           
                           
                           inline LimiterStatus getLimiterStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._limiterStatus;
                           }
                           
                           
                           
                           inline void setLimiterStatus(const LimiterStatus& limiterStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._limiterStatus = limiterStatus;
                           }
                           
                           
                           /**
                            * Generated
                            */
                           static std::string toString(const LimiterStatus& param);
                           
                           /**
                            * Generated
                            */
                           static std::string getLimiterStatusMapping();
                           
                           /**
                            * Generated
                            */
                           static std::string toString(const RefinementEvent& param);
                           
                           /**
                            * Generated
                            */
                           static std::string getRefinementEventMapping();
                           
                           /**
                            * Generated
                            */
                           static std::string toString(const Type& param);
                           
                           /**
                            * Generated
                            */
                           static std::string getTypeMapping();
                           
                           /**
                            * Generated
                            */
                           std::string toString() const;
                           
                           /**
                            * Generated
                            */
                           void toString(std::ostream& out) const;
                           
                           
                           PersistentRecords getPersistentRecords() const;
                           /**
                            * Generated
                            */
                           ADERDGCellDescriptionPacked convert() const;
                           
                           
                        #ifdef Parallel
                           protected:
                              static tarch::logging::Log _log;
                              
                           public:
                              
                              /**
                               * Global that represents the mpi datatype.
                               * There are two variants: Datatype identifies only those attributes marked with
                               * parallelise. FullDatatype instead identifies the whole record with all fields.
                               */
                              static MPI_Datatype Datatype;
                              static MPI_Datatype FullDatatype;
                              
                              /**
                               * Initializes the data type for the mpi operations. Has to be called
                               * before the very first send or receive operation is called.
                               */
                              static void initDatatype();
                              
                              static void shutdownDatatype();
                              
                              /**
                               * @param communicateSleep -1 Data exchange through blocking mpi
                               * @param communicateSleep  0 Data exchange through non-blocking mpi, i.e. pending messages are received via polling until MPI_Test succeeds
                               * @param communicateSleep >0 Same as 0 but in addition, each unsuccessful MPI_Test is follows by an usleep
                               */
                              void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
                              
                              void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
                              
                              static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                              
                              #endif
                                 
                              };
                              
                              #ifndef DaStGenPackedPadding
                                #define DaStGenPackedPadding 1      // 32 bit version
                                // #define DaStGenPackedPadding 2   // 64 bit version
                              #endif
                              
                              
                              #ifdef PackedRecords
                                 #pragma pack (push, DaStGenPackedPadding)
                              #endif
                              
                              /**
                               * @author This class is generated by DaStGen
                               * 		   DataStructureGenerator (DaStGen)
                               * 		   2007-2009 Wolfgang Eckhardt
                               * 		   2012      Tobias Weinzierl
                               *
                               * 		   build date: 09-02-2014 14:40
                               *
                               * @date   07/10/2016 15:03
                               */
                              class exahype::records::ADERDGCellDescriptionPacked { 
                                 
                                 public:
                                    
                                    typedef exahype::records::ADERDGCellDescription::Type Type;
                                    
                                    typedef exahype::records::ADERDGCellDescription::RefinementEvent RefinementEvent;
                                    
                                    typedef exahype::records::ADERDGCellDescription::LimiterStatus LimiterStatus;
                                    
                                    struct PersistentRecords {
                                       int _solverNumber;
                                       std::bitset<DIMENSIONS_TIMES_TWO> _riemannSolvePerformed;
                                       std::bitset<DIMENSIONS_TIMES_TWO> _isInside;
                                       int _parentIndex;
                                       Type _type;
                                       RefinementEvent _refinementEvent;
                                       int _level;
                                       tarch::la::Vector<DIMENSIONS,double> _offset;
                                       tarch::la::Vector<DIMENSIONS,double> _size;
                                       double _correctorTimeStepSize;
                                       double _correctorTimeStamp;
                                       double _predictorTimeStepSize;
                                       double _predictorTimeStamp;
                                       double _nextPredictorTimeStepSize;
                                       int _solution;
                                       int _solutionAverages;
                                       int _update;
                                       int _updateAverages;
                                       int _extrapolatedPredictor;
                                       int _extrapolatedPredictorAverages;
                                       int _fluctuation;
                                       int _fluctuationAverages;
                                       int _solutionMin;
                                       int _solutionMax;
                                       LimiterStatus _limiterStatus;
                                       /**
                                        * Generated
                                        */
                                       PersistentRecords();
                                       
                                       /**
                                        * Generated
                                        */
                                       PersistentRecords(const int& solverNumber, const std::bitset<DIMENSIONS_TIMES_TWO>& riemannSolvePerformed, const std::bitset<DIMENSIONS_TIMES_TWO>& isInside, const int& parentIndex, const Type& type, const RefinementEvent& refinementEvent, const int& level, const tarch::la::Vector<DIMENSIONS,double>& offset, const tarch::la::Vector<DIMENSIONS,double>& size, const double& correctorTimeStepSize, const double& correctorTimeStamp, const double& predictorTimeStepSize, const double& predictorTimeStamp, const double& nextPredictorTimeStepSize, const int& solution, const int& solutionAverages, const int& update, const int& updateAverages, const int& extrapolatedPredictor, const int& extrapolatedPredictorAverages, const int& fluctuation, const int& fluctuationAverages, const int& solutionMin, const int& solutionMax, const LimiterStatus& limiterStatus);
                                       
                                       
                                       inline int getSolverNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          return _solverNumber;
                                       }
                                       
                                       
                                       
                                       inline void setSolverNumber(const int& solverNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          _solverNumber = solverNumber;
                                       }
                                       
                                       
                                       
                                       /**
                                        * Generated and optimized
                                        * 
                                        * If you realise a for loop using exclusively arrays (vectors) and compile 
                                        * with -DUseManualAlignment you may add 
                                        * \code
                                        #pragma vector aligned
                                        #pragma simd
                                        \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                        * 
                                        * The alignment is tied to the unpacked records, i.e. for packed class
                                        * variants the machine's natural alignment is switched off to recude the  
                                        * memory footprint. Do not use any SSE/AVX operations or 
                                        * vectorisation on the result for the packed variants, as the data is misaligned. 
                                        * If you rely on vectorisation, convert the underlying record 
                                        * into the unpacked version first. 
                                        * 
                                        * @see convert()
                                        */
                                       inline std::bitset<DIMENSIONS_TIMES_TWO> getRiemannSolvePerformed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          return _riemannSolvePerformed;
                                       }
                                       
                                       
                                       
                                       /**
                                        * Generated and optimized
                                        * 
                                        * If you realise a for loop using exclusively arrays (vectors) and compile 
                                        * with -DUseManualAlignment you may add 
                                        * \code
                                        #pragma vector aligned
                                        #pragma simd
                                        \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                        * 
                                        * The alignment is tied to the unpacked records, i.e. for packed class
                                        * variants the machine's natural alignment is switched off to recude the  
                                        * memory footprint. Do not use any SSE/AVX operations or 
                                        * vectorisation on the result for the packed variants, as the data is misaligned. 
                                        * If you rely on vectorisation, convert the underlying record 
                                        * into the unpacked version first. 
                                        * 
                                        * @see convert()
                                        */
                                       inline void setRiemannSolvePerformed(const std::bitset<DIMENSIONS_TIMES_TWO>& riemannSolvePerformed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          _riemannSolvePerformed = (riemannSolvePerformed);
                                       }
                                       
                                       
                                       
                                       /**
                                        * Generated and optimized
                                        * 
                                        * If you realise a for loop using exclusively arrays (vectors) and compile 
                                        * with -DUseManualAlignment you may add 
                                        * \code
                                        #pragma vector aligned
                                        #pragma simd
                                        \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                        * 
                                        * The alignment is tied to the unpacked records, i.e. for packed class
                                        * variants the machine's natural alignment is switched off to recude the  
                                        * memory footprint. Do not use any SSE/AVX operations or 
                                        * vectorisation on the result for the packed variants, as the data is misaligned. 
                                        * If you rely on vectorisation, convert the underlying record 
                                        * into the unpacked version first. 
                                        * 
                                        * @see convert()
                                        */
                                       inline std::bitset<DIMENSIONS_TIMES_TWO> getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          return _isInside;
                                       }
                                       
                                       
                                       
                                       /**
                                        * Generated and optimized
                                        * 
                                        * If you realise a for loop using exclusively arrays (vectors) and compile 
                                        * with -DUseManualAlignment you may add 
                                        * \code
                                        #pragma vector aligned
                                        #pragma simd
                                        \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                        * 
                                        * The alignment is tied to the unpacked records, i.e. for packed class
                                        * variants the machine's natural alignment is switched off to recude the  
                                        * memory footprint. Do not use any SSE/AVX operations or 
                                        * vectorisation on the result for the packed variants, as the data is misaligned. 
                                        * If you rely on vectorisation, convert the underlying record 
                                        * into the unpacked version first. 
                                        * 
                                        * @see convert()
                                        */
                                       inline void setIsInside(const std::bitset<DIMENSIONS_TIMES_TWO>& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          _isInside = (isInside);
                                       }
                                       
                                       
                                       
                                       inline int getParentIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          return _parentIndex;
                                       }
                                       
                                       
                                       
                                       inline void setParentIndex(const int& parentIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          _parentIndex = parentIndex;
                                       }
                                       
                                       
                                       
                                       inline Type getType() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          return _type;
                                       }
                                       
                                       
                                       
                                       inline void setType(const Type& type) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          _type = type;
                                       }
                                       
                                       
                                       
                                       inline RefinementEvent getRefinementEvent() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          return _refinementEvent;
                                       }
                                       
                                       
                                       
                                       inline void setRefinementEvent(const RefinementEvent& refinementEvent) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          _refinementEvent = refinementEvent;
                                       }
                                       
                                       
                                       
                                       inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          return _level;
                                       }
                                       
                                       
                                       
                                       inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          _level = level;
                                       }
                                       
                                       
                                       
                                       /**
                                        * Generated and optimized
                                        * 
                                        * If you realise a for loop using exclusively arrays (vectors) and compile 
                                        * with -DUseManualAlignment you may add 
                                        * \code
                                        #pragma vector aligned
                                        #pragma simd
                                        \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                        * 
                                        * The alignment is tied to the unpacked records, i.e. for packed class
                                        * variants the machine's natural alignment is switched off to recude the  
                                        * memory footprint. Do not use any SSE/AVX operations or 
                                        * vectorisation on the result for the packed variants, as the data is misaligned. 
                                        * If you rely on vectorisation, convert the underlying record 
                                        * into the unpacked version first. 
                                        * 
                                        * @see convert()
                                        */
                                       inline tarch::la::Vector<DIMENSIONS,double> getOffset() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          return _offset;
                                       }
                                       
                                       
                                       
                                       /**
                                        * Generated and optimized
                                        * 
                                        * If you realise a for loop using exclusively arrays (vectors) and compile 
                                        * with -DUseManualAlignment you may add 
                                        * \code
                                        #pragma vector aligned
                                        #pragma simd
                                        \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                        * 
                                        * The alignment is tied to the unpacked records, i.e. for packed class
                                        * variants the machine's natural alignment is switched off to recude the  
                                        * memory footprint. Do not use any SSE/AVX operations or 
                                        * vectorisation on the result for the packed variants, as the data is misaligned. 
                                        * If you rely on vectorisation, convert the underlying record 
                                        * into the unpacked version first. 
                                        * 
                                        * @see convert()
                                        */
                                       inline void setOffset(const tarch::la::Vector<DIMENSIONS,double>& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          _offset = (offset);
                                       }
                                       
                                       
                                       
                                       /**
                                        * Generated and optimized
                                        * 
                                        * If you realise a for loop using exclusively arrays (vectors) and compile 
                                        * with -DUseManualAlignment you may add 
                                        * \code
                                        #pragma vector aligned
                                        #pragma simd
                                        \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                        * 
                                        * The alignment is tied to the unpacked records, i.e. for packed class
                                        * variants the machine's natural alignment is switched off to recude the  
                                        * memory footprint. Do not use any SSE/AVX operations or 
                                        * vectorisation on the result for the packed variants, as the data is misaligned. 
                                        * If you rely on vectorisation, convert the underlying record 
                                        * into the unpacked version first. 
                                        * 
                                        * @see convert()
                                        */
                                       inline tarch::la::Vector<DIMENSIONS,double> getSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          return _size;
                                       }
                                       
                                       
                                       
                                       /**
                                        * Generated and optimized
                                        * 
                                        * If you realise a for loop using exclusively arrays (vectors) and compile 
                                        * with -DUseManualAlignment you may add 
                                        * \code
                                        #pragma vector aligned
                                        #pragma simd
                                        \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                        * 
                                        * The alignment is tied to the unpacked records, i.e. for packed class
                                        * variants the machine's natural alignment is switched off to recude the  
                                        * memory footprint. Do not use any SSE/AVX operations or 
                                        * vectorisation on the result for the packed variants, as the data is misaligned. 
                                        * If you rely on vectorisation, convert the underlying record 
                                        * into the unpacked version first. 
                                        * 
                                        * @see convert()
                                        */
                                       inline void setSize(const tarch::la::Vector<DIMENSIONS,double>& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          _size = (size);
                                       }
                                       
                                       
                                       
                                       inline double getCorrectorTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          return _correctorTimeStepSize;
                                       }
                                       
                                       
                                       
                                       inline void setCorrectorTimeStepSize(const double& correctorTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          _correctorTimeStepSize = correctorTimeStepSize;
                                       }
                                       
                                       
                                       
                                       inline double getCorrectorTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          return _correctorTimeStamp;
                                       }
                                       
                                       
                                       
                                       inline void setCorrectorTimeStamp(const double& correctorTimeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          _correctorTimeStamp = correctorTimeStamp;
                                       }
                                       
                                       
                                       
                                       inline double getPredictorTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          return _predictorTimeStepSize;
                                       }
                                       
                                       
                                       
                                       inline void setPredictorTimeStepSize(const double& predictorTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          _predictorTimeStepSize = predictorTimeStepSize;
                                       }
                                       
                                       
                                       
                                       inline double getPredictorTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          return _predictorTimeStamp;
                                       }
                                       
                                       
                                       
                                       inline void setPredictorTimeStamp(const double& predictorTimeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          _predictorTimeStamp = predictorTimeStamp;
                                       }
                                       
                                       
                                       
                                       inline double getNextPredictorTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          return _nextPredictorTimeStepSize;
                                       }
                                       
                                       
                                       
                                       inline void setNextPredictorTimeStepSize(const double& nextPredictorTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          _nextPredictorTimeStepSize = nextPredictorTimeStepSize;
                                       }
                                       
                                       
                                       
                                       inline int getSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          return _solution;
                                       }
                                       
                                       
                                       
                                       inline void setSolution(const int& solution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          _solution = solution;
                                       }
                                       
                                       
                                       
                                       inline int getSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          return _solutionAverages;
                                       }
                                       
                                       
                                       
                                       inline void setSolutionAverages(const int& solutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          _solutionAverages = solutionAverages;
                                       }
                                       
                                       
                                       
                                       inline int getUpdate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          return _update;
                                       }
                                       
                                       
                                       
                                       inline void setUpdate(const int& update) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          _update = update;
                                       }
                                       
                                       
                                       
                                       inline int getUpdateAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          return _updateAverages;
                                       }
                                       
                                       
                                       
                                       inline void setUpdateAverages(const int& updateAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          _updateAverages = updateAverages;
                                       }
                                       
                                       
                                       
                                       inline int getExtrapolatedPredictor() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          return _extrapolatedPredictor;
                                       }
                                       
                                       
                                       
                                       inline void setExtrapolatedPredictor(const int& extrapolatedPredictor) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          _extrapolatedPredictor = extrapolatedPredictor;
                                       }
                                       
                                       
                                       
                                       inline int getExtrapolatedPredictorAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          return _extrapolatedPredictorAverages;
                                       }
                                       
                                       
                                       
                                       inline void setExtrapolatedPredictorAverages(const int& extrapolatedPredictorAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          _extrapolatedPredictorAverages = extrapolatedPredictorAverages;
                                       }
                                       
                                       
                                       
                                       inline int getFluctuation() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          return _fluctuation;
                                       }
                                       
                                       
                                       
                                       inline void setFluctuation(const int& fluctuation) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          _fluctuation = fluctuation;
                                       }
                                       
                                       
                                       
                                       inline int getFluctuationAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          return _fluctuationAverages;
                                       }
                                       
                                       
                                       
                                       inline void setFluctuationAverages(const int& fluctuationAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          _fluctuationAverages = fluctuationAverages;
                                       }
                                       
                                       
                                       
                                       inline int getSolutionMin() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          return _solutionMin;
                                       }
                                       
                                       
                                       
                                       inline void setSolutionMin(const int& solutionMin) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          _solutionMin = solutionMin;
                                       }
                                       
                                       
                                       
                                       inline int getSolutionMax() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          return _solutionMax;
                                       }
                                       
                                       
                                       
                                       inline void setSolutionMax(const int& solutionMax) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          _solutionMax = solutionMax;
                                       }
                                       
                                       
                                       
                                       inline LimiterStatus getLimiterStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          return _limiterStatus;
                                       }
                                       
                                       
                                       
                                       inline void setLimiterStatus(const LimiterStatus& limiterStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          _limiterStatus = limiterStatus;
                                       }
                                       
                                       
                                       
                                    };
                                    
                                 private: 
                                    PersistentRecords _persistentRecords;
                                    bool _skipSolutionUpdate;
                                    
                                 public:
                                    /**
                                     * Generated
                                     */
                                    ADERDGCellDescriptionPacked();
                                    
                                    /**
                                     * Generated
                                     */
                                    ADERDGCellDescriptionPacked(const PersistentRecords& persistentRecords);
                                    
                                    /**
                                     * Generated
                                     */
                                    ADERDGCellDescriptionPacked(const int& solverNumber, const std::bitset<DIMENSIONS_TIMES_TWO>& riemannSolvePerformed, const std::bitset<DIMENSIONS_TIMES_TWO>& isInside, const int& parentIndex, const Type& type, const RefinementEvent& refinementEvent, const int& level, const tarch::la::Vector<DIMENSIONS,double>& offset, const tarch::la::Vector<DIMENSIONS,double>& size, const double& correctorTimeStepSize, const double& correctorTimeStamp, const double& predictorTimeStepSize, const double& predictorTimeStamp, const double& nextPredictorTimeStepSize, const int& solution, const int& solutionAverages, const int& update, const int& updateAverages, const int& extrapolatedPredictor, const int& extrapolatedPredictorAverages, const int& fluctuation, const int& fluctuationAverages, const int& solutionMin, const int& solutionMax, const LimiterStatus& limiterStatus);
                                    
                                    /**
                                     * Generated
                                     */
                                    ADERDGCellDescriptionPacked(const int& solverNumber, const std::bitset<DIMENSIONS_TIMES_TWO>& riemannSolvePerformed, const std::bitset<DIMENSIONS_TIMES_TWO>& isInside, const int& parentIndex, const Type& type, const RefinementEvent& refinementEvent, const int& level, const tarch::la::Vector<DIMENSIONS,double>& offset, const tarch::la::Vector<DIMENSIONS,double>& size, const double& correctorTimeStepSize, const double& correctorTimeStamp, const double& predictorTimeStepSize, const double& predictorTimeStamp, const double& nextPredictorTimeStepSize, const int& solution, const int& solutionAverages, const int& update, const int& updateAverages, const int& extrapolatedPredictor, const int& extrapolatedPredictorAverages, const int& fluctuation, const int& fluctuationAverages, const int& solutionMin, const int& solutionMax, const bool& skipSolutionUpdate, const LimiterStatus& limiterStatus);
                                    
                                    /**
                                     * Generated
                                     */
                                    ~ADERDGCellDescriptionPacked();
                                    
                                    
                                    inline int getSolverNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _persistentRecords._solverNumber;
                                    }
                                    
                                    
                                    
                                    inline void setSolverNumber(const int& solverNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _persistentRecords._solverNumber = solverNumber;
                                    }
                                    
                                    
                                    
                                    /**
                                     * Generated and optimized
                                     * 
                                     * If you realise a for loop using exclusively arrays (vectors) and compile 
                                     * with -DUseManualAlignment you may add 
                                     * \code
                                     #pragma vector aligned
                                     #pragma simd
                                     \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                     * 
                                     * The alignment is tied to the unpacked records, i.e. for packed class
                                     * variants the machine's natural alignment is switched off to recude the  
                                     * memory footprint. Do not use any SSE/AVX operations or 
                                     * vectorisation on the result for the packed variants, as the data is misaligned. 
                                     * If you rely on vectorisation, convert the underlying record 
                                     * into the unpacked version first. 
                                     * 
                                     * @see convert()
                                     */
                                    inline std::bitset<DIMENSIONS_TIMES_TWO> getRiemannSolvePerformed() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _persistentRecords._riemannSolvePerformed;
                                    }
                                    
                                    
                                    
                                    /**
                                     * Generated and optimized
                                     * 
                                     * If you realise a for loop using exclusively arrays (vectors) and compile 
                                     * with -DUseManualAlignment you may add 
                                     * \code
                                     #pragma vector aligned
                                     #pragma simd
                                     \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                     * 
                                     * The alignment is tied to the unpacked records, i.e. for packed class
                                     * variants the machine's natural alignment is switched off to recude the  
                                     * memory footprint. Do not use any SSE/AVX operations or 
                                     * vectorisation on the result for the packed variants, as the data is misaligned. 
                                     * If you rely on vectorisation, convert the underlying record 
                                     * into the unpacked version first. 
                                     * 
                                     * @see convert()
                                     */
                                    inline void setRiemannSolvePerformed(const std::bitset<DIMENSIONS_TIMES_TWO>& riemannSolvePerformed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _persistentRecords._riemannSolvePerformed = (riemannSolvePerformed);
                                    }
                                    
                                    
                                    
                                    inline bool getRiemannSolvePerformed(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       assertion(elementIndex>=0);
                                       assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                                       return _persistentRecords._riemannSolvePerformed[elementIndex];
                                       
                                    }
                                    
                                    
                                    
                                    inline void setRiemannSolvePerformed(int elementIndex, const bool& riemannSolvePerformed) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       assertion(elementIndex>=0);
                                       assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                                       _persistentRecords._riemannSolvePerformed[elementIndex]= riemannSolvePerformed;
                                       
                                    }
                                    
                                    
                                    
                                    inline void flipRiemannSolvePerformed(int elementIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       assertion(elementIndex>=0);
                                       assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                                       _persistentRecords._riemannSolvePerformed.flip(elementIndex);
                                    }
                                    
                                    
                                    
                                    /**
                                     * Generated and optimized
                                     * 
                                     * If you realise a for loop using exclusively arrays (vectors) and compile 
                                     * with -DUseManualAlignment you may add 
                                     * \code
                                     #pragma vector aligned
                                     #pragma simd
                                     \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                     * 
                                     * The alignment is tied to the unpacked records, i.e. for packed class
                                     * variants the machine's natural alignment is switched off to recude the  
                                     * memory footprint. Do not use any SSE/AVX operations or 
                                     * vectorisation on the result for the packed variants, as the data is misaligned. 
                                     * If you rely on vectorisation, convert the underlying record 
                                     * into the unpacked version first. 
                                     * 
                                     * @see convert()
                                     */
                                    inline std::bitset<DIMENSIONS_TIMES_TWO> getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _persistentRecords._isInside;
                                    }
                                    
                                    
                                    
                                    /**
                                     * Generated and optimized
                                     * 
                                     * If you realise a for loop using exclusively arrays (vectors) and compile 
                                     * with -DUseManualAlignment you may add 
                                     * \code
                                     #pragma vector aligned
                                     #pragma simd
                                     \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                     * 
                                     * The alignment is tied to the unpacked records, i.e. for packed class
                                     * variants the machine's natural alignment is switched off to recude the  
                                     * memory footprint. Do not use any SSE/AVX operations or 
                                     * vectorisation on the result for the packed variants, as the data is misaligned. 
                                     * If you rely on vectorisation, convert the underlying record 
                                     * into the unpacked version first. 
                                     * 
                                     * @see convert()
                                     */
                                    inline void setIsInside(const std::bitset<DIMENSIONS_TIMES_TWO>& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _persistentRecords._isInside = (isInside);
                                    }
                                    
                                    
                                    
                                    inline bool getIsInside(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       assertion(elementIndex>=0);
                                       assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                                       return _persistentRecords._isInside[elementIndex];
                                       
                                    }
                                    
                                    
                                    
                                    inline void setIsInside(int elementIndex, const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       assertion(elementIndex>=0);
                                       assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                                       _persistentRecords._isInside[elementIndex]= isInside;
                                       
                                    }
                                    
                                    
                                    
                                    inline void flipIsInside(int elementIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       assertion(elementIndex>=0);
                                       assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                                       _persistentRecords._isInside.flip(elementIndex);
                                    }
                                    
                                    
                                    
                                    inline int getParentIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _persistentRecords._parentIndex;
                                    }
                                    
                                    
                                    
                                    inline void setParentIndex(const int& parentIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _persistentRecords._parentIndex = parentIndex;
                                    }
                                    
                                    
                                    
                                    inline Type getType() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _persistentRecords._type;
                                    }
                                    
                                    
                                    
                                    inline void setType(const Type& type) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _persistentRecords._type = type;
                                    }
                                    
                                    
                                    
                                    inline RefinementEvent getRefinementEvent() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _persistentRecords._refinementEvent;
                                    }
                                    
                                    
                                    
                                    inline void setRefinementEvent(const RefinementEvent& refinementEvent) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _persistentRecords._refinementEvent = refinementEvent;
                                    }
                                    
                                    
                                    
                                    inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _persistentRecords._level;
                                    }
                                    
                                    
                                    
                                    inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _persistentRecords._level = level;
                                    }
                                    
                                    
                                    
                                    /**
                                     * Generated and optimized
                                     * 
                                     * If you realise a for loop using exclusively arrays (vectors) and compile 
                                     * with -DUseManualAlignment you may add 
                                     * \code
                                     #pragma vector aligned
                                     #pragma simd
                                     \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                     * 
                                     * The alignment is tied to the unpacked records, i.e. for packed class
                                     * variants the machine's natural alignment is switched off to recude the  
                                     * memory footprint. Do not use any SSE/AVX operations or 
                                     * vectorisation on the result for the packed variants, as the data is misaligned. 
                                     * If you rely on vectorisation, convert the underlying record 
                                     * into the unpacked version first. 
                                     * 
                                     * @see convert()
                                     */
                                    inline tarch::la::Vector<DIMENSIONS,double> getOffset() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _persistentRecords._offset;
                                    }
                                    
                                    
                                    
                                    /**
                                     * Generated and optimized
                                     * 
                                     * If you realise a for loop using exclusively arrays (vectors) and compile 
                                     * with -DUseManualAlignment you may add 
                                     * \code
                                     #pragma vector aligned
                                     #pragma simd
                                     \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                     * 
                                     * The alignment is tied to the unpacked records, i.e. for packed class
                                     * variants the machine's natural alignment is switched off to recude the  
                                     * memory footprint. Do not use any SSE/AVX operations or 
                                     * vectorisation on the result for the packed variants, as the data is misaligned. 
                                     * If you rely on vectorisation, convert the underlying record 
                                     * into the unpacked version first. 
                                     * 
                                     * @see convert()
                                     */
                                    inline void setOffset(const tarch::la::Vector<DIMENSIONS,double>& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _persistentRecords._offset = (offset);
                                    }
                                    
                                    
                                    
                                    inline double getOffset(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       assertion(elementIndex>=0);
                                       assertion(elementIndex<DIMENSIONS);
                                       return _persistentRecords._offset[elementIndex];
                                       
                                    }
                                    
                                    
                                    
                                    inline void setOffset(int elementIndex, const double& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       assertion(elementIndex>=0);
                                       assertion(elementIndex<DIMENSIONS);
                                       _persistentRecords._offset[elementIndex]= offset;
                                       
                                    }
                                    
                                    
                                    
                                    /**
                                     * Generated and optimized
                                     * 
                                     * If you realise a for loop using exclusively arrays (vectors) and compile 
                                     * with -DUseManualAlignment you may add 
                                     * \code
                                     #pragma vector aligned
                                     #pragma simd
                                     \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                     * 
                                     * The alignment is tied to the unpacked records, i.e. for packed class
                                     * variants the machine's natural alignment is switched off to recude the  
                                     * memory footprint. Do not use any SSE/AVX operations or 
                                     * vectorisation on the result for the packed variants, as the data is misaligned. 
                                     * If you rely on vectorisation, convert the underlying record 
                                     * into the unpacked version first. 
                                     * 
                                     * @see convert()
                                     */
                                    inline tarch::la::Vector<DIMENSIONS,double> getSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _persistentRecords._size;
                                    }
                                    
                                    
                                    
                                    /**
                                     * Generated and optimized
                                     * 
                                     * If you realise a for loop using exclusively arrays (vectors) and compile 
                                     * with -DUseManualAlignment you may add 
                                     * \code
                                     #pragma vector aligned
                                     #pragma simd
                                     \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                     * 
                                     * The alignment is tied to the unpacked records, i.e. for packed class
                                     * variants the machine's natural alignment is switched off to recude the  
                                     * memory footprint. Do not use any SSE/AVX operations or 
                                     * vectorisation on the result for the packed variants, as the data is misaligned. 
                                     * If you rely on vectorisation, convert the underlying record 
                                     * into the unpacked version first. 
                                     * 
                                     * @see convert()
                                     */
                                    inline void setSize(const tarch::la::Vector<DIMENSIONS,double>& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _persistentRecords._size = (size);
                                    }
                                    
                                    
                                    
                                    inline double getSize(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       assertion(elementIndex>=0);
                                       assertion(elementIndex<DIMENSIONS);
                                       return _persistentRecords._size[elementIndex];
                                       
                                    }
                                    
                                    
                                    
                                    inline void setSize(int elementIndex, const double& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       assertion(elementIndex>=0);
                                       assertion(elementIndex<DIMENSIONS);
                                       _persistentRecords._size[elementIndex]= size;
                                       
                                    }
                                    
                                    
                                    
                                    inline double getCorrectorTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _persistentRecords._correctorTimeStepSize;
                                    }
                                    
                                    
                                    
                                    inline void setCorrectorTimeStepSize(const double& correctorTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _persistentRecords._correctorTimeStepSize = correctorTimeStepSize;
                                    }
                                    
                                    
                                    
                                    inline double getCorrectorTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _persistentRecords._correctorTimeStamp;
                                    }
                                    
                                    
                                    
                                    inline void setCorrectorTimeStamp(const double& correctorTimeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _persistentRecords._correctorTimeStamp = correctorTimeStamp;
                                    }
                                    
                                    
                                    
                                    inline double getPredictorTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _persistentRecords._predictorTimeStepSize;
                                    }
                                    
                                    
                                    
                                    inline void setPredictorTimeStepSize(const double& predictorTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _persistentRecords._predictorTimeStepSize = predictorTimeStepSize;
                                    }
                                    
                                    
                                    
                                    inline double getPredictorTimeStamp() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _persistentRecords._predictorTimeStamp;
                                    }
                                    
                                    
                                    
                                    inline void setPredictorTimeStamp(const double& predictorTimeStamp) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _persistentRecords._predictorTimeStamp = predictorTimeStamp;
                                    }
                                    
                                    
                                    
                                    inline double getNextPredictorTimeStepSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _persistentRecords._nextPredictorTimeStepSize;
                                    }
                                    
                                    
                                    
                                    inline void setNextPredictorTimeStepSize(const double& nextPredictorTimeStepSize) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _persistentRecords._nextPredictorTimeStepSize = nextPredictorTimeStepSize;
                                    }
                                    
                                    
                                    
                                    inline int getSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _persistentRecords._solution;
                                    }
                                    
                                    
                                    
                                    inline void setSolution(const int& solution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _persistentRecords._solution = solution;
                                    }
                                    
                                    
                                    
                                    inline int getSolutionAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _persistentRecords._solutionAverages;
                                    }
                                    
                                    
                                    
                                    inline void setSolutionAverages(const int& solutionAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _persistentRecords._solutionAverages = solutionAverages;
                                    }
                                    
                                    
                                    
                                    inline int getUpdate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _persistentRecords._update;
                                    }
                                    
                                    
                                    
                                    inline void setUpdate(const int& update) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _persistentRecords._update = update;
                                    }
                                    
                                    
                                    
                                    inline int getUpdateAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _persistentRecords._updateAverages;
                                    }
                                    
                                    
                                    
                                    inline void setUpdateAverages(const int& updateAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _persistentRecords._updateAverages = updateAverages;
                                    }
                                    
                                    
                                    
                                    inline int getExtrapolatedPredictor() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _persistentRecords._extrapolatedPredictor;
                                    }
                                    
                                    
                                    
                                    inline void setExtrapolatedPredictor(const int& extrapolatedPredictor) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _persistentRecords._extrapolatedPredictor = extrapolatedPredictor;
                                    }
                                    
                                    
                                    
                                    inline int getExtrapolatedPredictorAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _persistentRecords._extrapolatedPredictorAverages;
                                    }
                                    
                                    
                                    
                                    inline void setExtrapolatedPredictorAverages(const int& extrapolatedPredictorAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _persistentRecords._extrapolatedPredictorAverages = extrapolatedPredictorAverages;
                                    }
                                    
                                    
                                    
                                    inline int getFluctuation() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _persistentRecords._fluctuation;
                                    }
                                    
                                    
                                    
                                    inline void setFluctuation(const int& fluctuation) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _persistentRecords._fluctuation = fluctuation;
                                    }
                                    
                                    
                                    
                                    inline int getFluctuationAverages() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _persistentRecords._fluctuationAverages;
                                    }
                                    
                                    
                                    
                                    inline void setFluctuationAverages(const int& fluctuationAverages) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _persistentRecords._fluctuationAverages = fluctuationAverages;
                                    }
                                    
                                    
                                    
                                    inline int getSolutionMin() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _persistentRecords._solutionMin;
                                    }
                                    
                                    
                                    
                                    inline void setSolutionMin(const int& solutionMin) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _persistentRecords._solutionMin = solutionMin;
                                    }
                                    
                                    
                                    
                                    inline int getSolutionMax() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _persistentRecords._solutionMax;
                                    }
                                    
                                    
                                    
                                    inline void setSolutionMax(const int& solutionMax) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _persistentRecords._solutionMax = solutionMax;
                                    }
                                    
                                    
                                    
                                    inline bool getSkipSolutionUpdate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _skipSolutionUpdate;
                                    }
                                    
                                    
                                    
                                    inline void setSkipSolutionUpdate(const bool& skipSolutionUpdate) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _skipSolutionUpdate = skipSolutionUpdate;
                                    }
                                    
                                    
                                    
                                    inline LimiterStatus getLimiterStatus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _persistentRecords._limiterStatus;
                                    }
                                    
                                    
                                    
                                    inline void setLimiterStatus(const LimiterStatus& limiterStatus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _persistentRecords._limiterStatus = limiterStatus;
                                    }
                                    
                                    
                                    /**
                                     * Generated
                                     */
                                    static std::string toString(const Type& param);
                                    
                                    /**
                                     * Generated
                                     */
                                    static std::string getTypeMapping();
                                    
                                    /**
                                     * Generated
                                     */
                                    static std::string toString(const RefinementEvent& param);
                                    
                                    /**
                                     * Generated
                                     */
                                    static std::string getRefinementEventMapping();
                                    
                                    /**
                                     * Generated
                                     */
                                    static std::string toString(const LimiterStatus& param);
                                    
                                    /**
                                     * Generated
                                     */
                                    static std::string getLimiterStatusMapping();
                                    
                                    /**
                                     * Generated
                                     */
                                    std::string toString() const;
                                    
                                    /**
                                     * Generated
                                     */
                                    void toString(std::ostream& out) const;
                                    
                                    
                                    PersistentRecords getPersistentRecords() const;
                                    /**
                                     * Generated
                                     */
                                    ADERDGCellDescription convert() const;
                                    
                                    
                                 #ifdef Parallel
                                    protected:
                                       static tarch::logging::Log _log;
                                       
                                    public:
                                       
                                       /**
                                        * Global that represents the mpi datatype.
                                        * There are two variants: Datatype identifies only those attributes marked with
                                        * parallelise. FullDatatype instead identifies the whole record with all fields.
                                        */
                                       static MPI_Datatype Datatype;
                                       static MPI_Datatype FullDatatype;
                                       
                                       /**
                                        * Initializes the data type for the mpi operations. Has to be called
                                        * before the very first send or receive operation is called.
                                        */
                                       static void initDatatype();
                                       
                                       static void shutdownDatatype();
                                       
                                       /**
                                        * @param communicateSleep -1 Data exchange through blocking mpi
                                        * @param communicateSleep  0 Data exchange through non-blocking mpi, i.e. pending messages are received via polling until MPI_Test succeeds
                                        * @param communicateSleep >0 Same as 0 but in addition, each unsuccessful MPI_Test is follows by an usleep
                                        */
                                       void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
                                       
                                       void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
                                       
                                       static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                                       
                                       #endif
                                          
                                       };
                                       
                                       #ifdef PackedRecords
                                       #pragma pack (pop)
                                       #endif
                                       
                                       
                                       
                                    
                                 #endif
                                 
                                 #endif
                                 
