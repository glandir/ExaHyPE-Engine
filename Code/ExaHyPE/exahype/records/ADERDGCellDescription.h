#ifndef _EXAHYPE_RECORDS_ADERDGCELLDESCRIPTION_H
#define _EXAHYPE_RECORDS_ADERDGCELLDESCRIPTION_H

#include "tarch/compiler/CompilerSpecificSettings.h"
#include "peano/utils/PeanoOptimisations.h"
#ifdef Parallel
	#include "tarch/parallel/Node.h"
#endif
#ifdef Parallel
	#include <mpi.h>
#endif
#include "tarch/logging/Log.h"
#include "tarch/la/Vector.h"
#include <bitset>
#include <complex>
#include <string>
#include <iostream>

namespace exahype {
   namespace records {
      class ADERDGCellDescription;
      class ADERDGCellDescriptionPacked;
   }
}

/**
 * @author This class is generated by DaStGen
 * 		   DataStructureGenerator (DaStGen)
 * 		   2007-2009 Wolfgang Eckhardt
 * 		   2012      Tobias Weinzierl
 *
 * 		   build date: 09-02-2014 14:40
 *
 * @date   14/01/2016 09:41
 */
class exahype::records::ADERDGCellDescription { 
   
   public:
      
      typedef exahype::records::ADERDGCellDescriptionPacked Packed;
      
      struct PersistentRecords {
         #ifdef UseManualAlignment
         tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int> _spaceTimePredictor __attribute__((aligned(VectorisationAlignment)));
         #else
         tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int> _spaceTimePredictor;
         #endif
         #ifdef UseManualAlignment
         tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int> _spaceTimeVolumeFlux __attribute__((aligned(VectorisationAlignment)));
         #else
         tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int> _spaceTimeVolumeFlux;
         #endif
         #ifdef UseManualAlignment
         tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int> _solution __attribute__((aligned(VectorisationAlignment)));
         #else
         tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int> _solution;
         #endif
         #ifdef UseManualAlignment
         tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int> _update __attribute__((aligned(VectorisationAlignment)));
         #else
         tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int> _update;
         #endif
         #ifdef UseManualAlignment
         tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int> _predictor __attribute__((aligned(VectorisationAlignment)));
         #else
         tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int> _predictor;
         #endif
         #ifdef UseManualAlignment
         tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int> _volumeFlux __attribute__((aligned(VectorisationAlignment)));
         #else
         tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int> _volumeFlux;
         #endif
         #ifdef UseManualAlignment
         tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int> _extrapolatedPredictor __attribute__((aligned(VectorisationAlignment)));
         #else
         tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int> _extrapolatedPredictor;
         #endif
         #ifdef UseManualAlignment
         tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int> _fluctuation __attribute__((aligned(VectorisationAlignment)));
         #else
         tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int> _fluctuation;
         #endif
         int _level;
         #ifdef UseManualAlignment
         tarch::la::Vector<DIMENSIONS,double> _offset __attribute__((aligned(VectorisationAlignment)));
         #else
         tarch::la::Vector<DIMENSIONS,double> _offset;
         #endif
         #ifdef UseManualAlignment
         tarch::la::Vector<DIMENSIONS,double> _size __attribute__((aligned(VectorisationAlignment)));
         #else
         tarch::la::Vector<DIMENSIONS,double> _size;
         #endif
         /**
          * Generated
          */
         PersistentRecords();
         
         /**
          * Generated
          */
         PersistentRecords(const tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int>& spaceTimePredictor, const tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int>& spaceTimeVolumeFlux, const tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int>& solution, const tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int>& update, const tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int>& predictor, const tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int>& volumeFlux, const tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int>& extrapolatedPredictor, const tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int>& fluctuation, const int& level, const tarch::la::Vector<DIMENSIONS,double>& offset, const tarch::la::Vector<DIMENSIONS,double>& size);
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int> getSpaceTimePredictor() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _spaceTimePredictor;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setSpaceTimePredictor(const tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int>& spaceTimePredictor) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _spaceTimePredictor = (spaceTimePredictor);
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int> getSpaceTimeVolumeFlux() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _spaceTimeVolumeFlux;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setSpaceTimeVolumeFlux(const tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int>& spaceTimeVolumeFlux) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _spaceTimeVolumeFlux = (spaceTimeVolumeFlux);
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int> getSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _solution;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setSolution(const tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int>& solution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _solution = (solution);
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int> getUpdate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _update;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setUpdate(const tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int>& update) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _update = (update);
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int> getPredictor() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _predictor;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setPredictor(const tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int>& predictor) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _predictor = (predictor);
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int> getVolumeFlux() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _volumeFlux;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setVolumeFlux(const tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int>& volumeFlux) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _volumeFlux = (volumeFlux);
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int> getExtrapolatedPredictor() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _extrapolatedPredictor;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setExtrapolatedPredictor(const tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int>& extrapolatedPredictor) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _extrapolatedPredictor = (extrapolatedPredictor);
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int> getFluctuation() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _fluctuation;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setFluctuation(const tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int>& fluctuation) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _fluctuation = (fluctuation);
         }
         
         
         
         inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _level;
         }
         
         
         
         inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _level = level;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<DIMENSIONS,double> getOffset() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _offset;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setOffset(const tarch::la::Vector<DIMENSIONS,double>& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _offset = (offset);
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<DIMENSIONS,double> getSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _size;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setSize(const tarch::la::Vector<DIMENSIONS,double>& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _size = (size);
         }
         
         
         
      };
      
   private: 
      PersistentRecords _persistentRecords;
      
   public:
      /**
       * Generated
       */
      ADERDGCellDescription();
      
      /**
       * Generated
       */
      ADERDGCellDescription(const PersistentRecords& persistentRecords);
      
      /**
       * Generated
       */
      ADERDGCellDescription(const tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int>& spaceTimePredictor, const tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int>& spaceTimeVolumeFlux, const tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int>& solution, const tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int>& update, const tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int>& predictor, const tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int>& volumeFlux, const tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int>& extrapolatedPredictor, const tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int>& fluctuation, const int& level, const tarch::la::Vector<DIMENSIONS,double>& offset, const tarch::la::Vector<DIMENSIONS,double>& size);
      
      /**
       * Generated
       */
      ~ADERDGCellDescription();
      
      
      /**
       * Generated and optimized
       * 
       * If you realise a for loop using exclusively arrays (vectors) and compile 
       * with -DUseManualAlignment you may add 
       * \code
       #pragma vector aligned
       #pragma simd
       \endcode to this for loop to enforce your compiler to use SSE/AVX.
       * 
       * The alignment is tied to the unpacked records, i.e. for packed class
       * variants the machine's natural alignment is switched off to recude the  
       * memory footprint. Do not use any SSE/AVX operations or 
       * vectorisation on the result for the packed variants, as the data is misaligned. 
       * If you rely on vectorisation, convert the underlying record 
       * into the unpacked version first. 
       * 
       * @see convert()
       */
      inline tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int> getSpaceTimePredictor() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         return _persistentRecords._spaceTimePredictor;
      }
      
      
      
      /**
       * Generated and optimized
       * 
       * If you realise a for loop using exclusively arrays (vectors) and compile 
       * with -DUseManualAlignment you may add 
       * \code
       #pragma vector aligned
       #pragma simd
       \endcode to this for loop to enforce your compiler to use SSE/AVX.
       * 
       * The alignment is tied to the unpacked records, i.e. for packed class
       * variants the machine's natural alignment is switched off to recude the  
       * memory footprint. Do not use any SSE/AVX operations or 
       * vectorisation on the result for the packed variants, as the data is misaligned. 
       * If you rely on vectorisation, convert the underlying record 
       * into the unpacked version first. 
       * 
       * @see convert()
       */
      inline void setSpaceTimePredictor(const tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int>& spaceTimePredictor) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         _persistentRecords._spaceTimePredictor = (spaceTimePredictor);
      }
      
      
      
      inline int getSpaceTimePredictor(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         assertion(elementIndex>=0);
         assertion(elementIndex<EXAHYPE_PATCH_SIZE_TOTAL);
         return _persistentRecords._spaceTimePredictor[elementIndex];
         
      }
      
      
      
      inline void setSpaceTimePredictor(int elementIndex, const int& spaceTimePredictor) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         assertion(elementIndex>=0);
         assertion(elementIndex<EXAHYPE_PATCH_SIZE_TOTAL);
         _persistentRecords._spaceTimePredictor[elementIndex]= spaceTimePredictor;
         
      }
      
      
      
      /**
       * Generated and optimized
       * 
       * If you realise a for loop using exclusively arrays (vectors) and compile 
       * with -DUseManualAlignment you may add 
       * \code
       #pragma vector aligned
       #pragma simd
       \endcode to this for loop to enforce your compiler to use SSE/AVX.
       * 
       * The alignment is tied to the unpacked records, i.e. for packed class
       * variants the machine's natural alignment is switched off to recude the  
       * memory footprint. Do not use any SSE/AVX operations or 
       * vectorisation on the result for the packed variants, as the data is misaligned. 
       * If you rely on vectorisation, convert the underlying record 
       * into the unpacked version first. 
       * 
       * @see convert()
       */
      inline tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int> getSpaceTimeVolumeFlux() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         return _persistentRecords._spaceTimeVolumeFlux;
      }
      
      
      
      /**
       * Generated and optimized
       * 
       * If you realise a for loop using exclusively arrays (vectors) and compile 
       * with -DUseManualAlignment you may add 
       * \code
       #pragma vector aligned
       #pragma simd
       \endcode to this for loop to enforce your compiler to use SSE/AVX.
       * 
       * The alignment is tied to the unpacked records, i.e. for packed class
       * variants the machine's natural alignment is switched off to recude the  
       * memory footprint. Do not use any SSE/AVX operations or 
       * vectorisation on the result for the packed variants, as the data is misaligned. 
       * If you rely on vectorisation, convert the underlying record 
       * into the unpacked version first. 
       * 
       * @see convert()
       */
      inline void setSpaceTimeVolumeFlux(const tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int>& spaceTimeVolumeFlux) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         _persistentRecords._spaceTimeVolumeFlux = (spaceTimeVolumeFlux);
      }
      
      
      
      inline int getSpaceTimeVolumeFlux(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         assertion(elementIndex>=0);
         assertion(elementIndex<EXAHYPE_PATCH_SIZE_TOTAL);
         return _persistentRecords._spaceTimeVolumeFlux[elementIndex];
         
      }
      
      
      
      inline void setSpaceTimeVolumeFlux(int elementIndex, const int& spaceTimeVolumeFlux) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         assertion(elementIndex>=0);
         assertion(elementIndex<EXAHYPE_PATCH_SIZE_TOTAL);
         _persistentRecords._spaceTimeVolumeFlux[elementIndex]= spaceTimeVolumeFlux;
         
      }
      
      
      
      /**
       * Generated and optimized
       * 
       * If you realise a for loop using exclusively arrays (vectors) and compile 
       * with -DUseManualAlignment you may add 
       * \code
       #pragma vector aligned
       #pragma simd
       \endcode to this for loop to enforce your compiler to use SSE/AVX.
       * 
       * The alignment is tied to the unpacked records, i.e. for packed class
       * variants the machine's natural alignment is switched off to recude the  
       * memory footprint. Do not use any SSE/AVX operations or 
       * vectorisation on the result for the packed variants, as the data is misaligned. 
       * If you rely on vectorisation, convert the underlying record 
       * into the unpacked version first. 
       * 
       * @see convert()
       */
      inline tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int> getSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         return _persistentRecords._solution;
      }
      
      
      
      /**
       * Generated and optimized
       * 
       * If you realise a for loop using exclusively arrays (vectors) and compile 
       * with -DUseManualAlignment you may add 
       * \code
       #pragma vector aligned
       #pragma simd
       \endcode to this for loop to enforce your compiler to use SSE/AVX.
       * 
       * The alignment is tied to the unpacked records, i.e. for packed class
       * variants the machine's natural alignment is switched off to recude the  
       * memory footprint. Do not use any SSE/AVX operations or 
       * vectorisation on the result for the packed variants, as the data is misaligned. 
       * If you rely on vectorisation, convert the underlying record 
       * into the unpacked version first. 
       * 
       * @see convert()
       */
      inline void setSolution(const tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int>& solution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         _persistentRecords._solution = (solution);
      }
      
      
      
      inline int getSolution(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         assertion(elementIndex>=0);
         assertion(elementIndex<EXAHYPE_PATCH_SIZE_TOTAL);
         return _persistentRecords._solution[elementIndex];
         
      }
      
      
      
      inline void setSolution(int elementIndex, const int& solution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         assertion(elementIndex>=0);
         assertion(elementIndex<EXAHYPE_PATCH_SIZE_TOTAL);
         _persistentRecords._solution[elementIndex]= solution;
         
      }
      
      
      
      /**
       * Generated and optimized
       * 
       * If you realise a for loop using exclusively arrays (vectors) and compile 
       * with -DUseManualAlignment you may add 
       * \code
       #pragma vector aligned
       #pragma simd
       \endcode to this for loop to enforce your compiler to use SSE/AVX.
       * 
       * The alignment is tied to the unpacked records, i.e. for packed class
       * variants the machine's natural alignment is switched off to recude the  
       * memory footprint. Do not use any SSE/AVX operations or 
       * vectorisation on the result for the packed variants, as the data is misaligned. 
       * If you rely on vectorisation, convert the underlying record 
       * into the unpacked version first. 
       * 
       * @see convert()
       */
      inline tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int> getUpdate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         return _persistentRecords._update;
      }
      
      
      
      /**
       * Generated and optimized
       * 
       * If you realise a for loop using exclusively arrays (vectors) and compile 
       * with -DUseManualAlignment you may add 
       * \code
       #pragma vector aligned
       #pragma simd
       \endcode to this for loop to enforce your compiler to use SSE/AVX.
       * 
       * The alignment is tied to the unpacked records, i.e. for packed class
       * variants the machine's natural alignment is switched off to recude the  
       * memory footprint. Do not use any SSE/AVX operations or 
       * vectorisation on the result for the packed variants, as the data is misaligned. 
       * If you rely on vectorisation, convert the underlying record 
       * into the unpacked version first. 
       * 
       * @see convert()
       */
      inline void setUpdate(const tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int>& update) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         _persistentRecords._update = (update);
      }
      
      
      
      inline int getUpdate(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         assertion(elementIndex>=0);
         assertion(elementIndex<EXAHYPE_PATCH_SIZE_TOTAL);
         return _persistentRecords._update[elementIndex];
         
      }
      
      
      
      inline void setUpdate(int elementIndex, const int& update) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         assertion(elementIndex>=0);
         assertion(elementIndex<EXAHYPE_PATCH_SIZE_TOTAL);
         _persistentRecords._update[elementIndex]= update;
         
      }
      
      
      
      /**
       * Generated and optimized
       * 
       * If you realise a for loop using exclusively arrays (vectors) and compile 
       * with -DUseManualAlignment you may add 
       * \code
       #pragma vector aligned
       #pragma simd
       \endcode to this for loop to enforce your compiler to use SSE/AVX.
       * 
       * The alignment is tied to the unpacked records, i.e. for packed class
       * variants the machine's natural alignment is switched off to recude the  
       * memory footprint. Do not use any SSE/AVX operations or 
       * vectorisation on the result for the packed variants, as the data is misaligned. 
       * If you rely on vectorisation, convert the underlying record 
       * into the unpacked version first. 
       * 
       * @see convert()
       */
      inline tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int> getPredictor() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         return _persistentRecords._predictor;
      }
      
      
      
      /**
       * Generated and optimized
       * 
       * If you realise a for loop using exclusively arrays (vectors) and compile 
       * with -DUseManualAlignment you may add 
       * \code
       #pragma vector aligned
       #pragma simd
       \endcode to this for loop to enforce your compiler to use SSE/AVX.
       * 
       * The alignment is tied to the unpacked records, i.e. for packed class
       * variants the machine's natural alignment is switched off to recude the  
       * memory footprint. Do not use any SSE/AVX operations or 
       * vectorisation on the result for the packed variants, as the data is misaligned. 
       * If you rely on vectorisation, convert the underlying record 
       * into the unpacked version first. 
       * 
       * @see convert()
       */
      inline void setPredictor(const tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int>& predictor) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         _persistentRecords._predictor = (predictor);
      }
      
      
      
      inline int getPredictor(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         assertion(elementIndex>=0);
         assertion(elementIndex<EXAHYPE_PATCH_SIZE_TOTAL);
         return _persistentRecords._predictor[elementIndex];
         
      }
      
      
      
      inline void setPredictor(int elementIndex, const int& predictor) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         assertion(elementIndex>=0);
         assertion(elementIndex<EXAHYPE_PATCH_SIZE_TOTAL);
         _persistentRecords._predictor[elementIndex]= predictor;
         
      }
      
      
      
      /**
       * Generated and optimized
       * 
       * If you realise a for loop using exclusively arrays (vectors) and compile 
       * with -DUseManualAlignment you may add 
       * \code
       #pragma vector aligned
       #pragma simd
       \endcode to this for loop to enforce your compiler to use SSE/AVX.
       * 
       * The alignment is tied to the unpacked records, i.e. for packed class
       * variants the machine's natural alignment is switched off to recude the  
       * memory footprint. Do not use any SSE/AVX operations or 
       * vectorisation on the result for the packed variants, as the data is misaligned. 
       * If you rely on vectorisation, convert the underlying record 
       * into the unpacked version first. 
       * 
       * @see convert()
       */
      inline tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int> getVolumeFlux() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         return _persistentRecords._volumeFlux;
      }
      
      
      
      /**
       * Generated and optimized
       * 
       * If you realise a for loop using exclusively arrays (vectors) and compile 
       * with -DUseManualAlignment you may add 
       * \code
       #pragma vector aligned
       #pragma simd
       \endcode to this for loop to enforce your compiler to use SSE/AVX.
       * 
       * The alignment is tied to the unpacked records, i.e. for packed class
       * variants the machine's natural alignment is switched off to recude the  
       * memory footprint. Do not use any SSE/AVX operations or 
       * vectorisation on the result for the packed variants, as the data is misaligned. 
       * If you rely on vectorisation, convert the underlying record 
       * into the unpacked version first. 
       * 
       * @see convert()
       */
      inline void setVolumeFlux(const tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int>& volumeFlux) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         _persistentRecords._volumeFlux = (volumeFlux);
      }
      
      
      
      inline int getVolumeFlux(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         assertion(elementIndex>=0);
         assertion(elementIndex<EXAHYPE_PATCH_SIZE_TOTAL);
         return _persistentRecords._volumeFlux[elementIndex];
         
      }
      
      
      
      inline void setVolumeFlux(int elementIndex, const int& volumeFlux) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         assertion(elementIndex>=0);
         assertion(elementIndex<EXAHYPE_PATCH_SIZE_TOTAL);
         _persistentRecords._volumeFlux[elementIndex]= volumeFlux;
         
      }
      
      
      
      /**
       * Generated and optimized
       * 
       * If you realise a for loop using exclusively arrays (vectors) and compile 
       * with -DUseManualAlignment you may add 
       * \code
       #pragma vector aligned
       #pragma simd
       \endcode to this for loop to enforce your compiler to use SSE/AVX.
       * 
       * The alignment is tied to the unpacked records, i.e. for packed class
       * variants the machine's natural alignment is switched off to recude the  
       * memory footprint. Do not use any SSE/AVX operations or 
       * vectorisation on the result for the packed variants, as the data is misaligned. 
       * If you rely on vectorisation, convert the underlying record 
       * into the unpacked version first. 
       * 
       * @see convert()
       */
      inline tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int> getExtrapolatedPredictor() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         return _persistentRecords._extrapolatedPredictor;
      }
      
      
      
      /**
       * Generated and optimized
       * 
       * If you realise a for loop using exclusively arrays (vectors) and compile 
       * with -DUseManualAlignment you may add 
       * \code
       #pragma vector aligned
       #pragma simd
       \endcode to this for loop to enforce your compiler to use SSE/AVX.
       * 
       * The alignment is tied to the unpacked records, i.e. for packed class
       * variants the machine's natural alignment is switched off to recude the  
       * memory footprint. Do not use any SSE/AVX operations or 
       * vectorisation on the result for the packed variants, as the data is misaligned. 
       * If you rely on vectorisation, convert the underlying record 
       * into the unpacked version first. 
       * 
       * @see convert()
       */
      inline void setExtrapolatedPredictor(const tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int>& extrapolatedPredictor) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         _persistentRecords._extrapolatedPredictor = (extrapolatedPredictor);
      }
      
      
      
      inline int getExtrapolatedPredictor(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         assertion(elementIndex>=0);
         assertion(elementIndex<EXAHYPE_PATCH_SIZE_TOTAL);
         return _persistentRecords._extrapolatedPredictor[elementIndex];
         
      }
      
      
      
      inline void setExtrapolatedPredictor(int elementIndex, const int& extrapolatedPredictor) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         assertion(elementIndex>=0);
         assertion(elementIndex<EXAHYPE_PATCH_SIZE_TOTAL);
         _persistentRecords._extrapolatedPredictor[elementIndex]= extrapolatedPredictor;
         
      }
      
      
      
      /**
       * Generated and optimized
       * 
       * If you realise a for loop using exclusively arrays (vectors) and compile 
       * with -DUseManualAlignment you may add 
       * \code
       #pragma vector aligned
       #pragma simd
       \endcode to this for loop to enforce your compiler to use SSE/AVX.
       * 
       * The alignment is tied to the unpacked records, i.e. for packed class
       * variants the machine's natural alignment is switched off to recude the  
       * memory footprint. Do not use any SSE/AVX operations or 
       * vectorisation on the result for the packed variants, as the data is misaligned. 
       * If you rely on vectorisation, convert the underlying record 
       * into the unpacked version first. 
       * 
       * @see convert()
       */
      inline tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int> getFluctuation() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         return _persistentRecords._fluctuation;
      }
      
      
      
      /**
       * Generated and optimized
       * 
       * If you realise a for loop using exclusively arrays (vectors) and compile 
       * with -DUseManualAlignment you may add 
       * \code
       #pragma vector aligned
       #pragma simd
       \endcode to this for loop to enforce your compiler to use SSE/AVX.
       * 
       * The alignment is tied to the unpacked records, i.e. for packed class
       * variants the machine's natural alignment is switched off to recude the  
       * memory footprint. Do not use any SSE/AVX operations or 
       * vectorisation on the result for the packed variants, as the data is misaligned. 
       * If you rely on vectorisation, convert the underlying record 
       * into the unpacked version first. 
       * 
       * @see convert()
       */
      inline void setFluctuation(const tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int>& fluctuation) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         _persistentRecords._fluctuation = (fluctuation);
      }
      
      
      
      inline int getFluctuation(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         assertion(elementIndex>=0);
         assertion(elementIndex<EXAHYPE_PATCH_SIZE_TOTAL);
         return _persistentRecords._fluctuation[elementIndex];
         
      }
      
      
      
      inline void setFluctuation(int elementIndex, const int& fluctuation) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         assertion(elementIndex>=0);
         assertion(elementIndex<EXAHYPE_PATCH_SIZE_TOTAL);
         _persistentRecords._fluctuation[elementIndex]= fluctuation;
         
      }
      
      
      
      inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         return _persistentRecords._level;
      }
      
      
      
      inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         _persistentRecords._level = level;
      }
      
      
      
      /**
       * Generated and optimized
       * 
       * If you realise a for loop using exclusively arrays (vectors) and compile 
       * with -DUseManualAlignment you may add 
       * \code
       #pragma vector aligned
       #pragma simd
       \endcode to this for loop to enforce your compiler to use SSE/AVX.
       * 
       * The alignment is tied to the unpacked records, i.e. for packed class
       * variants the machine's natural alignment is switched off to recude the  
       * memory footprint. Do not use any SSE/AVX operations or 
       * vectorisation on the result for the packed variants, as the data is misaligned. 
       * If you rely on vectorisation, convert the underlying record 
       * into the unpacked version first. 
       * 
       * @see convert()
       */
      inline tarch::la::Vector<DIMENSIONS,double> getOffset() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         return _persistentRecords._offset;
      }
      
      
      
      /**
       * Generated and optimized
       * 
       * If you realise a for loop using exclusively arrays (vectors) and compile 
       * with -DUseManualAlignment you may add 
       * \code
       #pragma vector aligned
       #pragma simd
       \endcode to this for loop to enforce your compiler to use SSE/AVX.
       * 
       * The alignment is tied to the unpacked records, i.e. for packed class
       * variants the machine's natural alignment is switched off to recude the  
       * memory footprint. Do not use any SSE/AVX operations or 
       * vectorisation on the result for the packed variants, as the data is misaligned. 
       * If you rely on vectorisation, convert the underlying record 
       * into the unpacked version first. 
       * 
       * @see convert()
       */
      inline void setOffset(const tarch::la::Vector<DIMENSIONS,double>& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         _persistentRecords._offset = (offset);
      }
      
      
      
      inline double getOffset(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         assertion(elementIndex>=0);
         assertion(elementIndex<DIMENSIONS);
         return _persistentRecords._offset[elementIndex];
         
      }
      
      
      
      inline void setOffset(int elementIndex, const double& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         assertion(elementIndex>=0);
         assertion(elementIndex<DIMENSIONS);
         _persistentRecords._offset[elementIndex]= offset;
         
      }
      
      
      
      /**
       * Generated and optimized
       * 
       * If you realise a for loop using exclusively arrays (vectors) and compile 
       * with -DUseManualAlignment you may add 
       * \code
       #pragma vector aligned
       #pragma simd
       \endcode to this for loop to enforce your compiler to use SSE/AVX.
       * 
       * The alignment is tied to the unpacked records, i.e. for packed class
       * variants the machine's natural alignment is switched off to recude the  
       * memory footprint. Do not use any SSE/AVX operations or 
       * vectorisation on the result for the packed variants, as the data is misaligned. 
       * If you rely on vectorisation, convert the underlying record 
       * into the unpacked version first. 
       * 
       * @see convert()
       */
      inline tarch::la::Vector<DIMENSIONS,double> getSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         return _persistentRecords._size;
      }
      
      
      
      /**
       * Generated and optimized
       * 
       * If you realise a for loop using exclusively arrays (vectors) and compile 
       * with -DUseManualAlignment you may add 
       * \code
       #pragma vector aligned
       #pragma simd
       \endcode to this for loop to enforce your compiler to use SSE/AVX.
       * 
       * The alignment is tied to the unpacked records, i.e. for packed class
       * variants the machine's natural alignment is switched off to recude the  
       * memory footprint. Do not use any SSE/AVX operations or 
       * vectorisation on the result for the packed variants, as the data is misaligned. 
       * If you rely on vectorisation, convert the underlying record 
       * into the unpacked version first. 
       * 
       * @see convert()
       */
      inline void setSize(const tarch::la::Vector<DIMENSIONS,double>& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         _persistentRecords._size = (size);
      }
      
      
      
      inline double getSize(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         assertion(elementIndex>=0);
         assertion(elementIndex<DIMENSIONS);
         return _persistentRecords._size[elementIndex];
         
      }
      
      
      
      inline void setSize(int elementIndex, const double& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
         assertion(elementIndex>=0);
         assertion(elementIndex<DIMENSIONS);
         _persistentRecords._size[elementIndex]= size;
         
      }
      
      
      /**
       * Generated
       */
      std::string toString() const;
      
      /**
       * Generated
       */
      void toString(std::ostream& out) const;
      
      
      PersistentRecords getPersistentRecords() const;
      /**
       * Generated
       */
      ADERDGCellDescriptionPacked convert() const;
      
      
   #ifdef Parallel
      protected:
         static tarch::logging::Log _log;
         
      public:
         
         /**
          * Global that represents the mpi datatype.
          * There are two variants: Datatype identifies only those attributes marked with
          * parallelise. FullDatatype instead identifies the whole record with all fields.
          */
         static MPI_Datatype Datatype;
         static MPI_Datatype FullDatatype;
         
         /**
          * Initializes the data type for the mpi operations. Has to be called
          * before the very first send or receive operation is called.
          */
         static void initDatatype();
         
         static void shutdownDatatype();
         
         /**
          * @param communicateSleep -1 Data exchange through blocking mpi
          * @param communicateSleep  0 Data exchange through non-blocking mpi, i.e. pending messages are received via polling until MPI_Test succeeds
          * @param communicateSleep >0 Same as 0 but in addition, each unsuccessful MPI_Test is follows by an usleep
          */
         void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
         
         void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
         
         static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
         
         #endif
            
         };
         
         #ifndef DaStGenPackedPadding
           #define DaStGenPackedPadding 1      // 32 bit version
           // #define DaStGenPackedPadding 2   // 64 bit version
         #endif
         
         
         #ifdef PackedRecords
            #pragma pack (push, DaStGenPackedPadding)
         #endif
         
         /**
          * @author This class is generated by DaStGen
          * 		   DataStructureGenerator (DaStGen)
          * 		   2007-2009 Wolfgang Eckhardt
          * 		   2012      Tobias Weinzierl
          *
          * 		   build date: 09-02-2014 14:40
          *
          * @date   14/01/2016 09:41
          */
         class exahype::records::ADERDGCellDescriptionPacked { 
            
            public:
               
               struct PersistentRecords {
                  tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int> _spaceTimePredictor;
                  tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int> _spaceTimeVolumeFlux;
                  tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int> _solution;
                  tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int> _update;
                  tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int> _predictor;
                  tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int> _volumeFlux;
                  tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int> _extrapolatedPredictor;
                  tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int> _fluctuation;
                  int _level;
                  tarch::la::Vector<DIMENSIONS,double> _offset;
                  tarch::la::Vector<DIMENSIONS,double> _size;
                  /**
                   * Generated
                   */
                  PersistentRecords();
                  
                  /**
                   * Generated
                   */
                  PersistentRecords(const tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int>& spaceTimePredictor, const tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int>& spaceTimeVolumeFlux, const tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int>& solution, const tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int>& update, const tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int>& predictor, const tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int>& volumeFlux, const tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int>& extrapolatedPredictor, const tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int>& fluctuation, const int& level, const tarch::la::Vector<DIMENSIONS,double>& offset, const tarch::la::Vector<DIMENSIONS,double>& size);
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int> getSpaceTimePredictor() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _spaceTimePredictor;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline void setSpaceTimePredictor(const tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int>& spaceTimePredictor) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _spaceTimePredictor = (spaceTimePredictor);
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int> getSpaceTimeVolumeFlux() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _spaceTimeVolumeFlux;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline void setSpaceTimeVolumeFlux(const tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int>& spaceTimeVolumeFlux) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _spaceTimeVolumeFlux = (spaceTimeVolumeFlux);
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int> getSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _solution;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline void setSolution(const tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int>& solution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _solution = (solution);
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int> getUpdate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _update;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline void setUpdate(const tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int>& update) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _update = (update);
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int> getPredictor() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _predictor;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline void setPredictor(const tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int>& predictor) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _predictor = (predictor);
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int> getVolumeFlux() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _volumeFlux;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline void setVolumeFlux(const tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int>& volumeFlux) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _volumeFlux = (volumeFlux);
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int> getExtrapolatedPredictor() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _extrapolatedPredictor;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline void setExtrapolatedPredictor(const tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int>& extrapolatedPredictor) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _extrapolatedPredictor = (extrapolatedPredictor);
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int> getFluctuation() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _fluctuation;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline void setFluctuation(const tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int>& fluctuation) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _fluctuation = (fluctuation);
                  }
                  
                  
                  
                  inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _level;
                  }
                  
                  
                  
                  inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _level = level;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline tarch::la::Vector<DIMENSIONS,double> getOffset() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _offset;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline void setOffset(const tarch::la::Vector<DIMENSIONS,double>& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _offset = (offset);
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline tarch::la::Vector<DIMENSIONS,double> getSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _size;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline void setSize(const tarch::la::Vector<DIMENSIONS,double>& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _size = (size);
                  }
                  
                  
                  
               };
               
            private: 
               PersistentRecords _persistentRecords;
               
            public:
               /**
                * Generated
                */
               ADERDGCellDescriptionPacked();
               
               /**
                * Generated
                */
               ADERDGCellDescriptionPacked(const PersistentRecords& persistentRecords);
               
               /**
                * Generated
                */
               ADERDGCellDescriptionPacked(const tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int>& spaceTimePredictor, const tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int>& spaceTimeVolumeFlux, const tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int>& solution, const tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int>& update, const tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int>& predictor, const tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int>& volumeFlux, const tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int>& extrapolatedPredictor, const tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int>& fluctuation, const int& level, const tarch::la::Vector<DIMENSIONS,double>& offset, const tarch::la::Vector<DIMENSIONS,double>& size);
               
               /**
                * Generated
                */
               ~ADERDGCellDescriptionPacked();
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int> getSpaceTimePredictor() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._spaceTimePredictor;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setSpaceTimePredictor(const tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int>& spaceTimePredictor) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._spaceTimePredictor = (spaceTimePredictor);
               }
               
               
               
               inline int getSpaceTimePredictor(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<EXAHYPE_PATCH_SIZE_TOTAL);
                  return _persistentRecords._spaceTimePredictor[elementIndex];
                  
               }
               
               
               
               inline void setSpaceTimePredictor(int elementIndex, const int& spaceTimePredictor) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<EXAHYPE_PATCH_SIZE_TOTAL);
                  _persistentRecords._spaceTimePredictor[elementIndex]= spaceTimePredictor;
                  
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int> getSpaceTimeVolumeFlux() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._spaceTimeVolumeFlux;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setSpaceTimeVolumeFlux(const tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int>& spaceTimeVolumeFlux) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._spaceTimeVolumeFlux = (spaceTimeVolumeFlux);
               }
               
               
               
               inline int getSpaceTimeVolumeFlux(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<EXAHYPE_PATCH_SIZE_TOTAL);
                  return _persistentRecords._spaceTimeVolumeFlux[elementIndex];
                  
               }
               
               
               
               inline void setSpaceTimeVolumeFlux(int elementIndex, const int& spaceTimeVolumeFlux) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<EXAHYPE_PATCH_SIZE_TOTAL);
                  _persistentRecords._spaceTimeVolumeFlux[elementIndex]= spaceTimeVolumeFlux;
                  
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int> getSolution() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._solution;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setSolution(const tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int>& solution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._solution = (solution);
               }
               
               
               
               inline int getSolution(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<EXAHYPE_PATCH_SIZE_TOTAL);
                  return _persistentRecords._solution[elementIndex];
                  
               }
               
               
               
               inline void setSolution(int elementIndex, const int& solution) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<EXAHYPE_PATCH_SIZE_TOTAL);
                  _persistentRecords._solution[elementIndex]= solution;
                  
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int> getUpdate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._update;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setUpdate(const tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int>& update) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._update = (update);
               }
               
               
               
               inline int getUpdate(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<EXAHYPE_PATCH_SIZE_TOTAL);
                  return _persistentRecords._update[elementIndex];
                  
               }
               
               
               
               inline void setUpdate(int elementIndex, const int& update) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<EXAHYPE_PATCH_SIZE_TOTAL);
                  _persistentRecords._update[elementIndex]= update;
                  
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int> getPredictor() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._predictor;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setPredictor(const tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int>& predictor) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._predictor = (predictor);
               }
               
               
               
               inline int getPredictor(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<EXAHYPE_PATCH_SIZE_TOTAL);
                  return _persistentRecords._predictor[elementIndex];
                  
               }
               
               
               
               inline void setPredictor(int elementIndex, const int& predictor) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<EXAHYPE_PATCH_SIZE_TOTAL);
                  _persistentRecords._predictor[elementIndex]= predictor;
                  
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int> getVolumeFlux() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._volumeFlux;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setVolumeFlux(const tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int>& volumeFlux) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._volumeFlux = (volumeFlux);
               }
               
               
               
               inline int getVolumeFlux(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<EXAHYPE_PATCH_SIZE_TOTAL);
                  return _persistentRecords._volumeFlux[elementIndex];
                  
               }
               
               
               
               inline void setVolumeFlux(int elementIndex, const int& volumeFlux) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<EXAHYPE_PATCH_SIZE_TOTAL);
                  _persistentRecords._volumeFlux[elementIndex]= volumeFlux;
                  
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int> getExtrapolatedPredictor() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._extrapolatedPredictor;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setExtrapolatedPredictor(const tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int>& extrapolatedPredictor) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._extrapolatedPredictor = (extrapolatedPredictor);
               }
               
               
               
               inline int getExtrapolatedPredictor(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<EXAHYPE_PATCH_SIZE_TOTAL);
                  return _persistentRecords._extrapolatedPredictor[elementIndex];
                  
               }
               
               
               
               inline void setExtrapolatedPredictor(int elementIndex, const int& extrapolatedPredictor) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<EXAHYPE_PATCH_SIZE_TOTAL);
                  _persistentRecords._extrapolatedPredictor[elementIndex]= extrapolatedPredictor;
                  
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int> getFluctuation() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._fluctuation;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setFluctuation(const tarch::la::Vector<EXAHYPE_PATCH_SIZE_TOTAL,int>& fluctuation) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._fluctuation = (fluctuation);
               }
               
               
               
               inline int getFluctuation(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<EXAHYPE_PATCH_SIZE_TOTAL);
                  return _persistentRecords._fluctuation[elementIndex];
                  
               }
               
               
               
               inline void setFluctuation(int elementIndex, const int& fluctuation) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<EXAHYPE_PATCH_SIZE_TOTAL);
                  _persistentRecords._fluctuation[elementIndex]= fluctuation;
                  
               }
               
               
               
               inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._level;
               }
               
               
               
               inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._level = level;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS,double> getOffset() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._offset;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setOffset(const tarch::la::Vector<DIMENSIONS,double>& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._offset = (offset);
               }
               
               
               
               inline double getOffset(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS);
                  return _persistentRecords._offset[elementIndex];
                  
               }
               
               
               
               inline void setOffset(int elementIndex, const double& offset) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS);
                  _persistentRecords._offset[elementIndex]= offset;
                  
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS,double> getSize() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._size;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setSize(const tarch::la::Vector<DIMENSIONS,double>& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._size = (size);
               }
               
               
               
               inline double getSize(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS);
                  return _persistentRecords._size[elementIndex];
                  
               }
               
               
               
               inline void setSize(int elementIndex, const double& size) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS);
                  _persistentRecords._size[elementIndex]= size;
                  
               }
               
               
               /**
                * Generated
                */
               std::string toString() const;
               
               /**
                * Generated
                */
               void toString(std::ostream& out) const;
               
               
               PersistentRecords getPersistentRecords() const;
               /**
                * Generated
                */
               ADERDGCellDescription convert() const;
               
               
            #ifdef Parallel
               protected:
                  static tarch::logging::Log _log;
                  
               public:
                  
                  /**
                   * Global that represents the mpi datatype.
                   * There are two variants: Datatype identifies only those attributes marked with
                   * parallelise. FullDatatype instead identifies the whole record with all fields.
                   */
                  static MPI_Datatype Datatype;
                  static MPI_Datatype FullDatatype;
                  
                  /**
                   * Initializes the data type for the mpi operations. Has to be called
                   * before the very first send or receive operation is called.
                   */
                  static void initDatatype();
                  
                  static void shutdownDatatype();
                  
                  /**
                   * @param communicateSleep -1 Data exchange through blocking mpi
                   * @param communicateSleep  0 Data exchange through non-blocking mpi, i.e. pending messages are received via polling until MPI_Test succeeds
                   * @param communicateSleep >0 Same as 0 but in addition, each unsuccessful MPI_Test is follows by an usleep
                   */
                  void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
                  
                  void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
                  
                  static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                  
                  #endif
                     
                  };
                  
                  #ifdef PackedRecords
                  #pragma pack (pop)
                  #endif
                  
                  
                  #endif
                  
