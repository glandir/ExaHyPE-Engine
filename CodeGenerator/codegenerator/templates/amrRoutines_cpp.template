/**
 * This file is part of the ExaHyPE project.
 * Copyright (c) 2016  http://exahype.eu
 * All rights reserved.
 *
 * The project has received funding from the European Union's Horizon 
 * 2020 research and innovation programme under grant agreement
 * No 671698. For copyrights and licensing, please consult the webpage.
 *
 * Released under the BSD 3 Open Source License.
 * For the full license text, see LICENSE.txt
 **/
 
#include <algorithm> //copy_n

#include "tarch/la/Scalar.h"
#include "tarch/la/ScalarOperations.h"

#include "{{pathToOptKernel}}/Kernels.h"
#include "{{pathToOptKernel}}/DGMatrices.h"
#include "{{pathToOptKernel}}/Quadrature.h"

{% if useLibxsmm %}
#include "{{pathToOptKernel}}/gemmsCPP.h"
{% endif %}


void {{codeNamespace}}::faceUnknownsProlongation(
    double* restrict lQhbndFine,
    double* restrict lFhbndFine,
    const double* restrict lQhbndCoarse,
    const double* restrict lFhbndCoarse,
    const int coarseGridLevel,
    const int fineGridLevel,
    const tarch::la::Vector<DIMENSIONS-1, int>& subfaceIndex
) {
  const int levelDelta = fineGridLevel - coarseGridLevel;

  double lQhbndFineTemp[{{nDof*nDof3D*nDataPad}}] __attribute__((aligned(ALIGNMENT)));
  double lFhbndFineTemp[{{nDof*nDof3D*nVarPad }}] __attribute__((aligned(ALIGNMENT)));
{% if nDim == 3 and useLibxsmm %}
  double coeff[{{nDof*nDofPad}}] __attribute__((aligned(ALIGNMENT)));
{% endif %}

  double * pointerQhbnd1 = 0;
  double * pointerFhbnd1 = 0;

  double * pointerQhbnd2 = 0;
  double * pointerFhbnd2 = 0;

  // This ensures that the pointerQhbnd1 
  // of the last iteration points to lQhbndFine.
  // The same is done for pointerFhbnd1.
  if (levelDelta % 2 == 0) {
    pointerQhbnd1 = lQhbndFineTemp;
    pointerFhbnd1 = lFhbndFineTemp;
  } else {
    pointerQhbnd1 = lQhbndFine;
    pointerFhbnd1 = lFhbndFine;
  }

  int subfaceIndexPrevious_0 = subfaceIndex[0];
  int subfaceIndexCurrent_0;
  int subintervalIndex_0;
{% if nDim==3 %}
  int subfaceIndexPrevious_1 = subfaceIndex[1];
  int subfaceIndexCurrent_1;
  int subintervalIndex_1;
{% endif %}

  // This loop decodes the elements of subfaceIndex into a tertiary basis
  // starting with the highest significance 3^(levelDelta-1).
  // 
  // Per iteration, the digits corresponding to the current significances then determine
  // the subintervals for the single level prolongation.
  for (int l = 1; l < levelDelta+1; ++l) {
    const int significance = tarch::la::aPowI(levelDelta-l,3);
    subfaceIndexCurrent_0 = subfaceIndexPrevious_0 % significance;
    subintervalIndex_0    = (subfaceIndexPrevious_0 - subfaceIndexCurrent_0)/significance;
    assertion(subintervalIndex_0 < 3);
{% if nDim==3 %}
    subfaceIndexCurrent_1 = subfaceIndexPrevious_1 % significance;
    subintervalIndex_1    = (subfaceIndexPrevious_1 - subfaceIndexCurrent_1)/significance;
    assertion(subintervalIndex_1 < 3);
{% endif %}

    // Zero the values of the first pointer.
    std::fill_n(pointerQhbnd1, {{nDof*nDof3D*nDataPad}}, 0.0);
    std::fill_n(pointerFhbnd1, {{nDof*nDof3D*nVarPad }}, 0.0);

    // Apply the single level prolongation operator.
    // Use the coarse level unknowns as input in the first iteration.
    if (l==1) {
{% if useLibxsmm %}
{% if nDim==2 %}
#ifdef USE_IPO
      #pragma forceinline
#endif
      {{gemm_face_Q}}(&lQhbndCoarse[0],&{{codeNamespace}}::fineGridProjector1d[subintervalIndex_0][0],&pointerQhbnd1[0]);
#ifdef USE_IPO
      #pragma forceinline
#endif
      {{gemm_face_F}}(&lFhbndCoarse[0],&{{codeNamespace}}::fineGridProjector1d[subintervalIndex_0][0],&pointerFhbnd1[0]);
{% else %}
      for(int m2=0; m2<{{nDof}}; m2++) {
        for(int n2=0; n2<{{nDof}}; n2++) {
          #pragma vector aligned
          for(int it=0; it<{{nDof*nDofPad}}; it++) {
            coeff[it] = {{codeNamespace}}::fineGridProjector1d[subintervalIndex_0][it] * {{codeNamespace}}::fineGridProjector1d[subintervalIndex_1][m2*{{nDofPad}}+n2];
          }
#ifdef USE_IPO
#ifndef UNSAFE_IPO
          volatile double doNotOptimizeAway_coeff_0 = coeff[0]; //used to prevent the compiler from optimizing temp array away. Needs to be volatile
#endif
          #pragma forceinline
#endif
          {{gemm_face_Q}}(&lQhbndCoarse[n2*{{nDof*nDataPad}}],&coeff[0],&pointerQhbnd1[m2*{{nDof*nDataPad}}]);
#ifdef USE_IPO
      #pragma forceinline
#endif
          {{gemm_face_F}}(&lFhbndCoarse[n2*{{nDof*nVarPad }}],&coeff[0],&pointerFhbnd1[m2*{{nDof*nVarPad }}]);
        }
      }
{% endif %}
{% else %}{# useLibxsmm #}
      // singleLevelFaceUnknownsProlongation<{{nDataPad}}>(pointerQhbnd1, lQhbndCoarse, subintervalIndex);
      for (int m2=0; m2<{{nDof3D}}; m2++) {
        for (int m1 = 0; m1<{{nDof}}; m1++) {
          for (int d = 0; d<{{nDataPad}}; d++) {
            for (int n2=0; n2<{{nDof3D}}; n2++) {
              for (int n1 = 0; n1<{{nDof}}; n1++) {
                pointerQhbnd1[(m2*{{nDof}}+m1)*{{nDataPad}}+d] +=
                    lQhbndCoarse[(n2*{{nDof}}+n1)*{{nDataPad}}+d]
{% if nDim==3 %}
                    * {{codeNamespace}}::fineGridProjector1d[subintervalIndex_1][m2*{{nDofPad}}+n2]
{% endif %}
                    * {{codeNamespace}}::fineGridProjector1d[subintervalIndex_0][m1*{{nDofPad}}+n1];
              }
            }
          }
        }
      }    
      //singleLevelFaceUnknownsProlongation<{{nVarPad }}>(pointerFhbnd1, lFhbndCoarse, subintervalIndex);      
      for (int m2=0; m2<{{nDof3D}}; m2++) {
        for (int m1 = 0; m1<{{nDof}}; m1++) {
          for (int d = 0; d<{{nVarPad}}; d++) {
            for (int n2=0; n2<{{nDof3D}}; n2++) {
              for (int n1 = 0; n1<{{nDof}}; n1++) {
                pointerFhbnd1[(m2*{{nDof}}+m1)*{{nVarPad}}+d] +=
                    lFhbndCoarse[(n2*{{nDof}}+n1)*{{nVarPad }}+d]
{% if nDim==3 %}
                    * {{codeNamespace}}::fineGridProjector1d[subintervalIndex_1][m2*{{nDofPad}}+n2]
{% endif %}
                    * {{codeNamespace}}::fineGridProjector1d[subintervalIndex_0][m1*{{nDofPad}}+n1];
              }
            }
          }
        }
      }
{% endif %}{# useLibxsmm #}
    } else { // l!= 1
{% if useLibxsmm %}
{% if nDim==2 %}
#ifdef USE_IPO
      #pragma forceinline
#endif
      {{gemm_face_Q}}(&pointerQhbnd2[0],&{{codeNamespace}}::fineGridProjector1d[subintervalIndex_0][0],&pointerQhbnd1[0]);
#ifdef USE_IPO
      #pragma forceinline
#endif
      {{gemm_face_F}}(&pointerFhbnd2[0],&{{codeNamespace}}::fineGridProjector1d[subintervalIndex_0][0],&pointerFhbnd1[0]);
{% else %}
      for(int m2=0; m2<{{nDof}}; m2++) {
        for(int n2=0; n2<{{nDof}}; n2++) {
          #pragma vector aligned
          for(int it=0; it<{{nDof*nDofPad}}; it++) {
            coeff[it] = {{codeNamespace}}::fineGridProjector1d[subintervalIndex_0][it] * {{codeNamespace}}::fineGridProjector1d[subintervalIndex_1][m2*{{nDofPad}}+n2];
          }
#ifdef USE_IPO
#ifndef UNSAFE_IPO
          volatile double doNotOptimizeAway_coeff_1 = coeff[0]; //used to prevent the compiler from optimizing temp array away. Needs to be volatile
#endif
          #pragma forceinline
#endif
          {{gemm_face_Q}}(&pointerQhbnd2[n2*{{nDof*nDataPad}}],&coeff[0],&pointerQhbnd1[m2*{{nDof*nDataPad}}]);
#ifdef USE_IPO
          #pragma forceinline
#endif
          {{gemm_face_F}}(&pointerFhbnd2[n2*{{nDof*nVarPad }}],&coeff[0],&pointerFhbnd1[m2*{{nDof*nVarPad }}]);
        }
      }
{% endif %}
{% else%}
      // singleLevelFaceUnknownsProlongation<{{nDataPad}}>(pointerQhbnd1, pointerQhbnd2, subintervalIndex);
      for (int m2=0; m2<{{nDof3D}}; m2++) {
        for (int m1 = 0; m1<{{nDof}}; m1++) {
          for (int d = 0; d<{{nDataPad}}; d++) {
            for (int n2=0; n2<{{nDof3D}}; n2++) {
              for (int n1 = 0; n1<{{nDof}}; n1++) {
                pointerQhbnd1[(m2*{{nDof}}+m1)*{{nDataPad}}+d] +=
                    pointerQhbnd2[(n2*{{nDof}}+n1)*{{nDataPad}}+d]
{% if nDim==3 %}
                    * {{codeNamespace}}::fineGridProjector1d[subintervalIndex_1][m2*{{nDofPad}}+n2]
{% endif %}
                    * {{codeNamespace}}::fineGridProjector1d[subintervalIndex_0][m1*{{nDofPad}}+n1];
              }
            }
          }
        }
      }    
      //singleLevelFaceUnknownsProlongation<{{nVarPad }}>(pointerFhbnd1, pointerFhbnd2, subintervalIndex); 
      for (int m2=0; m2<{{nDof3D}}; m2++) {
        for (int m1 = 0; m1<{{nDof}}; m1++) {
          for (int d = 0; d<{{nVarPad}}; d++) {
            for (int n2=0; n2<{{nDof3D}}; n2++) {
              for (int n1 = 0; n1<{{nDof}}; n1++) {
                pointerFhbnd1[(m2*{{nDof}}+m1)*{{nVarPad}}+d] +=
                    pointerFhbnd2[(n2*{{nDof}}+n1)*{{nVarPad }}+d]
{% if nDim==3 %}
                    * {{codeNamespace}}::fineGridProjector1d[subintervalIndex_1][m2*{{nDofPad}}+n2]
{% endif %}
                    * {{codeNamespace}}::fineGridProjector1d[subintervalIndex_0][m1*{{nDofPad}}+n1];
              }
            }
          }
        }
      }     
{% endif %}{# useLibxsmm #}
    }

    // Prepare next iteration.
    subfaceIndexPrevious_0 = subfaceIndexCurrent_0;
{% if nDim==3 %}
    subfaceIndexPrevious_1 = subfaceIndexCurrent_1;
{% endif %}

    pointerQhbnd2 = pointerQhbnd1;
    pointerFhbnd2 = pointerFhbnd1;
    
    // Toggle the addresses of the pointers.
    if (pointerQhbnd1 == lQhbndFineTemp) {
      pointerQhbnd1 = lQhbndFine;
      pointerFhbnd1 = lFhbndFine;
    } else {
      pointerQhbnd1 = lQhbndFineTemp;
      pointerFhbnd1 = lFhbndFineTemp;
    }
  }

}


void {{codeNamespace}}::faceUnknownsRestriction(
    double* restrict lQhbndCoarse,
    double* restrict lFhbndCoarse,
    const double* restrict lQhbndFine,
    const double* restrict lFhbndFine,
    const int coarseGridLevel,
    const int fineGridLevel,
    const tarch::la::Vector<DIMENSIONS-1, int>& subfaceIndex
) {
  const int levelDelta     = fineGridLevel - coarseGridLevel;

  double lQhbndCoarseTemp1[{{nDof*nDof3D*nDataPad}}] __attribute__((aligned(ALIGNMENT)));
  double lFhbndCoarseTemp1[{{nDof*nDof3D*nVarPad }}] __attribute__((aligned(ALIGNMENT)));
  double lQhbndCoarseTemp2[{{nDof*nDof3D*nDataPad}}] __attribute__((aligned(ALIGNMENT)));
  double lFhbndCoarseTemp2[{{nDof*nDof3D*nVarPad }}] __attribute__((aligned(ALIGNMENT)));
  
{% if nDim == 3 and useLibxsmm %}
  double coeff[{{nDof*nDofPad}}] __attribute__((aligned(ALIGNMENT)));
{% endif %}

  double * pointerQhbnd1 = 0;
  double * pointerQhbnd2 = 0;
  double * pointerFhbnd1 = 0;
  double * pointerFhbnd2 = 0;

  pointerQhbnd1 = lQhbndCoarseTemp1;
  pointerFhbnd1 = lFhbndCoarseTemp1;
  
  int subfaceIndexCurrent_0 = subfaceIndex[0];
  int subintervalIndex_0;
{% if nDim==3 %}
  int subfaceIndexCurrent_1 = subfaceIndex[1];
  int subintervalIndex_1;
{% endif %}
  
  // This loop decodes the indices of subfaceIndex into a tertiary basis
  // starting with the lowest significance 3^0 (in contrast to the prolongation loop).
  //
  // Per iteration, the digits corresponding to the current significances then determine
  // the subintervals for the single level restriction.
  for (int l = 1; l < levelDelta+1; ++l) {
    subintervalIndex_0    = subfaceIndexCurrent_0 % 3;  
    subfaceIndexCurrent_0 = (subfaceIndexCurrent_0 - subintervalIndex_0)/3;
    assertion(subintervalIndex_0 < 3);
{% if nDim==3 %}
    subintervalIndex_1    = subfaceIndexCurrent_1 % 3;
    subfaceIndexCurrent_1 = (subfaceIndexCurrent_1 - subintervalIndex_1)/3;
    assertion(subintervalIndex_1 < 3);
{% endif %}

    // Zero the values of of the first pair of pointers.
    std::fill_n(pointerQhbnd1, {{nDof*nDof3D*nDataPad}}, 0.0);
    std::fill_n(pointerFhbnd1, {{nDof*nDof3D*nVarPad }}, 0.0);

    // Apply the single level restriction operator.
    // Use the fine level unknowns as input in the first iteration.
    if (l==1) {
{% if useLibxsmm %}
{% if nDim==2 %}
#ifdef USE_IPO
      #pragma forceinline
#endif
      {{gemm_face_Q}}(&lQhbndFine[0],&{{codeNamespace}}::fineGridProjector1d_T_weighted[subintervalIndex_0][0],&pointerQhbnd1[0]);
#ifdef USE_IPO
      #pragma forceinline
#endif
      {{gemm_face_F}}(&lFhbndFine[0],&{{codeNamespace}}::fineGridProjector1d_T_weighted[subintervalIndex_0][0],&pointerFhbnd1[0]);
{% else %}
      for(int m2=0; m2<{{nDof}}; m2++) {
        for(int n2=0; n2<{{nDof}}; n2++) {
          #pragma vector aligned
          for(int it=0; it<{{nDof*nDofPad}}; it++) {
            coeff[it] = {{codeNamespace}}::fineGridProjector1d_T_weighted[subintervalIndex_0][it] * {{codeNamespace}}::fineGridProjector1d_T_weighted[subintervalIndex_1][m2*{{nDofPad}}+n2];
          }
#ifdef USE_IPO
#ifndef UNSAFE_IPO
          volatile double doNotOptimizeAway_coeff_0 = coeff[0]; //used to prevent the compiler from optimizing temp array away. Needs to be volatile
#endif
          #pragma forceinline
#endif
          {{gemm_face_Q}}(&lQhbndFine[n2*{{nDof*nDataPad}}],&coeff[0],&pointerQhbnd1[m2*{{nDof*nDataPad}}]);
#ifdef USE_IPO
      #pragma forceinline
#endif
          {{gemm_face_F}}(&lFhbndFine[n2*{{nDof*nVarPad }}],&coeff[0],&pointerFhbnd1[m2*{{nDof*nVarPad }}]);
        }
      }
{% endif %}
{% else %}{# useLibxsmm #}
      //singleLevelFaceUnknownsRestriction<{{nDataPad}}>(pointerQhbnd1, lQhbndFine, subintervalIndex);
      for (int m2=0; m2<{{nDof3D}}; m2++) {
        for (int m1 = 0; m1<{{nDof}}; m1++) {
          for (int d = 0; d<{{nDataPad}}; d++) {
            for (int n2=0; n2<{{nDof3D}}; n2++) {
              for (int n1 = 0; n1<{{nDof}}; n1++) {          
                pointerQhbnd1[(m2*{{nDof}}+m1)*{{nDataPad}}+d] +=
                                 lQhbndFine[(n2*{{nDof}}+n1)*{{nDataPad}}+d]
{% if nDim==3 %}
                                 * {{codeNamespace}}::fineGridProjector1d_T_weighted[subintervalIndex_1][m2*{{nDofPad}}+n2]
{% endif %}
                                 * {{codeNamespace}}::fineGridProjector1d_T_weighted[subintervalIndex_0][m1*{{nDofPad}}+n1];
              }
            }
          }
        }
      }
      //singleLevelFaceUnknownsRestriction<{{nVarPad }}>(pointerFhbnd1, lFhbndFine, subintervalIndex);
      for (int m2=0; m2<{{nDof3D}}; m2++) {
        for (int m1 = 0; m1<{{nDof}}; m1++) {
          for (int d = 0; d<{{nVarPad}}; d++) {
            for (int n2=0; n2<{{nDof3D}}; n2++) {
              for (int n1 = 0; n1<{{nDof}}; n1++) {          
                pointerFhbnd1[(m2*{{nDof}}+m1)*{{nVarPad}}+d] +=
                                 lFhbndFine[(n2*{{nDof}}+n1)*{{nVarPad}}+d]
{% if nDim==3 %}
                                 * {{codeNamespace}}::fineGridProjector1d_T_weighted[subintervalIndex_1][m2*{{nDofPad}}+n2]
{% endif %}
                                 * {{codeNamespace}}::fineGridProjector1d_T_weighted[subintervalIndex_0][m1*{{nDofPad}}+n1];
              }
            }
          }
        }
      }
{% endif %}{# useLibxsmm #}
    } else { // l!= 1
{% if useLibxsmm %}
{% if nDim==2 %}
#ifdef USE_IPO
      #pragma forceinline
#endif
      {{gemm_face_Q}}(&pointerQhbnd2[0],&{{codeNamespace}}::fineGridProjector1d_T_weighted[subintervalIndex_0][0],&pointerQhbnd1[0]);
#ifdef USE_IPO
      #pragma forceinline
#endif
      {{gemm_face_F}}(&pointerFhbnd2[0],&{{codeNamespace}}::fineGridProjector1d_T_weighted[subintervalIndex_0][0],&pointerFhbnd1[0]);
{% else %}
      for(int m2=0; m2<{{nDof}}; m2++) {
        for(int n2=0; n2<{{nDof}}; n2++) {
          #pragma vector aligned
          for(int it=0; it<{{nDof*nDofPad}}; it++) {
            coeff[it] = {{codeNamespace}}::fineGridProjector1d_T_weighted[subintervalIndex_0][it] * {{codeNamespace}}::fineGridProjector1d_T_weighted[subintervalIndex_1][m2*{{nDofPad}}+n2];
          }
#ifdef USE_IPO
#ifndef UNSAFE_IPO
          volatile double doNotOptimizeAway_coeff_1 = coeff[0]; //used to prevent the compiler from optimizing temp array away. Needs to be volatile
#endif
          #pragma forceinline
#endif
          {{gemm_face_Q}}(&pointerQhbnd2[n2*{{nDof*nDataPad}}],&coeff[0],&pointerQhbnd1[m2*{{nDof*nDataPad}}]);
#ifdef USE_IPO
          #pragma forceinline
#endif
          {{gemm_face_F}}(&pointerFhbnd2[n2*{{nDof*nVarPad }}],&coeff[0],&pointerFhbnd1[m2*{{nDof*nVarPad }}]);
        }
      }
{% endif %}
{% else %}{# useLibxsmm #}  
      //singleLevelFaceUnknownsRestriction<{{nDataPad}}>(pointerQhbnd1, pointerQhbnd2, subintervalIndex);
      for (int m2=0; m2<{{nDof3D}}; m2++) {
        for (int m1 = 0; m1<{{nDof}}; m1++) {
          for (int d = 0; d<{{nDataPad}}; d++) {
            for (int n2=0; n2<{{nDof3D}}; n2++) {
              for (int n1 = 0; n1<{{nDof}}; n1++) {          
                pointerQhbnd1[(m2*{{nDof}}+m1)*{{nDataPad}}+d] +=
                                 pointerQhbnd2[(n2*{{nDof}}+n1)*{{nDataPad}}+d]
{% if nDim==3 %}
                                 * {{codeNamespace}}::fineGridProjector1d_T_weighted[subintervalIndex_1][m2*{{nDofPad}}+n2]
{% endif %}
                                 * {{codeNamespace}}::fineGridProjector1d_T_weighted[subintervalIndex_0][m1*{{nDofPad}}+n1];
              }
            }
          }
        }
      }
      //singleLevelFaceUnknownsRestriction<{{nVarPad }}>(pointerFhbnd1, pointerFhbnd2, subintervalIndex);
      for (int m2=0; m2<{{nDof3D}}; m2++) {
        for (int m1 = 0; m1<{{nDof}}; m1++) {
          for (int d = 0; d<{{nVarPad}}; d++) {
            for (int n2=0; n2<{{nDof3D}}; n2++) {
              for (int n1 = 0; n1<{{nDof}}; n1++) {          
                pointerFhbnd1[(m2*{{nDof}}+m1)*{{nVarPad}}+d] +=
                                 pointerFhbnd2[(n2*{{nDof}}+n1)*{{nVarPad}}+d]
{% if nDim==3 %}
                                 * {{codeNamespace}}::fineGridProjector1d_T_weighted[subintervalIndex_1][m2*{{nDofPad}}+n2]
{% endif %}
                                 * {{codeNamespace}}::fineGridProjector1d_T_weighted[subintervalIndex_0][m1*{{nDofPad}}+n1];
              }
            }
          }
        }
      }
{% endif %}{# useLibxsmm #}
    }

    // Prepare next iteration.
    pointerQhbnd2 = pointerQhbnd1;
    pointerFhbnd2 = pointerFhbnd1;
    // Toggle pointer pairs.
    if (pointerQhbnd1 == lQhbndCoarseTemp1) {
      pointerQhbnd1 = lQhbndCoarseTemp2;
      pointerFhbnd1 = lFhbndCoarseTemp2;
    } else {
      pointerQhbnd1 = lQhbndCoarseTemp1;
      pointerFhbnd1 = lFhbndCoarseTemp1;
    }
  }

  // Add restricted fine level unknowns to coarse level unknowns.
  #pragma simd
  for (int it = 0; it < {{nDof*nDof3D*nDataPad}}; it++) {
    lQhbndCoarse[it] += pointerQhbnd2[it];
  }
  #pragma simd
  for (int it = 0; it < {{nDof*nDof3D*nVarPad }}; it++) {
    lFhbndCoarse[it] += pointerFhbnd2[it];
  }

}


void {{codeNamespace}}::faceFluxRestriction(
    double* restrict lFhbndCoarse,
    const double* restrict lFhbndFine,
    const tarch::la::Vector<DIMENSIONS-1, int>& subfaceIndex,
    const int levelDelta
) {

  double lFhbndCoarseTemp1[{{nDof*nDof3D*nVarPad}}] __attribute__((aligned(ALIGNMENT)));
  double lFhbndCoarseTemp2[{{nDof*nDof3D*nVarPad}}] __attribute__((aligned(ALIGNMENT)));
  
{% if nDim == 3 and useLibxsmm %}
  double coeff[{{nDof*nDofPad}}] __attribute__((aligned(ALIGNMENT)));
{% endif %}

  double * pointerFhbnd1 = 0;
  double * pointerFhbnd2 = 0;

  pointerFhbnd1 = lFhbndCoarseTemp1;
  
  int subfaceIndexCurrent_0 = subfaceIndex[0];
  int subintervalIndex_0;
{% if nDim==3 %}
  int subfaceIndexCurrent_1 = subfaceIndex[1];
  int subintervalIndex_1;
{% endif %}
  
  // This loop decodes the indices of subfaceIndex into a tertiary basis
  // starting with the lowest significance 3^0 (in contrast to the prolongation loop).
  //
  // Per iteration, the digits corresponding to the current significances then determine
  // the subintervals for the single level restriction.
  for (int l = 1; l < levelDelta+1; ++l) {
    subintervalIndex_0    = subfaceIndexCurrent_0 % 3;  
    subfaceIndexCurrent_0 = (subfaceIndexCurrent_0 - subintervalIndex_0)/3;
    assertion(subintervalIndex_0 < 3);
{% if nDim==3 %}
    subintervalIndex_1    = subfaceIndexCurrent_1 % 3;
    subfaceIndexCurrent_1 = (subfaceIndexCurrent_1 - subintervalIndex_1)/3;
    assertion(subintervalIndex_1 < 3);
{% endif %}

    // Zero the values of of the first pair of pointers.
    std::fill_n(pointerFhbnd1, {{nDof*nDof3D*nVarPad }}, 0.0);

    // Apply the single level restriction operator.
    // Use the fine level unknowns as input in the first iteration.
    if (l==1) {
{% if useLibxsmm %}
{% if nDim==2 %}
#ifdef USE_IPO
      #pragma forceinline
#endif
      {{gemm_face_F}}(&lFhbndFine[0],&{{codeNamespace}}::fineGridProjector1d_T_weighted[subintervalIndex_0][0],&pointerFhbnd1[0]);
{% else %}
      for(int m2=0; m2<{{nDof}}; m2++) {
        for(int n2=0; n2<{{nDof}}; n2++) {
          #pragma vector aligned
          for(int it=0; it<{{nDof*nDofPad}}; it++) {
            coeff[it] = {{codeNamespace}}::fineGridProjector1d_T_weighted[subintervalIndex_0][it] * {{codeNamespace}}::fineGridProjector1d_T_weighted[subintervalIndex_1][m2*{{nDofPad}}+n2];
          }
#ifdef USE_IPO
#ifndef UNSAFE_IPO
          volatile double doNotOptimizeAway_coeff_0 = coeff[0]; //used to prevent the compiler from optimizing temp array away. Needs to be volatile
#endif
      #pragma forceinline
#endif
          {{gemm_face_F}}(&lFhbndFine[n2*{{nDof*nVarPad }}],&coeff[0],&pointerFhbnd1[m2*{{nDof*nVarPad }}]);
        }
      }
{% endif %}
{% else %}{# useLibxsmm #}
      //singleLevelFaceUnknownsRestriction<{{nVarPad }}>(pointerFhbnd1, lFhbndFine, subintervalIndex);
      for (int m2=0; m2<{{nDof3D}}; m2++) {
        for (int m1 = 0; m1<{{nDof}}; m1++) {
          for (int d = 0; d<{{nVarPad}}; d++) {
            for (int n2=0; n2<{{nDof3D}}; n2++) {
              for (int n1 = 0; n1<{{nDof}}; n1++) {          
                pointerFhbnd1[(m2*{{nDof}}+m1)*{{nVarPad}}+d] +=
                                 lFhbndFine[(n2*{{nDof}}+n1)*{{nVarPad}}+d]
{% if nDim==3 %}
                                 * {{codeNamespace}}::fineGridProjector1d_T_weighted[subintervalIndex_1][m2*{{nDofPad}}+n2]
{% endif %}
                                 * {{codeNamespace}}::fineGridProjector1d_T_weighted[subintervalIndex_0][m1*{{nDofPad}}+n1];
              }
            }
          }
        }
      }
{% endif %}{# useLibxsmm #}
    } else { // l!= 1
{% if useLibxsmm %}
{% if nDim==2 %}
#ifdef USE_IPO
      #pragma forceinline
#endif
      {{gemm_face_F}}(&pointerFhbnd2[0],&{{codeNamespace}}::fineGridProjector1d_T_weighted[subintervalIndex_0][0],&pointerFhbnd1[0]);
{% else %}
      for(int m2=0; m2<{{nDof}}; m2++) {
        for(int n2=0; n2<{{nDof}}; n2++) {
          #pragma vector aligned
          for(int it=0; it<{{nDof*nDofPad}}; it++) {
            coeff[it] = {{codeNamespace}}::fineGridProjector1d_T_weighted[subintervalIndex_0][it] * {{codeNamespace}}::fineGridProjector1d_T_weighted[subintervalIndex_1][m2*{{nDofPad}}+n2];
          }
#ifdef USE_IPO
#ifndef UNSAFE_IPO
          volatile double doNotOptimizeAway_coeff_1 = coeff[0]; //used to prevent the compiler from optimizing temp array away. Needs to be volatile
#endif
          #pragma forceinline
#endif
          {{gemm_face_F}}(&pointerFhbnd2[n2*{{nDof*nVarPad }}],&coeff[0],&pointerFhbnd1[m2*{{nDof*nVarPad }}]);
        }
      }
{% endif %}
{% else %}{# useLibxsmm #}  
      //singleLevelFaceUnknownsRestriction<{{nVarPad }}>(pointerFhbnd1, pointerFhbnd2, subintervalIndex);
      for (int m2=0; m2<{{nDof3D}}; m2++) {
        for (int m1 = 0; m1<{{nDof}}; m1++) {
          for (int d = 0; d<{{nVarPad}}; d++) {
            for (int n2=0; n2<{{nDof3D}}; n2++) {
              for (int n1 = 0; n1<{{nDof}}; n1++) {          
                pointerFhbnd1[(m2*{{nDof}}+m1)*{{nVarPad}}+d] +=
                                 pointerFhbnd2[(n2*{{nDof}}+n1)*{{nVarPad}}+d]
{% if nDim==3 %}
                                 * {{codeNamespace}}::fineGridProjector1d_T_weighted[subintervalIndex_1][m2*{{nDofPad}}+n2]
{% endif %}
                                 * {{codeNamespace}}::fineGridProjector1d_T_weighted[subintervalIndex_0][m1*{{nDofPad}}+n1];
              }
            }
          }
        }
      }
{% endif %}{# useLibxsmm #}
    }

    // Prepare next iteration.
    pointerFhbnd2 = pointerFhbnd1;
    // Toggle pointer pairs.
    if (pointerFhbnd1 == lFhbndCoarseTemp1) {
      pointerFhbnd1 = lFhbndCoarseTemp2;
    } else {
      pointerFhbnd1 = lFhbndCoarseTemp1;
    }
  }

  std::copy_n(pointerFhbnd2,{{nDof*nDof3D*nVarPad}},lFhbndCoarse);

}



void {{codeNamespace}}::volumeUnknownsProlongation(
    double* restrict luhFine,
    const double* restrict luhCoarse,
    const int coarseGridLevel,
    const int fineGridLevel,
    const tarch::la::Vector<DIMENSIONS, int>& subcellIndex
){
  const int levelDelta = fineGridLevel - coarseGridLevel;

  double luhFineTemp[{{nDof*nDof*nDof3D*nData}}] __attribute__((aligned(ALIGNMENT)));
  
{% if useLibxsmm %}
  double coeff[{{nDof*nDofPad}}] __attribute__((aligned(ALIGNMENT)));
{% endif %}

  double * pointerUh1 = 0;
  double * pointerUh2 = 0;

  // This ensures that the first pointer 
  // points to luhFine in the last iteration
  // of the following loop.
  if (levelDelta % 2 == 0) {
    pointerUh1 = luhFineTemp;
  } else {
    pointerUh1 = luhFine;
  }
  
  int subcellIndexPrevious_0 = subcellIndex[0];
  int subcellIndexCurrent_0;
  int subintervalIndex_0;
  int subcellIndexPrevious_1 = subcellIndex[1];
  int subcellIndexCurrent_1;
  int subintervalIndex_1;
{% if nDim==3 %}
  int subcellIndexPrevious_2 = subcellIndex[2];
  int subcellIndexCurrent_2;
  int subintervalIndex_2;
{% endif %}

  // This loop step by step decodes the elements of subcellIndex into a tertiary basis
  // starting with the highest significance 3^(levelDelta-1).
  // 
  // Per iteration, the digits corresponding to the current significances then determine
  // the subintervals for the single level prolongation.
  for (int l = 1; l < levelDelta+1; ++l) {
    const int significance = tarch::la::aPowI(levelDelta-l,3);
    subcellIndexCurrent_0 = subcellIndexPrevious_0 % significance;
    subintervalIndex_0    = (subcellIndexPrevious_0 - subcellIndexCurrent_0)/significance;
    assertion(subintervalIndex_0 < 3);
    subcellIndexCurrent_1 = subcellIndexPrevious_1 % significance;
    subintervalIndex_1    = (subcellIndexPrevious_1 - subcellIndexCurrent_1)/significance;
    assertion(subintervalIndex_1 < 3);
{% if nDim==3 %}
    subcellIndexCurrent_2 = subcellIndexPrevious_2 % significance;
    subintervalIndex_2    = (subcellIndexPrevious_2 - subcellIndexCurrent_2)/significance;
    assertion(subintervalIndex_2 < 3);
{% endif %}

    // Zero the values of the first pointer.
    std::fill_n(pointerUh1, {{nDof*nDof*nDof3D*nData}}, 0.0);

    // Apply the single level prolongation operator.
    // Use the coarse level unknowns as input in the first iteration.
    if (l==1) {
      //singleLevelVolumeUnknownsProlongation<{{nData}}>(pointerUh1, luhCoarse, subintervalIndex);
{% if useLibxsmm %}
      for (int m3=0; m3<{{nDof3D}}; m3++) {
        for (int n3=0; n3<{{nDof3D}}; n3++) {
          for(int m2=0; m2<{{nDof}}; m2++) {
            for(int n2=0; n2<{{nDof}}; n2++) {
              #pragma vector aligned
              for(int it=0; it<{{nDof*nDofPad}}; it++) {
                coeff[it] = {{codeNamespace}}::fineGridProjector1d[subintervalIndex_0][it] 
{% if nDim==3 %}
                          * {{codeNamespace}}::fineGridProjector1d[subintervalIndex_2][m3*{{nDofPad}}+n3]
{% endif %}
                          * {{codeNamespace}}::fineGridProjector1d[subintervalIndex_1][m2*{{nDofPad}}+n2];
              }
#ifdef USE_IPO
#ifndef UNSAFE_IPO
              volatile double doNotOptimizeAway_coeff_0 = coeff[0]; //used to prevent the compiler from optimizing temp array away. Needs to be volatile
#endif
              #pragma forceinline
#endif
              {{gemm_volume}}(&luhCoarse[(n3*{{nDof3D}}+n2)*{{nDof*nData}}],&coeff[0],&pointerUh1[(m3*{{nDof3D}}+m2)*{{nDof*nData}}]);
            }
          }
        }
      }
{% else %}{# useLibxsmm #}
      for (int m3=0; m3<{{nDof3D}}; m3++) {
        for (int m2=0; m2<{{nDof}}; m2++) {
          for (int m1 = 0; m1<{{nDof}}; m1++) {
            for (int d = 0; d<{{nData}}; d++) {
              for (int n3=0; n3<{{nDof3D}}; n3++) {
                for (int n2=0; n2<{{nDof}}; n2++) {
                  for (int n1 = 0; n1<{{nDof}}; n1++) {  
                    pointerUh1[((m3*{{nDof}}+m2)*{{nDof}}+m1)*{{nData}}+d]
                            += luhCoarse[((n3*{{nDof}}+n2)*{{nDof}}+n1)*{{nData}}+d] *
{% if nDim==3 %}
                            {{codeNamespace}}::fineGridProjector1d[subintervalIndex_2][m3*{{nDofPad}}+n3] *
{% endif %}
                            {{codeNamespace}}::fineGridProjector1d[subintervalIndex_1][m2*{{nDofPad}}+n2] *
                            {{codeNamespace}}::fineGridProjector1d[subintervalIndex_0][m1*{{nDofPad}}+n1];
                  }
                }
              }
            }
          }
        }
      }
{% endif %}{# useLibxsmm #}
    } else {
      //singleLevelVolumeUnknownsProlongation<{{nData}}>(pointerUh1, pointerUh2, subintervalIndex);
{% if useLibxsmm %}
      for (int m3=0; m3<{{nDof3D}}; m3++) {
        for (int n3=0; n3<{{nDof3D}}; n3++) {
          for(int m2=0; m2<{{nDof}}; m2++) {
            for(int n2=0; n2<{{nDof}}; n2++) {
              #pragma vector aligned
              for(int it=0; it<{{nDof*nDofPad}}; it++) {
                coeff[it] = {{codeNamespace}}::fineGridProjector1d[subintervalIndex_0][it] 
{% if nDim==3 %}
                          * {{codeNamespace}}::fineGridProjector1d[subintervalIndex_2][m3*{{nDofPad}}+n3]
{% endif %}
                          * {{codeNamespace}}::fineGridProjector1d[subintervalIndex_1][m2*{{nDofPad}}+n2];
              }
#ifdef USE_IPO
#ifndef UNSAFE_IPO
              volatile double doNotOptimizeAway_coeff_1 = coeff[0]; //used to prevent the compiler from optimizing temp array away. Needs to be volatile
#endif
              #pragma forceinline
#endif
              {{gemm_volume}}(&pointerUh2[(n3*{{nDof3D}}+n2)*{{nDof*nData}}],&coeff[0],&pointerUh1[(m3*{{nDof3D}}+m2)*{{nDof*nData}}]);
            }
          }
        }
      }
{% else %}{# useLibxsmm #}
      for (int m3=0; m3<{{nDof3D}}; m3++) {
        for (int m2=0; m2<{{nDof}}; m2++) {
          for (int m1 = 0; m1<{{nDof}}; m1++) {
            for (int d = 0; d<{{nData}}; d++) {
              for (int n3=0; n3<{{nDof3D}}; n3++) {
                for (int n2=0; n2<{{nDof}}; n2++) {
                  for (int n1 = 0; n1<{{nDof}}; n1++) {  
                    pointerUh1[((m3*{{nDof}}+m2)*{{nDof}}+m1)*{{nData}}+d]
                            += pointerUh2[((n3*{{nDof}}+n2)*{{nDof}}+n1)*{{nData}}+d] *
{% if nDim==3 %}
                            {{codeNamespace}}::fineGridProjector1d[subintervalIndex_2][m3*{{nDofPad}}+n3] *
{% endif %}
                            {{codeNamespace}}::fineGridProjector1d[subintervalIndex_1][m2*{{nDofPad}}+n2] *
                            {{codeNamespace}}::fineGridProjector1d[subintervalIndex_0][m1*{{nDofPad}}+n1];
                  }
                }
              }
            }
          }
        }
      }
{% endif %}{# useLibxsmm #}
    }

    // Prepare next iteration.
    subcellIndexPrevious_0 = subcellIndexCurrent_0;
    subcellIndexPrevious_1 = subcellIndexCurrent_1;
{% if nDim==3 %}
    subcellIndexPrevious_2 = subcellIndexCurrent_2;
{% endif %}

    pointerUh2 = pointerUh1;

    // Toggle pointers.
    if (pointerUh1 == luhFineTemp) {
      pointerUh1 = luhFine;
    } else {
      pointerUh1 = luhFineTemp;
    }
  }

}


void {{codeNamespace}}::volumeUnknownsRestriction(
    double* restrict luhCoarse,
    const double* restrict luhFine,
    const int coarseGridLevel,
    const int fineGridLevel,
    const tarch::la::Vector<DIMENSIONS, int>& subcellIndex
) {
  const int levelDelta    = fineGridLevel - coarseGridLevel;

  double luhCoarseTemp1[{{nDof*nDof*nDof3D*nData}}] __attribute__((aligned(ALIGNMENT)));
  double luhCoarseTemp2[{{nDof*nDof*nDof3D*nData}}] __attribute__((aligned(ALIGNMENT)));
  
{% if useLibxsmm %}
  double coeff[{{nDof*nDofPad}}] __attribute__((aligned(ALIGNMENT)));
{% endif %}

  double * pointerUh1 = 0;
  double * pointerUh2 = 0;

  pointerUh1 = luhCoarseTemp1;

  int subcellIndexCurrent_0 = subcellIndex[0];
  int subintervalIndex_0;
  int subcellIndexCurrent_1 = subcellIndex[1];
  int subintervalIndex_1;
{% if nDim==3 %}
  int subcellIndexCurrent_2 = subcellIndex[2];
  int subintervalIndex_2;
{% endif %}
  // This loop step by step decodes the elements of subcellIndex into a tertiary basis
  // starting with the lowest significance 3^0 (in contrast to the prolongation decoding).
  //
  // Per iteration, the digits corresponding to the current significances then determine
  // the subintervals for the single level restriction.
  for (int l = 1; l < levelDelta+1; ++l) {
    subintervalIndex_0    = subcellIndexCurrent_0 % 3;
    subcellIndexCurrent_0 = (subcellIndexCurrent_0 - subintervalIndex_0)/3;
    assertion(subintervalIndex_0 < 3);
    subintervalIndex_1    = subcellIndexCurrent_1 % 3;
    subcellIndexCurrent_1 = (subcellIndexCurrent_1 - subintervalIndex_1)/3;
    assertion(subintervalIndex_1 < 3);
{% if nDim==3 %}
    subintervalIndex_2    = subcellIndexCurrent_2 % 3;
    subcellIndexCurrent_2 = (subcellIndexCurrent_2 - subintervalIndex_2)/3;
    assertion(subintervalIndex_2 < 3);
{% endif %}

    // Zero the values of the first pointer.
    std::fill_n(pointerUh1, {{nDof*nDof*nDof3D*nData}}, 0.0);

    // Apply the single level restriction operator.
    // Use the fine level unknowns as input in the first iteration.
    if (l==1) {
      //singleLevelVolumeUnknownsRestriction<{{nData}}>(pointerUh1, luhFine, subintervalIndex);
{% if useLibxsmm %}
      for (int m3=0; m3<{{nDof3D}}; m3++) {
        for (int n3=0; n3<{{nDof3D}}; n3++) {
          for(int m2=0; m2<{{nDof}}; m2++) {
            for(int n2=0; n2<{{nDof}}; n2++) {
              #pragma vector aligned
              for(int it=0; it<{{nDof*nDofPad}}; it++) {
                coeff[it] = {{codeNamespace}}::fineGridProjector1d_T_weighted[subintervalIndex_0][it] 
{% if nDim==3 %}
                          * {{codeNamespace}}::fineGridProjector1d_T_weighted[subintervalIndex_2][m3*{{nDofPad}}+n3]
{% endif %}
                          * {{codeNamespace}}::fineGridProjector1d_T_weighted[subintervalIndex_1][m2*{{nDofPad}}+n2];
              }
#ifdef USE_IPO
#ifndef UNSAFE_IPO
              volatile double doNotOptimizeAway_coeff_0 = coeff[0]; //used to prevent the compiler from optimizing temp array away. Needs to be volatile
#endif
              #pragma forceinline
#endif
              {{gemm_volume}}(&luhFine[(n3*{{nDof3D}}+n2)*{{nDof*nData}}],&coeff[0],&pointerUh1[(m3*{{nDof3D}}+m2)*{{nDof*nData}}]);
            }
          }
        }
      }
{% else %}{# useLibxsmm #}
      for (int m3=0; m3<{{nDof3D}}; m3++) {
        for (int m2=0; m2<{{nDof}}; m2++) {
          for (int m1 = 0; m1<{{nDof}}; m1++) {
            for (int d = 0; d<{{nData}}; d++) {
              for (int n3=0; n3<{{nDof3D}}; n3++) {
                for (int n2=0; n2<{{nDof}}; n2++) {
                  for (int n1 = 0; n1<{{nDof}}; n1++) {  
                    pointerUh1[((m3*{{nDof}}+m2)*{{nDof}}+m1)*{{nData}}+d] +=
                        luhFine[((n3*{{nDof}}+n2)*{{nDof}}+n1)*{{nData}}+d]
{% if nDim==3 %}
                        * {{codeNamespace}}::fineGridProjector1d_T_weighted[subintervalIndex_2][m3*{{nDofPad}}+n3]
{% endif %}
                        * {{codeNamespace}}::fineGridProjector1d_T_weighted[subintervalIndex_1][m2*{{nDofPad}}+n2]
                        * {{codeNamespace}}::fineGridProjector1d_T_weighted[subintervalIndex_0][m1*{{nDofPad}}+n1];
                  }
                }
              }
            }
          }
        }
      }
{% endif %}{# useLibxsmm #}
    } else {
      //singleLevelVolumeUnknownsRestriction<{{nData}}>(pointerUh1, pointerUh2, subintervalIndex);
{% if useLibxsmm %}
      for (int m3=0; m3<{{nDof3D}}; m3++) {
        for (int n3=0; n3<{{nDof3D}}; n3++) {
          for(int m2=0; m2<{{nDof}}; m2++) {
            for(int n2=0; n2<{{nDof}}; n2++) {
              #pragma vector aligned
              for(int it=0; it<{{nDof*nDofPad}}; it++) {
                coeff[it] = {{codeNamespace}}::fineGridProjector1d_T_weighted[subintervalIndex_0][it] 
{% if nDim==3 %}
                          * {{codeNamespace}}::fineGridProjector1d_T_weighted[subintervalIndex_2][m3*{{nDofPad}}+n3]
{% endif %}
                          * {{codeNamespace}}::fineGridProjector1d_T_weighted[subintervalIndex_1][m2*{{nDofPad}}+n2];
              }
#ifdef USE_IPO
#ifndef UNSAFE_IPO
              volatile double doNotOptimizeAway_coeff_1 = coeff[0]; //used to prevent the compiler from optimizing temp array away. Needs to be volatile
#endif
              #pragma forceinline
#endif
              {{gemm_volume}}(&pointerUh2[(n3*{{nDof3D}}+n2)*{{nDof*nData}}],&coeff[0],&pointerUh1[(m3*{{nDof3D}}+m2)*{{nDof*nData}}]);
            }
          }
        }
      }
{% else %}{# useLibxsmm #}
      for (int m3=0; m3<{{nDof3D}}; m3++) {
        for (int m2=0; m2<{{nDof}}; m2++) {
          for (int m1 = 0; m1<{{nDof}}; m1++) {
            for (int d = 0; d<{{nData}}; d++) {
              for (int n3=0; n3<{{nDof3D}}; n3++) {
                for (int n2=0; n2<{{nDof}}; n2++) {
                  for (int n1 = 0; n1<{{nDof}}; n1++) {  
                    pointerUh1[((m3*{{nDof}}+m2)*{{nDof}}+m1)*{{nData}}+d] +=
                        pointerUh2[((n3*{{nDof}}+n2)*{{nDof}}+n1)*{{nData}}+d]
{% if nDim==3 %}
                        * {{codeNamespace}}::fineGridProjector1d_T_weighted[subintervalIndex_2][m3*{{nDofPad}}+n3]
{% endif %}
                        * {{codeNamespace}}::fineGridProjector1d_T_weighted[subintervalIndex_1][m2*{{nDofPad}}+n2]
                        * {{codeNamespace}}::fineGridProjector1d_T_weighted[subintervalIndex_0][m1*{{nDofPad}}+n1];
                  }
                }
              }
            }
          }
        }
      }
{% endif %}{# useLibxsmm #}
    }

    // Prepare next iteration.
    pointerUh2 = pointerUh1;

    // Toggle the addresses of the pointers.
    if (pointerUh1 == luhCoarseTemp1) {
      pointerUh1 = luhCoarseTemp2;
    } else {
      pointerUh1 = luhCoarseTemp1;
    }
  }

  // Add restricted fine level unknowns to coarse level unknowns.
  #pragma simd
  for (int it = 0; it < {{nDof*nDof*nDof3D*nData}}; it++) {
    luhCoarse[it] += pointerUh2[it];
  }

}
