#!/usr/bin/env python
# -- SK, 2016

"""
The convergence_table module is the key part in the libconvergence Python
library (Python2 or Pyton3 agnostic). It reads out the .env environment files
generated by the templated runner and .asc files generated by the ExaHyPE
integration routines.
This program is suitable to be run from command line as a standalone program,
interactively from (i)Python or itcan be embedded into other applications,
cf. the ConvergenceFrontend classes.

When running from command line, by default it looks for simulations in
the current working directory subdirectory "simulations/". If you want
it to point somewhere else, set the SIMBASE environment variable.

When running interactively, you can use make of the individual methods to
partially run the program and inspect the pandas tables which are stored
as class properties.

Example Python session
======================

# useful to start like this to enable python loggers:
$ ipython -i ./convergence_table.py -- --skip-plots
> reporter = ConvergenceReporter()
> reporter.reasonable_defaults()


Dependencies
============

Note that we have weird effects with pandas versions < 0.17.
Therefore, using pandas >= 0.17 is highly recommended.
"""

import numpy as np
import pandas as pd
pd.set_option('expand_frame_repr', False) # display for debugging in terminal
# batteries:
from glob import glob
from os import path, stat, getenv
from re import match, sub
import itertools, operator, sys, logging, inspect

# a module in this directory
from convergence_helpers import read_simulation_params, is_empty_file, simfile, gensvg, \
	Template, shortenPathsInTable, stripConstantColumns, keepColumnsIf, time_parser, \
	RemoveStringInColumns, is_headless, shell, StringIO, MethodActions, \
	pdsort, merge_dicts, df_case_insensitive
from convergence_arghelpers import ExaFrontend

def exitConvergenceStatus(convergencePassed):
	"other programs can use exit value to determine outcome of test"
	sys.exit(0 if convergencePassed else -3)

class Namespace:
	""""
	A namespace for collecting stuff, usage like
	> a = Namespace()
	> a.foo = "bar"
	> print a.asDict()
	>> { 'foo': 'bar' }
	This is used in the convergence_table for collecting index names.
	"""
	def asDict(self):
		"Get a dict of the values hold by this namespace"
		return vars(self)
	def __str__(self):
		return 'Namespace(%s)' % self.asDict()

# a global variable to hold various index names we just define in time
idx = Namespace()

class ConvergenceReporter:
	"""
	Does the actual convergence computation. Reads simulations from a list
	of succeeded simulations and uses the environment variable SIMBASE to
	determine the path to the simulations.
	"""

	# all tasks, chained
	steps = MethodActions()

	def __init__(self):
		self.logger = logging.getLogger("ConvergenceReporter")

		# be_headless: do not show up matplotlib windows even if an X11
		# terminal is attached. Set False if you want to do work with plots.
		self.be_headless = True

		# run simulation statistics program before with
		# ./showSimulationProgress.sh  | grep FINISHED > simulations.txt
		# or so. Make sure it is a CSV table with column names, not only simulation name!
		self.simulationListFilename = None

		# add a path before the simulations names, if neccessary.
		self.simulationPathPrefix = getenv('SIMBASE', 'simulations/')
		self.simulationPrefixer = lambda subpath: path.join(self.simulationPathPrefix, subpath)

		# do the same to determine where the templates are.
		self.templatePathPrefixer = lambda templatefname: path.join(self.libconvergence_dir(), templatefname)

		# which quantity shall we look at, in the moment?
		self.quantity = 'error-rho.asc' # rho should always be there, good default.
		
		# at which CSV file do we expect the timestep sizes?
		self.timestep_file = 'timesteps.csv'

		self.tpl = {} # common template variables
		
		self.plt = False # yeah.

	def libconvergence_dir(self):
		try:
			return path.dirname(path.realpath(__file__))
		except Exception as e:
			self.logger.warn("Could not reliably determine libconvergence directory.")
			return '.'
		
	def prepare_matplotlib(self):
		# we can generate plots, create strings holding the SVG file and embed
		# the figures as inline SVG to the HTML file.
		if not self.plt:
			import matplotlib
			if self.be_headless or is_headless():
				matplotlib.use('Agg')
			import matplotlib.pyplot as plt
			plt.ion(); plt.clf()
			self.plt = plt
		return self.plt

	def readSimulationProgress(self):
		"""
		Calls the shell script to read off the progress
		"""
		self.logger.info("Calling showSimulationProgress.sh to see progress")
		script = path.join(self.libconvergence_dir(), "showSimulationProgress.sh")
		try:
			csv = shell(script)
		except Exception as  e:
			self.logger.error("Could not execute '%s', error:" % script)
			self.logger.exception(e)
		self.logger.info("Simulation status:\n" + csv)
		return csv

	def add_group(self, argparser):
		group = argparser.add_argument_group('ConvergenceReporter', description=inspect.cleandoc(self.__doc__))
		group.add_argument('-o', '--porder', type=int, help="Do reporting only for one polynomial order. Default is all.")
		group.add_argument('--quantity', type=str, default=self.quantity, help="Which quantity to look at")
		group.add_argument('--simulations', type=str, default=self.simulationListFilename, help="Path to CSV file "+
			"holding the simulations to consider. If not given, generates list by ./showSimulationProgress.sh on the fly")
		group.add_argument('--minimal-reduction-length', type=int, default=20, help="Minimal number of lines in reductions to include")
		group.add_argument('--skip-plots', action='store_true', default=False, help="Skip the generation of inlined SVG plots")
		group.add_argument('--show-timesteps', action='store_true', default=False, help="Read timestep sizes from "+self.timestep_file)
		return group

	def apply_args(self, args, argparser):
		if args.porder:
			p = args.porder
			self.logger.info("Creating convergence table only for p=%d" % p)
			self.simulations = glob(self.simulationPrefixer('simulations/p%d*/'%p)) # mind the trailing slash to glob only directories
			self.report_outputfile = self.simulationPrefixer("generated-report-p%d.html"%p)
		else:
			self.logger.info("Using all polynomial orders for reporting")
			self.simulations = None
			self.report_outputfile = self.simulationPrefixer("generated-report.html")

		if args.simulations:
			self.logger.info("Reading off simulations from file %s" % args.simulations)
			self.simulationListFilename = args.simulations
		else:
			self.simulationListFilename = None

		self.quantity = args.quantity
		self.minimal_reduction_length = args.minimal_reduction_length
		self.skip_plots = args.skip_plots
		self.show_timesteps = args.show_timesteps

		# first time we can do debug output which is not suppressed
		self.logger.info("Working with SIMBASE='%s'" % self.simulationPathPrefix)
		# Print out some information about the versions used,
		# as we experience version dependent stuff quite frequently
		firstline = lambda possibly_multiline_string:  possibly_multiline_string.split("\n")[0]
		self.logger.debug("Python version: %s" % firstline(sys.version))
		self.logger.debug("Numpy version: %s" % np.version.version)
		self.logger.debug("Pandas version: %s" % pd.version.version)

	def reasonable_defaults(self):
		"Might be used from command line as an alternative to apply_args"
		self.skip_plots = True
		self.simulationListFilename = StringIO(self.readSimulationProgress())
		self.simulations = None  # determine automatically
		self.report_outputfile = reporter.simulationPrefixer("generated-report.html")

	def start(self):
		"Compose the solution, step by step"

		def steplog(i, key):
			self.logger.info("Step %d/%d: %s" % (i+1, len(self.steps.list()), self.steps.storage[key]))

		self.steps.sortedcall(self, steplog)

		return self.convergencePassed

	@steps.add(1, "Setting up basic simulation list")
	def collectSimulations(self):
		"""
		First step, collecting basic information about simulations and removing bad ones.
		Will change self.simulations..
		"""

		if not self.simulationListFilename:
			# moved from apply_args due to self.logger initialization issues
			self.simulationListFilename = StringIO(self.readSimulationProgress())

		# simulations is a list to dictionaries holding simulation data.
		self.quantityfile = path.join("output/", self.quantity)

		# these are names for columns in our dataframes which can appear as columns
		# in input and output data
		idx.SimName = 'SimulationName' # also used in showSimulationProgress.sh
		idx.ParamFileName = 'ParamFileName'
		idx.QuantityFileName = 'QuantityFileName'
		idx.TimestepFileName = 'TimestepFileName'

		# Load table of simulations
		# The shellscript takes around ~ 10-60 Seconds to generate the data, therefore
		# the generation is offloaded.
		self.statisticstable = pd.read_csv(self.simulationListFilename, sep="\t")
		assert idx.SimName in self.statisticstable.columns, "statisticstable misses essential data: \n"+str(self.statisticstable)

		# strip whitespace so a comparison is possible
		self.statisticstable[idx.SimName] = self.statisticstable[idx.SimName].map(str.strip)

		# allow to join a path before if simnames are somewhat broken or so.
		if self.simulationPathPrefix:
		    self.logger.info("Applying prefix '%s' on each simulation name" % self.simulationPathPrefix)
		    prefixer = lambda simname: path.join(self.simulationPathPrefix, simname)
		    # if working with pandas:
		    self.statisticstable[idx.SimName] = self.statisticstable[idx.SimName].apply(prefixer)
		    # if working with the list, interchange with if not simulations: ... but doesn't work.
		    #simulations = map(prefixer, simulations)

		if not self.simulations:
			self.simulations = list(self.statisticstable[idx.SimName])

		if not len(self.simulations):
			self.logger.error("I have not found any simulations")

		self.overview = Template(
			self.templatePathPrefixer("template-overview.html"),
			self.report_outputfile).addStatistics()
		self.evolution = Template(
			self.templatePathPrefixer("template-evolution.html"), 
			self.simulationPrefixer("evolution.html")).addStatistics()
		self.overview.set('LINK_DETAILED_REPORT', path.basename(self.evolution.outputfile)) # link them together
		self.tpl['QUANTITY'] = unicode(self.quantity, 'utf-8') # ...

		self.logger.info("Collected %d simulations suitable for proceeding. " % len(self.simulations))
		for t in enumerate(self.simulations): self.logger.debug(" Simulation %2i. %s" % t)

		return self.simulations

	@steps.add(2, "Retrieving simulation parameters and statistics")
	def retrieveSimulationData(self):
		"""
		Basically takes simulations list and reads in all the data (environment variables and
		ASCII CSV tables). Computes missing quantities.
		"""

		# store as class parameters for debugging purposes
		self.paramfiles = map(simfile('parameters.env'), self.simulations)
		self.quantityfiles = map(simfile(self.quantityfile), self.simulations)
		self.timestepfiles = map(simfile(self.timestep_file), self.simulations)

		# 2A) In the first step, simtable will hold information about simulation paths
		self.simtable = pd.DataFrame()
		self.simtable[idx.SimName] = self.simulations
		self.simtable[idx.ParamFileName] = self.paramfiles
		self.simtable[idx.QuantityFileName] = self.quantityfiles
		self.simtable[idx.TimestepFileName] = self.timestepfiles

		# 2B) read parameter files of each simulation
		paramdicts = map(read_simulation_params, self.paramfiles) # list of dicts
		self.paramtable = pd.DataFrame(paramdicts) # single table
		self.paramtable[idx.SimName] = self.simulations # give an index column

		# 2C) Compute number of cells, etc.

		# A case-insensitive finder for paramtable
		self.ci_paramtable = lambda colname: df_case_insensitive(self.paramtable, colname)
		self.get_ci_paramtable = lambda colname: self.paramtable[self.ci_paramtable(colname)]

		self.porders = self.get_ci_paramtable('EXAPORDER').astype(int)
		meshsizes = self.get_ci_paramtable('EXAREALMESHSIZE') # if not available, use 'EXAMESHSIZE'
		widths = self.get_ci_paramtable('ExaWidth') # used to be calles EXASPEC_WIDTH
		# shall be all equal, of course.
		if not np.all(widths == widths[0]):
			self.logger.warning("Not all simulation domains are equal! Sizes are: ", widths)
		ncells_float = widths / meshsizes
		self.ncells = np.rint(ncells_float).astype(int)
		assert np.all( np.abs(ncells_float - self.ncells) < 1e-10), "Nonintegral number of cells: "+str(ncells_float)

		# 2D) Beautifying of paramtable
		# the following beautifying is only done for printing the paramtable
		# beautify the parameter table: keep only columns with "EXA" inside
		## paramtable = keepColumnsIf(paramtable, lambda c: 'EXA' in c)
		# filter out PWD and OLDPWD which sometimes occur -.-
		self.paramtable = keepColumnsIf(self.paramtable, lambda c: 'PWD' not in c)

		# beautify the parameter table: shorten the paths
		shortenPathsInTable(self.paramtable, map(self.ci_paramtable, ['EXABINARY', 'ExaSpecFile']))
		# store number of cells back into paramtable for dumping
		idx.Ncells = 'nCells'
		self.paramtable[idx.Ncells] = self.ncells

		self.tpl['SIMULATION_PARAMETERS_TABLE'] = self.paramtable.to_html()
		self.tpl['SIMULATION_STATISTICS_TABLE'] = self.statisticstable.to_html()

		# todo: Extract the "meaning" columns which hold documentation about the exa-columns.

		# 2F) Compose a shorter simulation table which contains paramtable and statistics
		#     as well as shorten by extracting constant parameters
		self.fullsimtable = pd.merge(self.simtable, pd.merge(self.paramtable, self.statisticstable, on=idx.SimName), on=idx.SimName)

		# shorten paths for reduction
		shortenPathsInTable(self.fullsimtable, [idx.ParamFileName, idx.QuantityFileName])
		self.reducedsimtable, self.constant_parameters = stripConstantColumns(self.fullsimtable)
		# delete stuff we don't need, beautify up table
		self.reducedsimtable = keepColumnsIf(self.reducedsimtable, lambda c: 'exabinary' not in c.lower())
		RemoveStringInColumns(self.reducedsimtable, 'EXA', inplace=True)

		# display simulation paths as hyperlinks. The links only work if the output HTML is in the
		# right directory (straight in SIMBASE, ie. "simulations/" root).
		linkifyFormatter = { idx.SimName: lambda l: u"<a href='{0}'>{0}</a>".format(path.basename(l)) }
		# This also works instead:
		#self.reducedsimtable[idx.SimName] = self.reducedsimtable[idx.SimName].apply(lambda l: '<a href="{0}">{0}</a>'.format(l))

		# avoid truncating the strings, cf. older pandas versions http://stackoverflow.com/a/12001086
		pd.options.display.max_colwidth = 200
		self.tpl['SIMULATION_TABLE'] = self.reducedsimtable.to_html(formatters=linkifyFormatter, escape=False)
		self.tpl['CONSTANT_PARAMETERS'] = self.constant_parameters.to_html()

		# 2H) Compute the total walltime
		idx.Walltime = 'Walltime' # as set in showSimulationProgress.sh
		# standard time format (from /bin/time) is like "601m17.780s".
		# We ignore the seconds and sum up the minutes
		timedeltas = map(time_parser(r'\s*(?P<minutes>[\d]+)'), self.fullsimtable[idx.Walltime])
		# minor bug: this casts timedelta(0) == False.
		wellparsedCriterion = lambda timedelta: not (not timedelta)
		correctlyparsed = filter(wellparsedCriterion, timedeltas)
		errnousentries = len(timedeltas) - len(correctlyparsed) # find not correctly parsed entries
		timedeltas = correctlyparsed
		try:
		    totaltime = reduce(operator.add, timedeltas)
		    totalhours = totaltime.total_seconds() / (60*60)
		    self.tpl['TOTAL_CPU_HOURS'] = (u"%.1f" % totalhours) + (u" (ignoring %i errnous entries)"%errnousentries if errnousentries else u"")
		except:
		    self.tpl['TOTAL_CPU_HOURS'] = u"Not determinable"

		return self.fullsimtable

	@steps.add(3, "Load Error tables and compute convergence rate")
	def computeConvergenceRate(self):
		"""
		Based on paramtable and simtable, goes on.
		"""

		# 3A) Filter out all bad entries which are no further interesting for automatic
		#     processing

		# What criterion do make to filter out bad simulations? These are examples:
		minimalReductionsLengths = lambda SimRow: SimRow['EachRedLength'] > self.minimal_reduction_length
		emptyDirectoryCheck = lambda SimRow: not is_empty_file(SimRow[idx.SimName])
		# now choose:
		goodSimulationCriterion = minimalReductionsLengths

		# Compute the filter of entries which will be processed in the further steps
		includedSimulations = self.fullsimtable.apply(goodSimulationCriterion, axis=1) # row-wise
		idx.IgnoredSims='IgnoredSimulations'
		IgnoredKeyworder = lambda b: "Included" if b else "Discarded"
		self.fullsimtable[idx.IgnoredSims] = includedSimulations.map(IgnoredKeyworder)

		if not len(self.paramtable[includedSimulations]):
			self.logger.error("We filtered out all simulations as being noninteresting.")
			self.logger.error("This may be because all simulations are tainted or files where missing.")
			self.logger.error("You might exemplarily want to look into these two first simulation set files:")
			self.logger.error(self.paramfiles[0])
			self.logger.error(self.quantityfiles[0])
			self.logger.error("This is what I learned so far about the simulations:\n" + str(self.reducedsimtable))
			self.logger.error("With constants:\n" + str(self.constant_parameters))
			raise LookupError("Cannot continue: All simulations were filtered as noninteresting.")

		# apply filter
		self.paramtable = self.paramtable[includedSimulations]
		self.simtable = self.simtable[includedSimulations]
		self.quantityfiles = self.simtable[idx.QuantityFileName]
		self.ncells = self.ncells[includedSimulations]
		self.porders = self.porders[includedSimulations]

		# 2B) Load the error table CSV files

		# Caveats with header detection is very sensible to the first line's format.
		# this will not work:
		#    # 1:plotindex ,2:time ,3:l1norm ,4:l2norm ,5:max ,6:min ,7:avg ,
		# in constrast, the line has to look like
		#    1:plotindex 2:time 3:l1norm 4:l2norm 5:max 6:min 7:avg
		# or even better
		#    plotindex time l1norm l2norm max min avg
		# which allows us to directly address the columns with their names.

		# Read in of the actual error tables
		self.errortables = [pd.read_csv(qf, delim_whitespace=True) for qf in self.quantityfiles]

		# we do have one set of parameters for each error table
		assert len(self.errortables) == len(self.paramtable)

		# assign the index to each error table
		idx.cells = 'nCells' # name of 'number of cells' column
		idx.porder = 'pOrder' # name of 'polynomial order' column

		assert len(self.ncells) == len(self.porders)
		assert len(self.porders) == len(self.errortables)
		for simncells,simporder,errortable in zip(self.ncells,self.porders,self.errortables):
			errortable[idx.cells] = simncells
			errortable[idx.porder] = simporder

		# merge data frames to multiindex
		self.errors = pd.concat(self.errortables).reset_index() #, keys=ncells, names=[idx.cells])

		# how much data rows can we compare? This is the number of maximum
		# common evolution steps
		simulationsPerNCells = self.errors.groupby(by=idx.cells).size()
		maxcomparisons = simulationsPerNCells.min()

		self.logger.debug("For the following number of cells, we have this number of simlations:\n" + str(simulationsPerNCells))
		self.logger.info("Comparing up to %d simulations for convergence analysis.." % maxcomparisons)

		# too much data:
		#print "This is the full error table from all simulations (%d entries):" % len(errors)
		#print errors

		idx.plotindex = 'plotindex' # the column counting the rows in each simulation


		# 2B) Compute the convergence rate

		# we can either choose all overlapping data points for the common
		# error (ceslice) or only the very last entry (celast). With the first
		# one we can do plots showing the convergence order during evolution,
		# with the last one we can show compact tables.
		ceslice = self.errors.query('%s <= %d' % (idx.plotindex, maxcomparisons))
		celast = pdsort(ceslice.groupby(by=[idx.porder,idx.cells], as_index=False).last(), by=[idx.porder,idx.cells])
		# either do groupby(..., as_index=False) and then .sort([idx.porder,idx.cells])
		# or do as above: groupby() with index but still taking .last()
		# celast: could also replace .last() by .tail(1) but would loose row index information
		# CAVEAT: Make sure indices are unique! Otherwise the `for row in ce.iterrows()` will fail.
		ce = ceslice

		# no index any more
		# to get rid of the index:
		#ce = ce.reset_index()
		#ce.sort(idx.cells, inplace=True) # inserts a "level_1" column

		# find the reference nCells against which we make the convergence test.
		# This is usually the one nCell at the same plotindex smaller than the
		# actual one.
		def findSmallerNcellsThan(num):
			smallerNcells = self.ncells[ self.ncells < num ]
			return np.nan if not len(smallerNcells) else smallerNcells.max()

		#def compute_convergence_order(ce):
		#	"ce: common error table"
		#	ce = ce.copy(deep=True)
		idx.prev = 'nSmaller' # column name of linked idx.cells ('nCells')
		ce[idx.prev] = ce[idx.cells].apply(findSmallerNcellsThan)

		# insert the convergence measures for these columns
		# we dropped 'min avg' as they don't tell us so much concerning convergence
		idx.errorColumns = 'l1norm l2norm max'.split()
		idx.rateColumns = ["o"+c for c in idx.errorColumns]
		idx.time = 'time' # the column for time

		# add the new columns 
		for newcol in idx.rateColumns:
			ce[newcol] = 0.0

		# Turn on or off row entanglement debugging
		giveComparisonColumn = False

		idx.comparisoncolumn="smallerRow"
		if giveComparisonColumn:
			ce[idx.comparisoncolumn] = 'default'

		# compute the actual convergence number for each column
		# This requires unique indices in each row.
		for rowindex, row in ce.iterrows():
			if (not row[idx.prev]) or np.isnan(row[idx.prev]):
				# there is no smaller resolution available
				if giveComparisonColumn:
					ce.set_value(rowindex, idx.comparisoncolumn, 'noSmaller')
				continue

			# this is the crucial search for the comparable row. We don't rely on
			# sorted rows but do an expensive search based on values here.
			targetrow = ce[(ce[idx.cells] == row[idx.prev]) & (ce[idx.plotindex] == row[idx.plotindex]) & (ce[idx.porder] == row[idx.porder])]
			if targetrow.empty:
				# could not find a previous step
				self.logger.error("Did not find a target row for rowindex=%d and row[%s]=%s"  % (rowindex, idx.prev, str(row[idx.prev])))
				if giveComparisonColumn:
					ce.set_value(rowindex, idx.comparisoncolumn, 'CouldNotFind')
				continue

			if len(targetrow) != 1:
				self.logger.error("Could not find unique counterpart for row ", row)
				self.logger.error("Found instead %d rows: " % len(targetrow), targetrow)
				if giveComparisonColumn:
					ce.set_value(rowindex, idx.comparisoncolumn, 'FoundMultiple')
				continue

			# very verbose output
			#print "I am in row %d and matched nCells(%f) with prev(%f), outcome:" % (rowindex, row[idx.cells], row[idx.prev])
			#print pd.concat([ row.to_frame().T, targetrow ])

			for col, outcol in zip(idx.errorColumns, idx.rateColumns):
				value = np.log(row[col] / targetrow[col]) / np.log( targetrow[idx.cells] / row[idx.cells] )
				#print "Computed row[%s]=%f" % (outcol, value)
				ce.set_value(rowindex, outcol, value)

			if giveComparisonColumn:
				ce.set_value(rowindex, idx.comparisoncolumn, str(list(targetrow[[idx.plotindex,idx.cells,idx.time,'l1norm']].values.flatten()))) # yes that's stupid


		#self.logger.info("Computing convergence tables...")
		#convEvolution = compute_convergence_order(ceslice)
		#convFinal = compute_convergence_order(celast)

		# nice compact display of small and large floats, integers
		#compactfloat = lambda f: sub(r'\.0+$', '',(u'%.3'+('f' if abs(f)<999 else 'e'))%f)
		#compactfloat = lambda f: (u'%.3'+('f' if abs(f)<999 else 'e'))%f

		# full tables which do *not* go to the overview

		# print out a subset of the table
		self.convergence_table = ce[[idx.porder,idx.cells,idx.prev,idx.plotindex,idx.time] + idx.errorColumns + idx.rateColumns + ([idx.comparisoncolumn] if giveComparisonColumn else [])]
		self.logger.debug("Computed this convergence table for the individual reductions:\n" + str(self.convergence_table) )
		self.convergence_table = pdsort(self.convergence_table, by=[idx.porder, idx.cells, idx.plotindex])
		self.final_time_convergence_table = self.convergence_table.groupby(by=[idx.porder, idx.cells]).last()

		self.tpl['ERROR_EVOLUTION_TABLE'] = self.errors.to_html()
		self.tpl['CONVERGENCE_EVOLUTION_TABLE'] = self.convergence_table.to_html() #float_format=compactfloat)
		self.tpl['COMBINED_FINAL_CONVERGENCE_ERRROR_TABLE'] = self.final_time_convergence_table.to_html()

		return self.convergence_table
	
	@steps.add(3.5, "Timestep plotting")
	def analyze_timesteps(self):
		if not self.show_timesteps:
			self.logger.info("Skipping timestep plots");
			self.tpl['TIMESTEP_SVG_FIGURE'] = u"<em>skipped</em>";
			return
		
		plt = self.prepare_matplotlib()
		timestepPlot = plt.figure(figsize=(18,8))

		timestep_statistics = []
		for simncells,simporder,tstfname in zip(self.ncells,self.porders,self.simtable[idx.TimestepFileName]):
			try:
				timesteps = pd.read_csv(tstfname, delim_whitespace=True)
				# The columns
				idx.tstep = 'step'
				idx.t_min = 't_min'
				idx.dt_min = 'dt_min'
				
				# just go on and make a plot
				plt.plot(timesteps[idx.t_min], timesteps[idx.dt_min], "o-", label="P=%d,Nc=%d"%(simporder,simncells))

				# poor mans "append row to pandas"				
				dt = timesteps[idx.dt_min]
				idx.dtmean = 'dt_mean'
				idx.dtstd = 'dt_std'
				timestep_statistics.append({
					idx.cells: simncells, idx.porder: simporder,
					idx.dtmean: dt.mean(), idx.dtstd: dt.std()
				})
			except IOError as e:
				self.logger.warn("Could not read in timestep table '%s', reason: '%s'. Proabbly you have to run the timestep parser before." % (tstfname, str(e)))
			except Exception as e:
				self.logger.error("Error when dispatching the timestep table for '%s" % tstfname)
				self.logger.exception(e)

		plt.grid()
		plt.legend(loc='center left', bbox_to_anchor=(1,0.5))
		plt.subplots_adjust(right=0.8)
		plt.title("Timestep sizes")
		plt.xlabel("Simulation time (t_min)")
		plt.ylabel("Timestep size (dt_min)")
		ax = plt.gca()
		#ax.set_yscale('log')

		self.tpl['TIMESTEP_SVG_FIGURE'] = gensvg(timestepPlot)

		if len(timestep_statistics):
			self.timestep_statistics = pd.DataFrame.from_dict(timestep_statistics)
			self.tpl['TIMESTEP_STATISTICS_TABLE'] = self.timestep_statistics.to_html()
		else:
			self.tpl['TIMESTEP_STATISTICS_TABLE'] = u"<em>Was not computed.</em>"

	@steps.add(4, "Convergence evolution and order plots")
	def do_plots(self):
		if self.skip_plots:
			self.logger.info("Skipping plot creation")
			self.tpl['CONVERGENCE_SVG_FIGURE'] = u"<em>skipped plot generation</em>"
			self.tpl['ERROR_SVG_FIGURE'] = u"<em>skipped plot generation</em>"
			return

		plt = self.prepare_matplotlib()

		uniquelist = lambda k: list(np.unique(k))
		uniqueintlist = lambda k: map(int, uniquelist(k))
		lookupdict = lambda k,v: dict(zip(k,v))
		window = lambda l: itertools.izip(l, l[1:])
		iround = lambda f: int(round(f))
		# repeat tile until it fits to target. or reduce tile down to target.
		repeatfit = lambda tile, target: (max(1,int(len(target)/len(tile)))*tile)[:len(target)]
		lookupfitdict = lambda k,v: dict(itertools.izip(k, itertools.cycle(v))) # lookupdict(k, repeatfit(v, k))

		allporders, allncells = uniqueintlist(self.porders), uniqueintlist(self.ncells)
		self.logger.info("Plots are for porders=%s and ncells=%s" % (allporders,allncells))

		# this never gave good colors:
		#possiblecolors = plt.cm.jet(np.linspace(0,0.9,len(allporders)))
		# use instead:
		possiblecolors = "r g k b m y r c m".split(" ")
		#possiblestyles = ['-', '--', '-.', ':']
		possiblemarkers = ['o', '>', 's', 'D','H', '*','<',]
		# commented out because currently unused, but code works.
		#assert len(possiblemarkers) == len(uniquelist(ncells))
		#styles = lookupdict(allncells, possiblestyles)
		markerdict = lookupfitdict(allporders, possiblemarkers)
		colordict = lookupfitdict(allncells, possiblecolors)
		#self.logger.info("markers for %s: %s" % (str(allporders), str(markerdict)))
		markers = lambda p, nc: markerdict[int(p)]
		colors = lambda p, nc: colordict[int(nc)]

		##### SIMPLE ERROR EVOLUTION PLOTS
		self.logger.debug("Simple Error Evolution Plots")
		errorPlot = plt.figure(figsize=(18,8))
		plt.grid()
		ycolumn = 'max' # l2norm, l1norm

		num_markers = 10
		dt_markers = max(self.errors[idx.time]) / num_markers
		for groupidx,rows in self.errors.groupby(by=[idx.porder,idx.cells]):
			# groupidx = (p,nc)
			plt.plot(rows[idx.time], rows[ycolumn], label="P=%d,Nc=%d"%groupidx, color=colors(*groupidx), marker=markers(*groupidx), markevery=0.03)

		plt.legend(loc='center left', bbox_to_anchor=(1,0.5))
		plt.subplots_adjust(right=0.8)
		plt.title("Error evolution: %s of quantity %s" % (ycolumn, self.quantity))
		plt.xlabel("Simulation time")
		plt.ylabel("Error")
		ax = plt.gca()
		ax.set_yscale('log')
		plt.ylim(1e-10, 1e0)

		##### CONVERGENCE EVOLUTION PLOTS
		self.logger.debug("Convergence Evolution Plots")
		ycolumn = 'ol2norm'
		reprID = self.get_ci_paramtable('EXAHYPE_INITIALDATA').iloc[0]
		reprSpecFile = self.get_ci_paramtable('ExaSpecfile').iloc[0]

		convergencePlot = plt.figure(figsize=(18,8)) # plt.gcf()
		convergencePlot.suptitle("ExaHyPE convergence of %s for %s with %s" % (ycolumn, reprID, reprSpecFile), fontsize=18)

		# this is an ugly bug, we are dealing with ints but storing floats
		# and then making comparisons float(9) == float(9.000000...) which never hold true.
		allporders, allncells = uniquelist(self.porders), uniquelist(self.ncells)

		# do plots for all neighbouring (Nprev, Ncur)
		for Ni, (Nprev, Ncur) in enumerate(window(allncells)):
			self.logger.debug("Neighbouring plot between (%d, %d)" % (Nprev, Ncur))
			plt.subplot(1, len(allncells)-1, Ni+1)
			plt.grid()

			plt.title("Convergence from %d to %d cells" % (iround(Nprev), iround(Ncur)))
			for groupidx,rows in pdsort(self.convergence_table[self.convergence_table[idx.cells]==Ncur], by=[idx.porder,idx.cells], ascending=False).groupby(by=[idx.porder,idx.cells]):
				treshholdToShowPoints = 40
				style = "o-" if len(rows[ycolumn]) < treshholdToShowPoints else "-"				
				(p,nc) = groupidx
				plt.plot(rows[idx.time], rows[ycolumn], style, label="P=%d" % int(p))#, color=colors(*groupidx))#, marker=markers[nc], markersize=25)

			plt.xlabel("Simulation time")
			plt.ylabel("Convergence order")
			plt.legend()#.draggable()
			plt.ylim(0,10)


		try:
			self.logger.debug("Generating convergence SVG figure")
			self.tpl['CONVERGENCE_SVG_FIGURE'] = gensvg(convergencePlot)
			self.logger.debug("Generating error SVG figure")
			self.tpl['ERROR_SVG_FIGURE'] = gensvg(errorPlot)
		except Exception as e:
			self.logger.error("Could not create plots, reason: %s" % str(e))
			self.logger.exception(e)
			self.tpl['CONVERGENCE_SVG_FIGURE'] = u"<em>Creation crashed!</em>"
			self.tpl['ERROR_SVG_FIGURE'] = u"<em>Creation crashed!</em>"


	@steps.add(5, "Determine whether convergence was observed")
	def determineConvergenceQuality(self):
		# We can also programmatically decide whether decent convergence
		# is present or not. We measure acceptability as the difference from
		# ideal scaling.
		def acceptability(row):
			# assuming the ideal scaling is always bigger than the real scaling, we sum
			# up defects which are an indicator about scaling quality
			return sum([ (row[idx.porder]+1) - row[rateCol] for rateCol in idx.rateColumns ])

		convergenceQuality = self.convergence_table.apply(acceptability, axis=1)
		convergenceQualityIndicator = convergenceQuality.mean()

		convergenceQualityBelowNames = { 1: 'excellent', 5: 'passes', 10: 'failed' }
		self.convergencePassed = ( convergenceQualityIndicator < 5.0 )

		self.tpl['CONVERGENCE_QUALITY_INDICATOR'] = u"%.2f" % convergenceQualityIndicator
		self.tpl['CONVERGENCE_PASSED'] = (u'PASSED' if self.convergencePassed else u'FAILED')

		self.logger.info("Convergence test results:")
		self.logger.info("Convergence factor is " + self.tpl['CONVERGENCE_QUALITY_INDICATOR'])
		self.logger.info("Convergence test is " + self.tpl['CONVERGENCE_PASSED'])

		return self.convergencePassed

	@steps.add(6, "Writing out HTML report files")
	def writeOutReportFiles(self):
		self.overview.execute(self.tpl)
		self.evolution.execute(self.tpl)

def main():
	"A main function for the convergence table computation."
	logger = logging.getLogger("convergence_table.py")

	frontend = ExaFrontend(program_description=__doc__)
	reporter = ConvergenceReporter()
	frontend.add_module(reporter)

	args = frontend.parse_args()
	convergencePassed = reporter.start()

	logger.info("Finishing with convergencePassed=%s" % str(convergencePassed))
	exitConvergenceStatus(convergencePassed)

if __name__ == "__main__":
	main()


