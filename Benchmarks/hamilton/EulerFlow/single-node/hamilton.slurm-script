#!/bin/bash
#SBATCH --job-name="ExaHyPE-EulerFlow"
#SBATCH -o ExaHyPE-EulerFlow.%A.out
#SBATCH -e ExaHyPE-EulerFlow.%A.err
#SBATCH -t 01:00:00
#SBATCH --exclusive
#SBATCH -p par7.q
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=1
#SBATCH --mail-user=dominic.e.charrier@durham.ac.uk
#SBATCH --mail-type=ALL

source /etc/profile.d/modules.sh
module purge
module load slurm
module load intel/xe_2017.2
module load intelmpi/intel/2017.2
module load gcc

export TBB_SHLIB="-L/ddn/apps/Cluster-Apps/intel/xe_2017.2/tbb/lib/intel64/gcc4.7 -ltbb"

export I_MPI_FABRICS="shm:dapl"

nodes=1
tasksPerNode=1
coresPerTask=1
let procsPerNode=tasksPerNode*coresPerTask
let tasks=nodes*tasksPerNode

compiler=Intel
sharedMem=None
prefix=EulerFlow-no-output-p3-regular-0
out=$prefix-n$nodes-t$tasksPerNode-c$coresPerTask-$sharedMem-$compiler.out

script=hamilton.slurm-script
spec=benchmarks/multicore/$prefix-t$tasksPerNode-c$coresPerTask.exahype

# pipe some information into output file
module list 2>$out
echo "" >> $out
cat $script >>$out
echo "" >> $out
cat $spec >> $out
#ExaHyPE-EulerFlow-p3-$sharedMem-$compiler --version >> $out

# execute the job
mpiexec -np $tasks ./ExaHyPE-EulerFlow-p3-$sharedMem-$compiler $spec >> $out
